{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "lang = 'eng'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "features_dir = f'../data/features_30/{lang}/'\n",
    "results_dir = f'../data/results_canon/{lang}/'\n",
    "sentiment_dir = '../data/labels_sentiment/'\n",
    "canonization_labels_dir = '../data/labels_canon/'\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b00c037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR, SVC\n",
    "import xgboost\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBRegressor\n",
    "from copy import deepcopy\n",
    "from scipy.stats import pearsonr\n",
    "from utils import read_canon_labels\n",
    "from math import sqrt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline') # %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import statistics\n",
    "import random\n",
    "random.seed(2)\n",
    "\n",
    "labels = read_canon_labels(canonization_labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647b38f-c89e-4225-ba24-e37cbd256951",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fehler\n",
    "Brooke_Frances_Emily_Montague_1769\n",
    "Lennox_Charlotte_The-Female_Quixote_1752\n",
    "Stoker_Bram-Dracula_1897\n",
    "\n",
    "Nicolai_Friedrich_Sebaldus-Nothanker-1773\n",
    "Jung-Stilling_Heinrich-Stillings-Jugend_1777\n",
    "Sacher-Masoch_Venus-im-Pelz_1869\n",
    "Hunold_Christian_Friedrich_Die-liebenswuerdige-Adalie_1681\n",
    "\n",
    "Hoffmansthal_Hugo ['Hoffmansthal_Hugo_Andreas-oder-die-Vereinigten_1907', 'Hoffmansthal_Hugo_Das-Maerchen-der-672-Nacht_1895'] \n",
    "Hoffmansthal_Hugo-von ['Hoffmansthal_Hugo-von_Ein-Brief_1902', 'Hoffmansthal_Hugo-von_Reitergeschichte_1899'] \n",
    "\n",
    "\n",
    "Anonymous anonymous\n",
    "\n",
    "df = df.loc[df['file_name'] != 'Defoe_Daniel_Roxana_1724']\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_list = []\n",
    "\n",
    "class HyperParameterOptimizer(object):\n",
    "    def __init__(self, file_names=None):\n",
    "        self.file_names = file_names\n",
    "        # self.model = None\n",
    "        # self.feature_importances_ = None\n",
    "    \n",
    "    def fit(self, _train_X, _train_y):\n",
    "        global global_list\n",
    "        n_fold = 5\n",
    "        df = np.hstack((self.file_names, _train_X))\n",
    "        df = pd.DataFrame(df, columns=['file_name'] + [f'col_{i}' for i in range(_train_X.shape[1])])\n",
    "        \n",
    "        print(f'Current shape of feature matrix:', _train_X.shape)\n",
    "        num_boost_round = 99999\n",
    "\n",
    "        def feval(preds, train_data):\n",
    "            labels = train_data.get_label()\n",
    "            return 'r2', r2_score(labels, preds)\n",
    "        \n",
    "        all_results = []\n",
    "        for max_depth in [4]:\n",
    "            for learning_rate in [0.03]:\n",
    "                for colsample_bytree in [0.33]:\n",
    "                    for min_child_weight in [6]:\n",
    "                        train_X = deepcopy(_train_X)\n",
    "                        train_y = deepcopy(_train_y)\n",
    "                        dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "                        params = {'max_depth': max_depth, 'learning_rate': learning_rate, 'colsample_bytree': colsample_bytree, 'min_child_weight': min_child_weight, 'n_jobs': -1}\n",
    "                        cv_results = xgboost.cv(\n",
    "                            params,\n",
    "                            dtrain,\n",
    "                            num_boost_round=num_boost_round,\n",
    "                            seed=40,\n",
    "                            nfold=n_fold,\n",
    "                            folds=AuthorSplit(df, n_fold, seed=3, return_indices=True).split(),\n",
    "                            feval=feval,\n",
    "                            maximize=True,\n",
    "                            early_stopping_rounds=5,\n",
    "                            verbose_eval=False\n",
    "                        )\n",
    "                        print(train_X.shape[1], max_depth, learning_rate, colsample_bytree, min_child_weight, np.round(float(cv_results['test-r2-mean'].iloc[len(cv_results['test-r2-mean'])-1]), 4))\n",
    "                        all_results.append((max_depth, learning_rate, colsample_bytree, min_child_weight, cv_results))\n",
    "        all_results_df = pd.DataFrame(all_results, columns=['max_depth', 'learning_rate', 'colsample_bytree', 'min_child_weight', 'cv_results'])\n",
    "        all_results_df['best_validation_r2'] = all_results_df['cv_results'].apply(lambda x: x.iloc[len(x)-1]['test-r2-mean'])\n",
    "        all_results_df = all_results_df.sort_values(by='best_validation_r2', ascending=False).reset_index(drop=True)\n",
    "        best_parameters = all_results_df.iloc[0]\n",
    "        best_max_depth = int(best_parameters['max_depth'])\n",
    "        best_learning_rate = float(best_parameters['learning_rate'])\n",
    "        best_colsample_bytree = float(best_parameters['colsample_bytree'])\n",
    "        best_min_child_weight = float(best_parameters['min_child_weight'])\n",
    "        best_validation_r2 = float(best_parameters['best_validation_r2'])\n",
    "        best_num_boost_round = int(len(best_parameters['cv_results']))\n",
    "        \n",
    "        print('Current best max_depth:', best_max_depth)\n",
    "        print('Current best learning_rate:', best_learning_rate)\n",
    "        print('Current best colsample_bytree:', best_colsample_bytree)\n",
    "        print('Current best min_child_weight:', best_min_child_weight)\n",
    "        print('Current best num_boost_round:', best_num_boost_round)\n",
    "        print(f'Current best validation r2 score is {np.round(best_validation_r2, 4)}')\n",
    "        print('############################')\n",
    "        params = {'max_depth': best_max_depth, 'learning_rate': best_learning_rate, 'colsample_bytree': best_colsample_bytree, 'min_child_weight': best_min_child_weight, 'n_jobs': -1}\n",
    "        \n",
    "        train_X = deepcopy(_train_X)\n",
    "        train_y = deepcopy(_train_y)\n",
    "        dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "        model = xgboost.train(params,\n",
    "                              dtrain,\n",
    "                              num_boost_round=best_num_boost_round,\n",
    "                              verbose_eval=False)\n",
    "        feature_importances = model.get_score(importance_type='gain')\n",
    "        feature_importances_list = []\n",
    "        for i in range(train_X.shape[1]):\n",
    "            current_key = f'f{i}'\n",
    "            if current_key in feature_importances:\n",
    "                feature_importances_list.append(feature_importances[current_key])\n",
    "            else:\n",
    "                feature_importances_list.append(0.0)\n",
    "        self.model = model\n",
    "        self.feature_importances_ = np.array(feature_importances_list)\n",
    "        \n",
    "        global_list.append((best_validation_r2, train_X.shape[1], best_num_boost_round, params))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(xgboost.DMatrix(X))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.predict(X)\n",
    "\n",
    "    def score(self, X=None, y=None):\n",
    "        return 0.0\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {'file_names': self.file_names}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.file_names = params['file_names']\n",
    "        return self\n",
    "\n",
    "    def _get_tags(self):\n",
    "        return {'allow_nan': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd37a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorSplit():\n",
    "    \"\"\"\n",
    "    Distribute book names over splits.\n",
    "    All works of an author are in the same split.\n",
    "    Adapted from https://www.titanwolf.org/Network/q/b7ee732a-7c92-4416-bc80-a2bd2ed136f1/y\n",
    "    Stevenson-Grift_Robert-Louis-Fanny-van-de_The-Dynamiter_1885\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, df, nr_splits, seed, return_indices=False):\n",
    "        self.df = df\n",
    "        self.nr_splits = nr_splits\n",
    "        self.return_indices = return_indices\n",
    "        self.file_names = df['file_name'].unique()\n",
    "        self.author_bookname_mapping, self.works_per_author = self.get_author_books()\n",
    "        random.seed(seed)\n",
    "\n",
    "    def get_author_books(self):\n",
    "        authors = []\n",
    "        author_bookname_mapping = {}\n",
    "        #Get authors\n",
    "        for file_name in self.file_names:\n",
    "            author = '_'.join(file_name.split('_')[:2])\n",
    "            authors.append(author)\n",
    "            if author in author_bookname_mapping:\n",
    "                author_bookname_mapping[author].append(file_name)\n",
    "            else:\n",
    "                author_bookname_mapping[author] = []\n",
    "                author_bookname_mapping[author].append(file_name)\n",
    "                \n",
    "        # Aggregate if author has worked anlone and with others\n",
    "#         if lang == 'ger':\n",
    "#             agg_dict = {'Hoffmansthal_Hugo': ['Hoffmansthal_Hugo-von'], ###################################\n",
    "#                         'Schlaf_Johannes': ['Holz-Schlaf_Arno-Johannes'],\n",
    "#                          'Arnim_Bettina': ['Arnim-Arnim_Bettina-Gisela']}\n",
    "#         else:\n",
    "#             agg_dict = {'Stevenson_Robert-Louis': ['Stevenson-Grift_Robert-Louis-Fanny-van-de', \n",
    "#                                                    'Stevenson-Osbourne_Robert-Louis-Lloyde']}\n",
    "            \n",
    "#         for author, aliases in agg_dict.items():\n",
    "#             if author in authors:\n",
    "#                 for alias in aliases:\n",
    "#                     author_bookname_mapping[author].extend(author_bookname_mapping[alias]) \n",
    "#                     del author_bookname_mapping[alias]\n",
    "#                     authors = [author for author in authors if author != alias]\n",
    "        \n",
    "        works_per_author = Counter(authors)\n",
    "        return author_bookname_mapping, works_per_author\n",
    "    \n",
    "    def split(self):\n",
    "        splits = [[] for _ in range(0,self.nr_splits)]\n",
    "        totals = [(0,i) for i in range (0, self.nr_splits)]\n",
    "        # heapify based on first element of tuple, inplace\n",
    "        heapq.heapify(totals)\n",
    "        while bool(self.works_per_author):\n",
    "            author = random.choice(list(self.works_per_author.keys()))\n",
    "            author_workcount = self.works_per_author.pop(author)\n",
    "            # find split with smallest number of books\n",
    "            total, index = heapq.heappop(totals)\n",
    "            splits[index].append(author)\n",
    "            heapq.heappush(totals, (total + author_workcount, index))\n",
    "\n",
    "        if not self.return_indices:\n",
    "            #Map author splits to book names\n",
    "            map_splits = []\n",
    "            for split in splits:\n",
    "                new = []\n",
    "                for author in split:\n",
    "                    new.extend(self.author_bookname_mapping[author])\n",
    "                map_splits.append(new)\n",
    "        else:\n",
    "            file_name_idx_mapping = dict((file_name, index) for index, file_name in enumerate(self.file_names))\n",
    "            map_splits = []\n",
    "            for split in splits:\n",
    "                test_split = []\n",
    "                for author in split:\n",
    "                    test_split.extend([file_name_idx_mapping[file_name] for file_name in  self.author_bookname_mapping[author]])\n",
    "                train_split = list(set(file_name_idx_mapping.values()) - set(test_split))\n",
    "                map_splits.append((train_split, test_split))\n",
    "        return map_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression():\n",
    "    def __init__(self, language, features, drop_columns, dimensionality_reduction, model_param, model, verbose, params_to_use):\n",
    "        assert isinstance(drop_columns, list)\n",
    "        for i in drop_columns:\n",
    "            assert isinstance(i, str)\n",
    "        assert (dimensionality_reduction in ['k_best_f_reg_0_10', 'k_best_mutual_info_0_10', 'ss_pca_0_95', 'rfe']) or (dimensionality_reduction is None)\n",
    "        self._check_class_specific_assertions()\n",
    "        \n",
    "        self.language = language\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.drop_columns = drop_columns\n",
    "        self.dimensionality_reduction = dimensionality_reduction\n",
    "        self.model_param = model_param\n",
    "        self.model = model\n",
    "        self.verbose = verbose\n",
    "        self.params_to_use = params_to_use\n",
    "        self.best_features = []\n",
    "\n",
    "        if self.features == 'book':\n",
    "            self.df = deepcopy(book_df)\n",
    "        elif self.features == 'chunk':\n",
    "            self.df = deepcopy(chunk_df)\n",
    "        elif self.features == 'chunk_and_copied_book':\n",
    "            self.df = deepcopy(chunk_and_copied_book_df)\n",
    "        elif self.features == 'book_and_averaged_chunk':\n",
    "            self.df = deepcopy(book_and_averaged_chunk_df)\n",
    "\n",
    "        columns_before_drop = set(self.df.columns)\n",
    "        if self.drop_columns:\n",
    "            self.df = self.df[[column for column in self.df.columns if not self._drop_column(column)]].reset_index(drop=True)\n",
    "        columns_after_drop = set(self.df.columns)\n",
    "        if self.verbose:\n",
    "            print(f'Dropped {len(columns_before_drop - columns_after_drop)} columns.')\n",
    "            \n",
    "    def _check_class_specific_assertions(self):\n",
    "        assert model in ['xgboost', 'svr', 'lasso']\n",
    "        assert features in ['book', 'chunk', 'book_and_averaged_chunk', 'chunk_and_copied_book']\n",
    "\n",
    "    def _drop_column(self, column):\n",
    "        for string in self.drop_columns:\n",
    "            if string in column:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def _custom_pca(self, train_X):\n",
    "        for i in range(5, train_X.shape[1], int((train_X.shape[1] - 5) / 10)):\n",
    "            pca = PCA(n_components=i)\n",
    "            new_train_X = pca.fit_transform(train_X)\n",
    "            if pca.explained_variance_ratio_.sum() >= 0.95:\n",
    "                break\n",
    "        return new_train_X, pca\n",
    "\n",
    "    def _select_features(self, train_X, train_y, validation_X, train_file_names):\n",
    "        if self.dimensionality_reduction == 'ss_pca_0_95':\n",
    "            ss = StandardScaler()\n",
    "            train_X = ss.fit_transform(train_X)\n",
    "            validation_X = ss.transform(validation_X)\n",
    "            train_X, pca = self._custom_pca(train_X)\n",
    "            validation_X = pca.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == 'k_best_f_reg_0_10':\n",
    "            #Find best featues\n",
    "            k_best = SelectKBest(f_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            k_best = k_best.fit(train_X, train_y)\n",
    "            mask = k_best.get_support(indices=True)\n",
    "            selected_features_df = train_X.iloc[:,mask]\n",
    "            self.best_features.extend(selected_features_df.columns.tolist())\n",
    "            \n",
    "            print('trainX before feature selection', train_X.shape)\n",
    "            train_X = k_best.transform(train_X)\n",
    "            print('trainX after feature selection', train_X.shape)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "\n",
    "        elif self.dimensionality_reduction == 'k_best_mutual_info_0_10':\n",
    "            k_best = SelectKBest(mutual_info_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == 'rfe':\n",
    "            global global_list\n",
    "            best_num_boost_round = None\n",
    "            hpo = HyperParameterOptimizer(file_names=train_file_names)\n",
    "            rfe = RFE(hpo, step=50, n_features_to_select=10, verbose=0) # only 14 features if all columns are dropped\n",
    "            rfe.fit(train_X, train_y)\n",
    "            global_list = sorted(global_list, key=lambda x: -x[0])\n",
    "            best_feature_count = global_list[0][1]\n",
    "            best_num_boost_round = global_list[0][2]\n",
    "            best_params = global_list[0][3]\n",
    "            for ranking in range(rfe.ranking_.max()):\n",
    "                if (rfe.ranking_ <= ranking).sum() == best_feature_count:\n",
    "                    break\n",
    "            train_X = train_X[:, np.argwhere((rfe.ranking_ <= ranking) == True).T[0]]\n",
    "            validation_X = validation_X[:, np.argwhere((rfe.ranking_ <= ranking) == True).T[0]]\n",
    "            global_list = []\n",
    "        elif self.dimensionality_reduction is None:\n",
    "            pass\n",
    "        print('Feature Selection Done')\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _impute(self, train_X, validation_X):\n",
    "        imputer = KNNImputer()\n",
    "        train_X = imputer.fit_transform(train_X)\n",
    "        validation_X = imputer.transform(validation_X)\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _get_model(self, model_param, train_X=None, train_y=None, train_file_names=None, task_type=None):\n",
    "        if self.model == 'xgboost':\n",
    "            if task_type == 'binary_classification':\n",
    "                is_classification = True\n",
    "                class_weights = dict(enumerate(compute_class_weight('balanced', classes=[0, 1], y=train_y.astype(int).tolist())))\n",
    "            elif task_type == 'multiclass_classification':\n",
    "                is_classification = True\n",
    "                class_weights = dict(enumerate(compute_class_weight('balanced', classes=[0, 1, 2, 3], y=train_y.astype(int).tolist())))\n",
    "            elif task_type == 'regression':\n",
    "                is_classification = False\n",
    "            else:\n",
    "                raise Exception('Not a valid task_type')\n",
    "            \n",
    "            def feval(preds, train_data):\n",
    "                labels = train_data.get_label()\n",
    "                if is_classification:\n",
    "                    labels = labels.astype(int)\n",
    "                    preds = preds.argmax(axis=1).astype(int)\n",
    "                    if task_type == 'binary_classification':\n",
    "                        return 'acc', accuracy_score(labels, preds)\n",
    "                    elif task_type == 'multiclass_classification':\n",
    "                        return 'f1', f1_score(labels, preds, average='macro')\n",
    "                else:\n",
    "                    return 'rmse', np.sqrt(mean_squared_error(labels, preds))\n",
    "            \n",
    "            if is_classification:\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y.astype(int), weight=[class_weights[int(i)] for i in train_y])\n",
    "            else:\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "            results = []\n",
    "            df = np.hstack((train_file_names, train_X))\n",
    "            df = pd.DataFrame(df, columns=['file_name'] + [f'col_{i}' for i in range(train_X.shape[1])])\n",
    "            for max_depth in [2, 4, 6, 8]:\n",
    "                for learning_rate in [None, 0.01, 0.033, 0.1]:\n",
    "                    for colsample_bytree in [0.33, 0.60, 0.75]:\n",
    "                        if task_type == 'multiclass_classification':\n",
    "                            params = {'max_depth': max_depth, 'learning_rate': learning_rate, 'colsample_bytree': colsample_bytree, 'n_jobs': -1, 'objective': 'multi:softmax', 'num_class': 4, 'eval_metric': 'mlogloss'}\n",
    "                        elif task_type == 'binary_classification':\n",
    "                            params = {'max_depth': max_depth, 'learning_rate': learning_rate, 'colsample_bytree': colsample_bytree, 'n_jobs': -1, 'objective': 'multi:softmax', 'num_class': 2, 'eval_metric': 'mlogloss'}\n",
    "                        elif task_type == 'regression':\n",
    "                            params = {'max_depth': max_depth, 'learning_rate': learning_rate, 'colsample_bytree': colsample_bytree, 'n_jobs': -1}\n",
    "                        else:\n",
    "                            raise Exception('Not a valid task_type')\n",
    "                        cv_results = xgboost.cv(\n",
    "                                        params,\n",
    "                                        dtrain,\n",
    "                                        num_boost_round=99999,\n",
    "                                        seed=42,\n",
    "                                        nfold=5,\n",
    "                                        folds=AuthorSplit(df, 5, seed=8, return_indices=True).split(),\n",
    "                                        feval=feval,\n",
    "                                        maximize=is_classification, # if classification, maximize f1/acc score.\n",
    "                                        early_stopping_rounds=10,\n",
    "                                        verbose_eval=False)\n",
    "\n",
    "                        if task_type == 'binary_classification':\n",
    "                            nested_cv_score = cv_results.iloc[len(cv_results)-1]['test-acc-mean']\n",
    "                        elif task_type == 'multiclass_classification':\n",
    "                            nested_cv_score = cv_results.iloc[len(cv_results)-1]['test-f1-mean']\n",
    "                        elif task_type == 'regression':\n",
    "                            nested_cv_score = cv_results.iloc[len(cv_results)-1]['test-rmse-mean']\n",
    "                        else:\n",
    "                            raise Exception('Not a valid task_type')\n",
    "                        num_boost_round = len(cv_results)\n",
    "                        if task_type == 'multiclass_classification':\n",
    "                            results.append({'max_depth': max_depth, 'learning_rate': learning_rate, 'colsample_bytree': colsample_bytree, 'num_boost_round': num_boost_round, 'nested_cv_score': nested_cv_score, 'objective': 'multi:softmax', 'num_class': 4, 'eval_metric': 'mlogloss'})\n",
    "                        elif task_type == 'binary_classification':\n",
    "                            results.append({'max_depth': max_depth, 'learning_rate': learning_rate, 'colsample_bytree': colsample_bytree, 'num_boost_round': num_boost_round, 'nested_cv_score': nested_cv_score, 'objective': 'multi:softmax', 'num_class': 2, 'eval_metric': 'mlogloss'})\n",
    "                        elif task_type == 'regression':\n",
    "                            results.append({'max_depth': max_depth, 'learning_rate': learning_rate, 'colsample_bytree': colsample_bytree, 'num_boost_round': num_boost_round, 'nested_cv_score': nested_cv_score})\n",
    "                        else:\n",
    "                            raise Exception('Not a valid task_type')\n",
    "            best_parameters = sorted(results, key=lambda x: x['nested_cv_score'], reverse=is_classification)[0]\n",
    "            return best_parameters\n",
    "        elif self.model == 'svr':\n",
    "            return SVR(C=model_param)\n",
    "        elif self.model == 'lasso':\n",
    "            return Lasso(alpha=model_param)\n",
    "        elif self.model == 'svc':\n",
    "            return SVC(C=model_param, class_weight='balanced')\n",
    "        \n",
    "    \n",
    "    def _get_pvalue(self, validation_corr_pvalues):\n",
    "        # Harmonic mean p-value\n",
    "        denominator = sum([1/x for x in validation_corr_pvalues])\n",
    "        mean_p_value = len(validation_corr_pvalues)/denominator\n",
    "        return mean_p_value\n",
    "    \n",
    "    def _combine_df_labels(self, df):\n",
    "        #Average of sentiscores per book\n",
    "        df = df.merge(right=self.labels, on='file_name', how='inner', validate='many_to_one')\n",
    "        return df\n",
    "    \n",
    "    def run(self):\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        train_mses = []\n",
    "        train_maes = []\n",
    "        train_r2s = []\n",
    "        train_corrs = []\n",
    "        \n",
    "        validation_mses = []\n",
    "        validation_maes = []\n",
    "        validation_r2s = []\n",
    "        validation_corrs = []\n",
    "        validation_corr_pvalues = []\n",
    "\n",
    "        df = self.df\n",
    "        df = self._combine_df_labels(df)\n",
    "        file_names_split = AuthorSplit(df, 5, seed=2, return_indices=False).split() ## 10 folds\n",
    "        all_validation_books = []\n",
    "\n",
    "        for index, split in enumerate(file_names_split):\n",
    "            train_df = df[~df['file_name'].isin(split)]\n",
    "            validation_df = df[df['file_name'].isin(split)]\n",
    "            \n",
    "            train_X = train_df.drop(columns=['y', 'file_name'])\n",
    "            train_y = train_df['y']\n",
    "            validation_X = validation_df.drop(columns=['y', 'file_name'])\n",
    "            validation_y = validation_df['y']\n",
    "            \"\"\"\n",
    "            train_X = train_df.drop(columns=['y', 'file_name']).values\n",
    "            train_y = train_df['y'].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=['y', 'file_name']).values\n",
    "            validation_y = validation_df['y'].values.ravel()\n",
    "            \"\"\"\n",
    "            #Impute missing values if df contains NaNs\n",
    "            #train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f'train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}')\n",
    "            train_X, validation_X = self._select_features(train_X, train_y, validation_X, train_file_names=train_df['file_name'].values.reshape(-1, 1))\n",
    "            #if self.verbose:\n",
    "            #    print(f'train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}')\n",
    "            if self.model == 'xgboost':\n",
    "                train_file_names = train_df['file_name'].values.reshape(-1, 1)\n",
    "                best_parameters = self._get_model(self.model_param, train_X, train_y, train_file_names, task_type='regression')\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "                num_boost_round = best_parameters['num_boost_round']\n",
    "                best_parameters.pop('nested_cv_score')\n",
    "                best_parameters.pop('num_boost_round')\n",
    "                model = xgboost.train(best_parameters,\n",
    "                                      dtrain,\n",
    "                                      num_boost_round=num_boost_round,\n",
    "                                      verbose_eval=False)\n",
    "            else:\n",
    "                model = self._get_model(self.model_param)\n",
    "                model.fit(train_X, train_y)\n",
    "            \n",
    "            train_books = deepcopy(train_df[['file_name', 'y']])\n",
    "            validation_books = deepcopy(validation_df[['file_name', 'y']])\n",
    "            \n",
    "            if self.model == 'xgboost':\n",
    "                train_books['yhat'] = model.predict(xgboost.DMatrix(train_X))\n",
    "                validation_books['yhat'] = model.predict(xgboost.DMatrix(validation_X))\n",
    "                \n",
    "                print('train preds:', model.predict(xgboost.DMatrix(train_X)))\n",
    "                print('validation preds:', model.predict(xgboost.DMatrix(validation_X)))\n",
    "            else:\n",
    "                train_books['yhat'] = model.predict(train_X)\n",
    "                validation_books['yhat'] = model.predict(validation_X)\n",
    "            \n",
    "            train_books = train_books.groupby('file_name').mean()\n",
    "            validation_books = validation_books.groupby('file_name').mean()\n",
    "            all_validation_books.append(validation_books.reset_index())\n",
    "            \n",
    "            train_y = train_books['y'].tolist()\n",
    "            train_yhat = train_books['yhat'].tolist()\n",
    "            validation_y = validation_books['y'].tolist()\n",
    "            validation_yhat = validation_books['yhat'].tolist()\n",
    "            \n",
    "            all_labels.extend(validation_y)\n",
    "            all_predictions.extend(validation_yhat)\n",
    "            \n",
    "            train_mse = mean_squared_error(train_y, train_yhat)\n",
    "            train_mae = mean_absolute_error(train_y, train_yhat)\n",
    "            train_r2 = r2_score(train_y, train_yhat)\n",
    "            train_corr = pearsonr(train_y, train_yhat)[0]\n",
    "            \n",
    "            validation_mse = mean_squared_error(validation_y, validation_yhat)\n",
    "            validation_mae = mean_absolute_error(validation_y, validation_yhat)\n",
    "            validation_r2 = r2_score(validation_y, validation_yhat)\n",
    "            validation_corr = pearsonr(validation_y, validation_yhat)[0]\n",
    "            p_value = pearsonr(validation_y, validation_yhat)[1]\n",
    "            \n",
    "            train_mses.append(train_mse)\n",
    "            train_maes.append(train_mae)\n",
    "            train_r2s.append(train_r2)\n",
    "            train_corrs.append(train_corr)\n",
    "            \n",
    "            validation_mses.append(validation_mse)\n",
    "            validation_maes.append(validation_mae)\n",
    "            validation_r2s.append(validation_r2)\n",
    "            validation_corrs.append(validation_corr)\n",
    "            validation_corr_pvalues.append(p_value)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'Fold: {index+1}, TrainMSE: {np.round(train_mse, 3)}, TrainMAE: {np.round(train_mae, 3)}, ValMSE: {np.round(validation_mse, 3)},'\n",
    "                    f'ValMAE: {np.round(validation_mae, 3)}, ValR2: {np.round(validation_r2, 3)}, ValCorr: {np.round(validation_corr, 3)}')\n",
    "        print('loop finished')\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        \n",
    "        # Save y and y_pred for examples\n",
    "        model_info_string = f'{self.language}-{self.model}-{self.dimensionality_reduction}-{self.features}-pram{self.model_param}-{self.params_to_use}'\n",
    "        pd.concat(all_validation_books).to_csv(f'{results_dir}y_yhat-{model_info_string}.csv', index=False)\n",
    "       \n",
    "        mean_train_mse = np.mean(train_mses)\n",
    "        mean_train_rmse = np.mean([sqrt(x) for x in train_mses])\n",
    "        mean_train_mae = np.mean(train_maes)\n",
    "        mean_train_r2 = np.mean(train_r2s)\n",
    "        mean_train_corr = np.mean(train_corrs)\n",
    "        \n",
    "        mean_validation_mse = np.mean(validation_mses)\n",
    "        mean_validation_rmse = np.mean([sqrt(x) for x in validation_mses])\n",
    "        mean_validation_mae = np.mean(validation_maes)\n",
    "        mean_validation_r2 = np.mean(validation_r2s)\n",
    "        mean_validation_corr = np.mean(validation_corrs)\n",
    "        mean_p_value = self._get_pvalue(validation_corr_pvalues)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f'TrainMSE: {np.round(mean_train_mse, 3)}, TrainRMSE: {np.round(mean_train_rmse, 3)}, TrainMAE: {np.round(mean_train_mae, 3)}, TrainR2: {np.round(mean_train_r2, 3)},'\n",
    "                    f'TrainCorr: {np.round(mean_train_corr, 3)}, ValMSE: {np.round(mean_validation_mse, 3)}, ValRMSE: {np.round(mean_validation_rmse, 3)}, ValMAE: {np.round(mean_validation_mae, 3)},'\n",
    "                    f'ValR2: {np.round(mean_validation_r2, 3)}, ValCorr: {np.round(mean_validation_corr, 3)}, ValCorrPValue: {np.round(mean_p_value, 3)}')\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.xticks(fontsize=15)\n",
    "            plt.yticks(fontsize=15)\n",
    "            plt.xlim([0,1])\n",
    "            plt.ylim([0,1])\n",
    "\n",
    "            plt.scatter(all_labels, all_predictions, s=6)\n",
    "            plt.xlabel('Canonization Scores', fontsize=20)\n",
    "            plt.ylabel('Predicted Scores', fontsize=20)\n",
    "            plt.savefig(f'{results_dir}{model_info_string}.png', dpi=400, bbox_inches='tight')   \n",
    "            plt.show();\n",
    "            print('\\n---------------------------------------------------\\n')\n",
    "\n",
    "        best_features = dict(Counter(self.best_features))\n",
    "        \n",
    "        return (\n",
    "            mean_train_mse, \n",
    "            mean_train_rmse, \n",
    "            mean_train_mae, \n",
    "            mean_train_r2, \n",
    "            mean_train_corr, \n",
    "            mean_validation_mse, \n",
    "            mean_validation_rmse, \n",
    "            mean_validation_mae, \n",
    "            mean_validation_r2, \n",
    "            mean_validation_corr, \n",
    "            mean_p_value, \n",
    "            best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27abc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameter combinations\n",
    "\"\"\"\n",
    "drop_columns_list = [\n",
    "    ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'],\n",
    "    ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'],\n",
    "    ]\n",
    "if lang == 'eng':\n",
    "    drop_columns_list.extend([\n",
    "        ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'], \n",
    "        ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos']\n",
    "    ])\n",
    "    \n",
    "models = ['svr', 'lasso', 'xgboost', 'svc']\n",
    "model_params = {'svr': [1], 'lasso': [1, 4], 'xgboost': [None], 'svc': [0.1, 1, 10, 100, 1000, 10000]} \n",
    "dimensionality_reduction = ['ss_pca_0_95', 'k_best_f_reg_0_10', 'k_best_mutual_info_0_10', 'rfe', [None]]\n",
    "features = ['book', 'chunk', 'book_and_averaged_chunk', 'chunk_and_copied_book']\n",
    "\n",
    "regression_params = {'model': models[2], 'dimensionality_reduction': dimensionality_reduction[-1], 'features': [features[0]]}\n",
    "testing_params = {'model': models[0], 'dimensionality_reduction': dimensionality_reduction[1], 'features': [features[0]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30654f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run Regression\n",
    "\"\"\"\n",
    "results = []\n",
    "params_to_use = 'testing' \n",
    "if params_to_use == 'regression':\n",
    "    param_dict = regression_params\n",
    "elif params_to_use == 'testing':\n",
    "    param_dict = testing_params\n",
    "elif params_to_use == 'multiclass':\n",
    "    param_dict = multiclass_params\n",
    "elif params_to_use == 'full_cv':\n",
    "    param_dict = full_cv_params\n",
    "\n",
    "book_df = pd.read_csv(f'{features_dir}book_df.csv')\n",
    "print(book_df.isnull().values.any())\n",
    "# book_and_averaged_chunk_df = pd.read_csv(f'{features_dir}book_and_averaged_chunk_df.csv')\n",
    "# chunk_df = pd.read_csv(f'{features_dir}chunk_df.csv')\n",
    "# chunk_and_copied_book_df = pd.read_csv(f'{features_dir}chunk_and_copied_book_df.csv')\n",
    "\n",
    "book_df = book_df.loc[book_df['file_name'] != 'Defoe_Daniel_Roxana_1724'] ########################################\n",
    "# book_and_averaged_chunk_df = book_and_averaged_chunk_df.loc[book_and_averaged_chunk_df['file_name'] != 'Defoe_Daniel_Roxana_1724']\n",
    "# chunk_df = chunk_df.loc[chunk_df['file_name'] != 'Defoe_Daniel_Roxana_1724']\n",
    "# chunk_and_copied_book_df = chunk_and_copied_book_df.loc[chunk_and_copied_book_df['file_name'] != 'Defoe_Daniel_Roxana_1724']\n",
    "\n",
    "\n",
    "for lang in [lang]:\n",
    "    for model in [param_dict['model']]:\n",
    "        model_param = model_params[model]\n",
    "        for model_param in model_param:\n",
    "            for dimensionality_reduction in [param_dict['dimensionality_reduction']]:\n",
    "                for features in param_dict['features']:\n",
    "                    for drop_columns in [drop_columns_list[3]]:\n",
    "                        print(params_to_use, lang, model, features, drop_columns, dimensionality_reduction, 'param=', model_param)\n",
    "                        try:\n",
    "                            experiment = Regression(\n",
    "                                language=lang,\n",
    "                                features=features,\n",
    "                                drop_columns=drop_columns,\n",
    "                                dimensionality_reduction = dimensionality_reduction,\n",
    "                                model_param=model_param,\n",
    "                                model=model,\n",
    "                                verbose=True,\n",
    "                                params_to_use = params_to_use)\n",
    "                            returned_values = experiment.run()\n",
    "                            results.append((lang, model, features, drop_columns, dimensionality_reduction, model_param) + returned_values)\n",
    "                        except Exception as e:\n",
    "                            print(f'Error in {lang}, {model}, {features}, {drop_columns}, {dimensionality_reduction}')\n",
    "                            print(e)\n",
    "                            raise e\n",
    "    results_df = pd.DataFrame(results, columns=['lang', 'model', 'features', 'drop_columns', \n",
    "    'dimensionality_reduction', 'model_param', 'mean_train_mse', 'mean_train_rmse', \n",
    "    'mean_train_mae', 'mean_train_r2', 'mean_train_corr', 'mean_validation_mse', 'mean_validation_rmse',\n",
    "    'mean_validation_mae', 'mean_validation_r2', 'mean_validation_corr', 'mean_p_value', 'best_features'])\n",
    "    results_df.to_csv(f'{results_dir}results-{lang}-{params_to_use}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c1b57-e071-4f4f-a1f8-82b639e94d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distances based on best features\n",
    "best_features = list(results_df.loc[0, 'best_features'].keys())\n",
    "reduced_df = book_df[['file_name'] + best_features]\n",
    "\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "matrix = pd.DataFrame(squareform(pdist(reduced_df.iloc[:,1:], metric='cosine')), index = reduced_df['file_name'], columns = reduced_df['file_name'])\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4038a-1ad6-438d-831d-ef882899717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbitrary threshold for splitting books into canonized, not canonized\n",
    "discrete_labels = labels.copy()\n",
    "discrete_labels['y'] = (discrete_labels['y']>0.5).astype(int)\n",
    "\n",
    "discrete_labels['y'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aaf2f9-ccb4-489f-83ff-373aa241ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_labels = discrete_labels[discrete_labels['file_name'].isin(matrix.columns.tolist())]\n",
    "discrete_labels['file_name']==matrix.index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
