{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b64b32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "lang = \"eng\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../src/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "features_dir = f\"../data/features/{lang}/\"\n",
    "results_dir = f\"../data/results_sentiment/classification_library/{lang}/\" ##############3############33\n",
    "sentiment_labels_dir = \"../data/labels_sentiment/\"\n",
    "canonization_labels_dir = \"../data/labels_canon/\"\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b8e470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR, SVC\n",
    "import xgboost\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBRegressor\n",
    "from copy import deepcopy\n",
    "from scipy.stats import pearsonr\n",
    "from utils import read_sentiment_scores, read_library_scores\n",
    "from math import sqrt\n",
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\") # %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import statistics\n",
    "import random\n",
    "random.seed(2)\n",
    "\n",
    "labels = read_sentiment_scores(sentiment_labels_dir, canonization_labels_dir, lang)\n",
    "library_scores = read_library_scores(sentiment_labels_dir, canonization_labels_dir, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a2338fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_name</th>\n",
       "      <th>y</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austen_Jane_Pride-and-Prejudice_1813</td>\n",
       "      <td>0.029731</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austen_Jane_Sense-and-Sensibility_1811</td>\n",
       "      <td>0.049103</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barrie_J-M_Auld-Licht-Idylls_1888</td>\n",
       "      <td>0.016107</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barrie_J-M_Sentimental-Tommy_1896</td>\n",
       "      <td>0.038327</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Beerbohm_Max_Zuleika-Dobson_1911</td>\n",
       "      <td>0.053656</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Wollstonecraft_Mary_Mary_1788</td>\n",
       "      <td>0.040233</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Scott_Walter_The-Black-Dwarf_1816</td>\n",
       "      <td>-0.021840</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Beckford_William_Vathek_1786</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Radcliffe_Ann_Udolpho_1794</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>Smollett_Tobias_Humphry-Clinker_1771</td>\n",
       "      <td>0.010482</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  book_name         y  c\n",
       "1      Austen_Jane_Pride-and-Prejudice_1813  0.029731  3\n",
       "2    Austen_Jane_Sense-and-Sensibility_1811  0.049103  3\n",
       "3         Barrie_J-M_Auld-Licht-Idylls_1888  0.016107  3\n",
       "4         Barrie_J-M_Sentimental-Tommy_1896  0.038327  3\n",
       "5          Beerbohm_Max_Zuleika-Dobson_1911  0.053656  3\n",
       "..                                      ...       ... ..\n",
       "148           Wollstonecraft_Mary_Mary_1788  0.040233  3\n",
       "170       Scott_Walter_The-Black-Dwarf_1816 -0.021840  2\n",
       "177            Beckford_William_Vathek_1786  0.006286  2\n",
       "234              Radcliffe_Ann_Udolpho_1794  0.004301  2\n",
       "241    Smollett_Tobias_Humphry-Clinker_1771  0.010482  2\n",
       "\n",
       "[191 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ece3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Labels statistics\n",
    "print(len(pd.unique(labels[\"book_name\"]))) #197\n",
    "# 254 labels, 197 different book_names -> 57 second/third... reviews\n",
    "# 36 book_names with more than 1 label, these 36 book_names have 93 labels\n",
    "# 93 = 36 first reviews + 57 second/third... reviews\n",
    "# 6 texts have opposing reviews (13 reviews are opposing)\n",
    "# 191 texts after aggregating (without opposing reviews)\n",
    "\n",
    "labels[\"y\"].plot.hist(grid=True, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a35df",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fehler\n",
    "Brooke_Frances_Emily_Montague_1769\n",
    "Lennox_Charlotte_The-Female_Quixote_1752\n",
    "Stoker_Bram-Dracula_1897\n",
    "\n",
    "Nicolai_Friedrich_Sebaldus-Nothanker-1773\n",
    "Jung-Stilling_Heinrich-Stillings-Jugend_1777\n",
    "Sacher-Masoch_Venus-im-Pelz_1869\n",
    "Hunold_Christian_Friedrich_Die-liebenswuerdige-Adalie_1681\n",
    "\n",
    "Hoffmansthal_Hugo ['Hoffmansthal_Hugo_Andreas-oder-die-Vereinigten_1907', 'Hoffmansthal_Hugo_Das-Maerchen-der-672-Nacht_1895'] \n",
    "Hoffmansthal_Hugo-von ['Hoffmansthal_Hugo-von_Ein-Brief_1902', 'Hoffmansthal_Hugo-von_Reitergeschichte_1899'] \n",
    "\n",
    "\n",
    "Anonymous anonymous\n",
    "\n",
    "df = df.loc[df[\"book_name\"] != \"Defoe_Daniel_Roxana_1724\"]\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdcaabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorSplit():\n",
    "    \"\"\"\n",
    "    Distribute book names over splits.\n",
    "    All works of an author are in the same split.\n",
    "    Adapted from https://www.titanwolf.org/Network/q/b7ee732a-7c92-4416-bc80-a2bd2ed136f1/y\n",
    "    Stevenson-Grift_Robert-Louis-Fanny-van-de_The-Dynamiter_1885\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, df, nr_splits, seed, return_indices=False):\n",
    "        self.df = df\n",
    "        self.nr_splits = nr_splits\n",
    "        self.return_indices = return_indices\n",
    "        self.book_names = df[\"book_name\"].unique()\n",
    "        self.author_bookname_mapping, self.works_per_author = self.get_author_books()\n",
    "        random.seed(seed)\n",
    "\n",
    "    def get_author_books(self):\n",
    "        authors = []\n",
    "        author_bookname_mapping = {}\n",
    "        #Get authors\n",
    "        for book_name in self.book_names:\n",
    "            author = \"_\".join(book_name.split(\"_\")[:2])\n",
    "            authors.append(author)\n",
    "            if author in author_bookname_mapping:\n",
    "                author_bookname_mapping[author].append(book_name)\n",
    "            else:\n",
    "                author_bookname_mapping[author] = []\n",
    "                author_bookname_mapping[author].append(book_name)\n",
    "                \n",
    "        # Aggregate if author has worked anlone and with others\n",
    "        if lang == \"ger\":\n",
    "            agg_dict = {\"Hoffmansthal_Hugo\": [\"Hoffmansthal_Hugo-von\"], \n",
    "                        \"Schlaf_Johannes\": [\"Holz-Schlaf_Arno-Johannes\"],\n",
    "                         \"Arnim_Bettina\": [\"Arnim-Arnim_Bettina-Gisela\"]}\n",
    "        else:\n",
    "            agg_dict = {\"Stevenson_Robert-Louis\": [\"Stevenson-Grift_Robert-Louis-Fanny-van-de\", \n",
    "                                                   \"Stevenson-Osbourne_Robert-Louis-Lloyde\"]}\n",
    "            \n",
    "        print(authors)\n",
    "        for author, aliases in agg_dict.items():\n",
    "            if author in authors:\n",
    "                print(author, aliases)\n",
    "                for alias in aliases:\n",
    "                    print(alias)\n",
    "                    author_bookname_mapping[author].extend(author_bookname_mapping[alias]) \n",
    "                    del author_bookname_mapping[alias]\n",
    "                    authors = [author for author in authors if author != alias]\n",
    "        \n",
    "        works_per_author = Counter(authors)\n",
    "        return author_bookname_mapping, works_per_author\n",
    "    \n",
    "    def split(self):\n",
    "        splits = [[] for _ in range(0,self.nr_splits)]\n",
    "        totals = [(0,i) for i in range (0, self.nr_splits)]\n",
    "        # heapify based on first element of tuple, inplace\n",
    "        heapq.heapify(totals)\n",
    "        while bool(self.works_per_author):\n",
    "            author = random.choice(list(self.works_per_author.keys()))\n",
    "            author_workcount = self.works_per_author.pop(author)\n",
    "            # find split with smallest number of books\n",
    "            total, index = heapq.heappop(totals)\n",
    "            splits[index].append(author)\n",
    "            heapq.heappush(totals, (total + author_workcount, index))\n",
    "\n",
    "        if not self.return_indices:\n",
    "            #Map author splits to book names\n",
    "            map_splits = []\n",
    "            for split in splits:\n",
    "                new = []\n",
    "                for author in split:\n",
    "                    new.extend(self.author_bookname_mapping[author])\n",
    "                map_splits.append(new)\n",
    "        else:\n",
    "            book_name_idx_mapping = dict((book_name, index) for index, book_name in enumerate(self.book_names))\n",
    "            map_splits = []\n",
    "            for split in splits:\n",
    "                test_split = []\n",
    "                for author in split:\n",
    "                    test_split.extend([book_name_idx_mapping[book_name] for book_name in  self.author_bookname_mapping[author]])\n",
    "                train_split = list(set(book_name_idx_mapping.values()) - set(test_split))\n",
    "                map_splits.append((train_split, test_split))\n",
    "        return map_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af32ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression():\n",
    "    def __init__(self, language, features, drop_columns, dimensionality_reduction, model_param, model, verbose, params_to_use):\n",
    "        assert isinstance(drop_columns, list)\n",
    "        for i in drop_columns:\n",
    "            assert isinstance(i, str)\n",
    "        assert (dimensionality_reduction in [\"k_best_f_reg_0_10\", \"k_best_mutual_info_0_10\", \"ss_pca_0_95\"]) or (dimensionality_reduction is None)\n",
    "        self._check_class_specific_assertions()\n",
    "        \n",
    "        self.language = language\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.labels = self._prepare_labels()\n",
    "        self.drop_columns = drop_columns\n",
    "        self.dimensionality_reduction = dimensionality_reduction\n",
    "        self.model_param = model_param\n",
    "        self.model = model\n",
    "        self.verbose = verbose\n",
    "        self.params_to_use = params_to_use\n",
    "\n",
    "        if self.features == \"book\":\n",
    "            self.df = deepcopy(book_df)\n",
    "        elif self.features == \"chunk\":\n",
    "            self.df = deepcopy(chunk_df)\n",
    "        elif self.features == \"chunk_and_copied_book\":\n",
    "            self.df = deepcopy(chunk_and_copied_book_df)\n",
    "        elif self.features == \"book_and_averaged_chunk\":\n",
    "            self.df = deepcopy(book_and_averaged_chunk_df)\n",
    "\n",
    "        columns_before_drop = set(self.df.columns)\n",
    "        if self.drop_columns:\n",
    "            self.df = self.df[[column for column in self.df.columns if not self._drop_column(column)]].reset_index(drop=True)\n",
    "        columns_after_drop = set(self.df.columns)\n",
    "        if self.verbose:\n",
    "            print(f\"Dropped {len(columns_before_drop - columns_after_drop)} columns.\")\n",
    "            \n",
    "    def _check_class_specific_assertions(self):\n",
    "        assert model in [\"xgboost\", \"svr\", \"lasso\"]\n",
    "        assert features in [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "    \n",
    "    def _prepare_labels(self):\n",
    "        return self.labels.drop(columns=\"c\")\n",
    "\n",
    "    def _drop_column(self, column):\n",
    "        for string in self.drop_columns:\n",
    "            if string in column:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def _custom_pca(self, train_X):\n",
    "        for i in range(5, train_X.shape[1], int((train_X.shape[1] - 5) / 10)):\n",
    "            pca = PCA(n_components=i)\n",
    "            new_train_X = pca.fit_transform(train_X)\n",
    "            if pca.explained_variance_ratio_.sum() >= 0.95:\n",
    "                break\n",
    "        return new_train_X, pca\n",
    "\n",
    "    def _select_features(self, train_X, train_y, validation_X):\n",
    "        if self.dimensionality_reduction == \"ss_pca_0_95\":\n",
    "            ss = StandardScaler()\n",
    "            train_X = ss.fit_transform(train_X)\n",
    "            validation_X = ss.transform(validation_X)\n",
    "            train_X, pca = self._custom_pca(train_X)\n",
    "            validation_X = pca.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_f_reg_0_10\":\n",
    "            k_best = SelectKBest(f_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_mutual_info_0_10\":\n",
    "            k_best = SelectKBest(mutual_info_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction is None:\n",
    "            pass\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _impute(self, train_X, validation_X):\n",
    "        imputer = KNNImputer()\n",
    "        train_X = imputer.fit_transform(train_X)\n",
    "        validation_X = imputer.transform(validation_X)\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _get_model(self, model_param, train_X=None, train_y=None, train_book_names=None, task_type=None):\n",
    "        if self.model == \"xgboost\":\n",
    "            if task_type == \"binary_classification\":\n",
    "                is_classification = True\n",
    "                class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1], y=train_y.astype(int).tolist())))\n",
    "            elif task_type == \"multiclass_classification\":\n",
    "                is_classification = True\n",
    "                class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1, 2, 3], y=train_y.astype(int).tolist())))\n",
    "            elif task_type == \"regression\":\n",
    "                is_classification = False\n",
    "            else:\n",
    "                raise Exception(\"Not a valid task_type\")\n",
    "            \n",
    "            def feval(preds, train_data):\n",
    "                labels = train_data.get_label()\n",
    "                if is_classification:\n",
    "                    labels = labels.astype(int)\n",
    "                    preds = preds.argmax(axis=1).astype(int)\n",
    "                    if task_type == \"binary_classification\":\n",
    "                        return 'acc', accuracy_score(labels, preds)\n",
    "                    elif task_type == \"multiclass_classification\":\n",
    "                        return 'f1', f1_score(labels, preds, average='macro')\n",
    "                else:\n",
    "                    return 'rmse', np.sqrt(mean_squared_error(labels, preds))\n",
    "            \n",
    "            if is_classification:\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y.astype(int), weight=[class_weights[int(i)] for i in train_y])\n",
    "            else:\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "            results = []\n",
    "            df = np.hstack((train_book_names, train_X))\n",
    "            df = pd.DataFrame(df, columns=[\"book_name\"] + [f\"col_{i}\" for i in range(train_X.shape[1])])\n",
    "            for max_depth in [2, 4, 6, 8]:\n",
    "                for learning_rate in [None, 0.01, 0.033, 0.1]:\n",
    "                    for colsample_bytree in [0.33, 0.60, 0.75]:\n",
    "                        if task_type == \"multiclass_classification\":\n",
    "                            params = {\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"n_jobs\": -1, \"objective\": \"multi:softmax\", \"num_class\": 4, \"eval_metric\": \"mlogloss\"}\n",
    "                        elif task_type == \"binary_classification\":\n",
    "                            params = {\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"n_jobs\": -1, \"objective\": \"multi:softmax\", \"num_class\": 2, \"eval_metric\": \"mlogloss\"}\n",
    "                        elif task_type == \"regression\":\n",
    "                            params = {\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"n_jobs\": -1}\n",
    "                        else:\n",
    "                            raise Exception(\"Not a valid task_type\")\n",
    "                        cv_results = xgboost.cv(\n",
    "                                        params,\n",
    "                                        dtrain,\n",
    "                                        num_boost_round=99999,\n",
    "                                        seed=42,\n",
    "                                        nfold=5,\n",
    "                                        folds=AuthorSplit(df, 5, seed=8, return_indices=True).split(),\n",
    "                                        feval=feval,\n",
    "                                        maximize=is_classification, # if classification, maximize f1/acc score.\n",
    "                                        early_stopping_rounds=10,\n",
    "                                        verbose_eval=False)\n",
    "\n",
    "                        if task_type == \"binary_classification\":\n",
    "                            nested_cv_score = cv_results.iloc[len(cv_results)-1][\"test-acc-mean\"]\n",
    "                        elif task_type == \"multiclass_classification\":\n",
    "                            nested_cv_score = cv_results.iloc[len(cv_results)-1][\"test-f1-mean\"]\n",
    "                        elif task_type == \"regression\":\n",
    "                            nested_cv_score = cv_results.iloc[len(cv_results)-1][\"test-rmse-mean\"]\n",
    "                        else:\n",
    "                            raise Exception(\"Not a valid task_type\")\n",
    "                        num_boost_round = len(cv_results)\n",
    "                        if task_type == \"multiclass_classification\":\n",
    "                            results.append({\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"num_boost_round\": num_boost_round, \"nested_cv_score\": nested_cv_score, \"objective\": \"multi:softmax\", \"num_class\": 4, \"eval_metric\": \"mlogloss\"})\n",
    "                        elif task_type == \"binary_classification\":\n",
    "                            results.append({\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"num_boost_round\": num_boost_round, \"nested_cv_score\": nested_cv_score, \"objective\": \"multi:softmax\", \"num_class\": 2, \"eval_metric\": \"mlogloss\"})\n",
    "                        elif task_type == \"regression\":\n",
    "                            results.append({\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"num_boost_round\": num_boost_round, \"nested_cv_score\": nested_cv_score})\n",
    "                        else:\n",
    "                            raise Exception(\"Not a valid task_type\")\n",
    "            best_parameters = sorted(results, key=lambda x: x[\"nested_cv_score\"], reverse=is_classification)[0]\n",
    "            return best_parameters\n",
    "        elif self.model == \"svr\":\n",
    "            return SVR(C=model_param)\n",
    "        elif self.model == \"lasso\":\n",
    "            return Lasso(alpha=model_param)\n",
    "        elif self.model == \"svc\":\n",
    "            return SVC(C=model_param, class_weight=\"balanced\")\n",
    "        \n",
    "    \n",
    "    def _get_pvalue(self, validation_corr_pvalues):\n",
    "        # Harmonic mean p-value\n",
    "        denominator = sum([1/x for x in validation_corr_pvalues])\n",
    "        mean_p_value = len(validation_corr_pvalues)/denominator\n",
    "        return mean_p_value\n",
    "    \n",
    "    def _combine_df_labels(self, df):\n",
    "        #Average of sentiscores per book\n",
    "        df = df.merge(right=self.labels, on=\"book_name\", how=\"inner\", validate=\"many_to_one\")\n",
    "        return df\n",
    "    \n",
    "    def run(self):\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        train_mses = []\n",
    "        train_maes = []\n",
    "        train_r2s = []\n",
    "        train_corrs = []\n",
    "        \n",
    "        validation_mses = []\n",
    "        validation_maes = []\n",
    "        validation_r2s = []\n",
    "        validation_corrs = []\n",
    "        validation_corr_pvalues = []\n",
    "\n",
    "        df = self.df\n",
    "        df = self._combine_df_labels(df)\n",
    "        book_names_split = AuthorSplit(df, 5, seed=2, return_indices=False).split() ## 10 folds\n",
    "        all_validation_books = []\n",
    "\n",
    "        for index, split in enumerate(book_names_split):\n",
    "            train_df = df[~df[\"book_name\"].isin(split)]\n",
    "            validation_df = df[df[\"book_name\"].isin(split)]\n",
    "            \n",
    "            train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            train_y = train_df[\"y\"].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            validation_y = validation_df[\"y\"].values.ravel()\n",
    "            train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            if self.model == \"xgboost\":\n",
    "                train_book_names = train_df[\"book_name\"].values.reshape(-1, 1)\n",
    "                best_parameters = self._get_model(self.model_param, train_X, train_y, train_book_names, task_type=\"regression\")\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "                num_boost_round = best_parameters[\"num_boost_round\"]\n",
    "                best_parameters.pop(\"nested_cv_score\")\n",
    "                best_parameters.pop(\"num_boost_round\")\n",
    "                model = xgboost.train(best_parameters,\n",
    "                                      dtrain,\n",
    "                                      num_boost_round=num_boost_round,\n",
    "                                      verbose_eval=False)\n",
    "            else:\n",
    "                model = self._get_model(self.model_param)\n",
    "                model.fit(train_X, train_y)\n",
    "            \n",
    "            train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "            validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            \n",
    "            if self.model == \"xgboost\":\n",
    "                train_books[\"yhat\"] = model.predict(xgboost.DMatrix(train_X))\n",
    "                validation_books[\"yhat\"] = model.predict(xgboost.DMatrix(validation_X))\n",
    "                \n",
    "                print(\"train preds:\", model.predict(xgboost.DMatrix(train_X)))\n",
    "                print(\"validation preds:\", model.predict(xgboost.DMatrix(validation_X)))\n",
    "            else:\n",
    "                train_books[\"yhat\"] = model.predict(train_X)\n",
    "                validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "            \n",
    "            train_books = train_books.groupby(\"book_name\").mean()\n",
    "            validation_books = validation_books.groupby(\"book_name\").mean()\n",
    "            all_validation_books.append(validation_books.reset_index())\n",
    "            \n",
    "            train_y = train_books[\"y\"].tolist()\n",
    "            train_yhat = train_books[\"yhat\"].tolist()\n",
    "            validation_y = validation_books[\"y\"].tolist()\n",
    "            validation_yhat = validation_books[\"yhat\"].tolist()\n",
    "            \n",
    "            all_labels.extend(validation_y)\n",
    "            all_predictions.extend(validation_yhat)\n",
    "            \n",
    "            train_mse = mean_squared_error(train_y, train_yhat)\n",
    "            train_mae = mean_absolute_error(train_y, train_yhat)\n",
    "            train_r2 = r2_score(train_y, train_yhat)\n",
    "            train_corr = pearsonr(train_y, train_yhat)[0]\n",
    "            \n",
    "            validation_mse = mean_squared_error(validation_y, validation_yhat)\n",
    "            validation_mae = mean_absolute_error(validation_y, validation_yhat)\n",
    "            validation_r2 = r2_score(validation_y, validation_yhat)\n",
    "            validation_corr = pearsonr(validation_y, validation_yhat)[0]\n",
    "            p_value = pearsonr(validation_y, validation_yhat)[1]\n",
    "            \n",
    "            train_mses.append(train_mse)\n",
    "            train_maes.append(train_mae)\n",
    "            train_r2s.append(train_r2)\n",
    "            train_corrs.append(train_corr)\n",
    "            \n",
    "            validation_mses.append(validation_mse)\n",
    "            validation_maes.append(validation_mae)\n",
    "            validation_r2s.append(validation_r2)\n",
    "            validation_corrs.append(validation_corr)\n",
    "            validation_corr_pvalues.append(p_value)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Fold: {index+1}, TrainMSE: {np.round(train_mse, 3)}, TrainMAE: {np.round(train_mae, 3)}, ValMSE: {np.round(validation_mse, 3)}, ValMAE: {np.round(validation_mae, 3)}, ValR2: {np.round(validation_r2, 3)}, ValCorr: {np.round(validation_corr, 3)}\")\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        \n",
    "        # Save y and y_pred for examples\n",
    "        model_info_string = f\"{self.language}-{self.model}-{self.dimensionality_reduction}-{self.features}-pram{self.model_param}-{self.params_to_use}\"\n",
    "        pd.concat(all_validation_books).to_csv(f\"{results_dir}y_yhat-{model_info_string}.csv\", index=False)\n",
    "        \n",
    "        mean_train_mse = np.mean(train_mses)\n",
    "        mean_train_rmse = np.mean([sqrt(x) for x in train_mses])\n",
    "        mean_train_mae = np.mean(train_maes)\n",
    "        mean_train_r2 = np.mean(train_r2s)\n",
    "        mean_train_corr = np.mean(train_corrs)\n",
    "        \n",
    "        mean_validation_mse = np.mean(validation_mses)\n",
    "        mean_validation_rmse = np.mean([sqrt(x) for x in validation_mses])\n",
    "        mean_validation_mae = np.mean(validation_maes)\n",
    "        mean_validation_r2 = np.mean(validation_r2s)\n",
    "        mean_validation_corr = np.mean(validation_corrs)\n",
    "        mean_p_value = self._get_pvalue(validation_corr_pvalues)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\"\"TrainMSE: {np.round(mean_train_mse, 3)}, TrainRMSE: {np.round(mean_train_rmse, 3)}, TrainMAE: {np.round(mean_train_mae, 3)}, TrainR2: {np.round(mean_train_r2, 3)}, TrainCorr: {np.round(mean_train_corr, 3)}, ValMSE: {np.round(mean_validation_mse, 3)}, ValRMSE: {np.round(mean_validation_rmse, 3)}, ValMAE: {np.round(mean_validation_mae, 3)}, ValR2: {np.round(mean_validation_r2, 3)}, ValCorr: {np.round(mean_validation_corr, 3)}, ValCorrPValue: {np.round(mean_p_value, 3)}\"\"\")\n",
    "            print(\"\\n---------------------------------------------------\\n\")\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.xticks(fontsize=15)\n",
    "            plt.yticks(fontsize=15)\n",
    "            plt.xlim([0,1])\n",
    "            plt.ylim([0,1])\n",
    "\n",
    "            plt.scatter(all_labels, all_predictions, s=6)\n",
    "            plt.xlabel(\"Canonization Scores\", fontsize=20)\n",
    "            plt.ylabel(\"Predicted Scores\", fontsize=20)\n",
    "            plt.savefig(f\"{results_dir}{model_info_string}.png\", dpi=400, bbox_inches=\"tight\")\n",
    "            plt.show();\n",
    "        return mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe2b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameter combinations\n",
    "'''\n",
    "drop_columns_list = [\n",
    "    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\"],\n",
    "    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"pos\"],\n",
    "    ]\n",
    "if lang == \"eng\":\n",
    "    drop_columns_list.extend([\n",
    "        [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\"], \n",
    "        [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\", \"pos\"]\n",
    "    ])\n",
    "    \n",
    "models = [\"svr\", \"lasso\", \"xgboost\", \"svc\"]\n",
    "model_params = {\"svr\": [1], \"lasso\": [1, 4], \"xgboost\": [None], \"svc\": [0.1, 1, 10, 100, 1000, 10000]} \n",
    "dimensionality_reduction = [\"ss_pca_0_95\", 'k_best_f_reg_0_10', 'k_best_mutual_info_0_10', [None]]\n",
    "features = [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "\n",
    "regression_params = {\"model\": [\"xgboost\"], \"dimensionality_reduction\": dimensionality_reduction[-1], \"features\": [features[0]]}\n",
    "testing_params = {\"model\": models[3], \"dimensionality_reduction\": dimensionality_reduction[-1], \n",
    "                  \"features\": [\"book\"]}\n",
    "twoclass_params = {\"model\": [\"svc\", \"xgboost\"], \"dimensionality_reduction\": dimensionality_reduction[-1], \n",
    "                  \"features\": [\"book\", \"book_and_averaged_chunk\"]}\n",
    "multiclass_params = {\"model\": [\"svc\", \"xgboost\"], \"dimensionality_reduction\": dimensionality_reduction[-1],\n",
    "                     \"features\": [\"book\", \"book_and_averaged_chunk\"]}\n",
    "library_params = {\"model\": [\"svc\", \"xgboost\"], \"dimensionality_reduction\": dimensionality_reduction[-1], \n",
    "                  \"features\": [\"book\", \"book_and_averaged_chunk\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run Regression\n",
    "'''\n",
    "results = []\n",
    "params_to_use = \"regression\" \n",
    "if params_to_use == \"regression\":\n",
    "    param_dict = regression_params\n",
    "elif params_to_use == \"testing\":\n",
    "    param_dict = testing_params\n",
    "\n",
    "#Eng: 605 books\n",
    "book_df = pd.read_csv(f\"{features_dir}book_df.csv\")\n",
    "book_and_averaged_chunk_df = pd.read_csv(f\"{features_dir}book_and_averaged_chunk_df.csv\")\n",
    "chunk_df = pd.read_csv(f\"{features_dir}chunk_df.csv\")\n",
    "chunk_and_copied_book_df = pd.read_csv(f\"{features_dir}chunk_and_copied_book_df.csv\")\n",
    "\n",
    "book_df = book_df.loc[book_df[\"book_name\"] != \"Defoe_Daniel_Roxana_1724\"] ########################################\n",
    "book_and_averaged_chunk_df = book_and_averaged_chunk_df.loc[book_and_averaged_chunk_df[\"book_name\"] != \"Defoe_Daniel_Roxana_1724\"]\n",
    "chunk_df = chunk_df.loc[chunk_df[\"book_name\"] != \"Defoe_Daniel_Roxana_1724\"]\n",
    "chunk_and_copied_book_df = chunk_and_copied_book_df.loc[chunk_and_copied_book_df[\"book_name\"] != \"Defoe_Daniel_Roxana_1724\"]\n",
    "\n",
    "\n",
    "for lang in [lang]:\n",
    "    for model in param_dict['model']:\n",
    "        model_param = model_params[model]\n",
    "        for model_param in model_param:\n",
    "            for dimensionality_reduction in param_dict[\"dimensionality_reduction\"]:\n",
    "                for features in param_dict[\"features\"]:\n",
    "                    for drop_columns in drop_columns_list:\n",
    "                        print(params_to_use, lang, model, features, drop_columns, dimensionality_reduction, 'param=', model_param)\n",
    "                        try:\n",
    "                            experiment = Regression(\n",
    "                                language=lang,\n",
    "                                features=features,\n",
    "                                drop_columns=drop_columns,\n",
    "                                dimensionality_reduction = dimensionality_reduction,\n",
    "                                model_param=model_param,\n",
    "                                model=model,\n",
    "                                verbose=True,\n",
    "                                params_to_use = params_to_use\n",
    "                            )\n",
    "                            experiment.run()\n",
    "                        mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value = experiment.run()\n",
    "                        results.append((lang, model, features, drop_columns, dimensionality_reduction, model_param, mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in {lang}, {model}, {features}, {drop_columns}, {dimensionality_reduction}\")\n",
    "                            print(e)\n",
    "                            raise e\n",
    "    results_df = pd.DataFrame(results, columns=[\"lang\", \"model\", \"features\", \"drop_columns\", \n",
    "    \"dimensionality_reduction\", \"model_param\", \"mean_train_mse\", \"mean_train_rmse\", \n",
    "    \"mean_train_mae\", \"mean_train_r2\", \"mean_train_corr\", \"mean_validation_mse\", \"mean_validation_rmse\",\n",
    "    \"mean_validation_mae\", \"mean_validation_r2\", \"mean_validation_corr\", \"mean_p_value\"])\n",
    "    results_df.to_csv(f\"{results_dir}results-{lang}-{params_to_use}'.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09330efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Classification into reviewed/not reviewed\n",
    "# '''\n",
    "\n",
    "# class TwoclassClassification(Regression):\n",
    "#     def __init__(self, language, features, drop_columns, dimensionality_reduction, model_param, model, verbose, params_to_use):\n",
    "#         super().__init__(language, features, drop_columns, dimensionality_reduction, model_param, model, verbose, params_to_use)\n",
    "\n",
    "#     def _check_class_specific_assertions(self):\n",
    "#         assert model in [\"svc\", \"xgboost\"]\n",
    "#         assert features in [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "        \n",
    "#     def _prepare_labels(self):\n",
    "#         labels = self.labels.drop(columns=\"y\").rename(columns={\"c\":\"y\"})\n",
    "#         return labels\n",
    "        \n",
    "#     def _combine_df_labels(self, df):\n",
    "#         #Reviews zum englischen Korpus beginnnen mit 1759 und decken alles bis 1914 ab\n",
    "#         agg_labels = self.labels[[\"book_name\"]].drop_duplicates()\n",
    "#         agg_labels[\"y\"] = 1\n",
    "#         df = df.merge(right=agg_labels, on=\"book_name\", how=\"left\", validate=\"many_to_one\")\n",
    "#         df[\"y\"] = df[\"y\"].fillna(value=0)\n",
    "#         #Select books written after year of first review)\n",
    "#         year = df[\"book_name\"].str.replace('-', '_').str.split('_').str[-1].astype('int64')\n",
    "#         df = df.loc[year>=min(year)]\n",
    "#         return df\n",
    "    \n",
    "#     def _get_sample_weights(self, df):\n",
    "#         # Weights for calculating accuracy \n",
    "#         chunks_per_book = df[\"book_name\"].value_counts(sort=False).rename('chunks_per_book')\n",
    "#         chunks_per_book = chunks_per_book.reset_index().rename(columns={\"index\":'book_name'})\n",
    "#         chunks_per_book[\"chunks_per_book\"] = 1/chunks_per_book[\"chunks_per_book\"]\n",
    "#         df = df.merge(right=chunks_per_book, how=\"left\", on=\"book_name\")\n",
    "#         sample_weights = df[\"chunks_per_book\"].tolist()\n",
    "#         return sample_weights\n",
    "    \n",
    "#     def _aggregate_chunk_predictions(self, df):\n",
    "#         g = df.groupby(\"book_name\")\n",
    "        \n",
    "#         # Majority vote\n",
    "#         # If one value is more common, assign it to every chunk\n",
    "#         # Therefore, accuracy is either 0 or 1\n",
    "#         # If both values are equally likely, leave them unchanged, and accuracy is 0.5\n",
    "#         def _get_mode_accuracy(group):\n",
    "#             counts = group[\"yhat\"].value_counts()\n",
    "#             if len(counts) == 1:\n",
    "#                 mode_acc = counts.index[0]\n",
    "#             else:\n",
    "#                 mode_acc = 0.5\n",
    "#             return mode_acc\n",
    "#         mode_accs = g.apply(_get_mode_accuracy).rename(\"mode_acc\").reset_index()\n",
    "#         mode_acc = mode_accs[\"mode_acc\"].mean()\n",
    "        \n",
    "#         # Average accuracy within book\n",
    "#         book_acc = g.apply(lambda group: accuracy_score(group[\"y\"], group[\"yhat\"])).mean()\n",
    "#         #Accuracy when each chunk is treated as single document\n",
    "#         chunk_acc = accuracy_score(df[\"y\"], df[\"yhat\"])#, sample_weight = self._get_sample_weights(df))\n",
    "#         return {\"mode_acc\": mode_acc, \"book_acc\": book_acc, \"chunk_acc\": chunk_acc}\n",
    "    \n",
    "#     def _split_booknames_stratified(self, df, nr_splits, return_indices=False):\n",
    "#         label_splits = []\n",
    "#         combined_splits = []\n",
    "#         # Split df into folds for each label individualls\n",
    "#         df_by_labels = df.groupby(\"y\")\n",
    "#         for name, group in df_by_labels:\n",
    "#             split = self._split_booknames(group, 5)\n",
    "#             label_splits.append(split)\n",
    "#         # Combine splits so that one split combines splits of all labels\n",
    "#         for fold in range(0, nr_splits):\n",
    "#             combined_split = []\n",
    "#             for label in range(0, len(pd.unique(df[\"y\"]))):\n",
    "#                 label_split = label_splits[label]\n",
    "#                 fold_split = label_split[fold]\n",
    "#                 combined_split.extend(fold_split)\n",
    "#             combined_splits.append(combined_split)\n",
    "#         return combined_splits                            \n",
    "                             \n",
    "#     def run(self):\n",
    "#         train_accs = []\n",
    "#         validation_accs = []\n",
    "#         df = self.df\n",
    "#         df = self._combine_df_labels(df)\n",
    "#         book_names_split_stratified = self._split_booknames_stratified(df, nr_splits=5, return_indices=False)\n",
    "#         all_validation_books = []\n",
    "\n",
    "#         for index, split in enumerate(book_names_split_stratified):\n",
    "#             train_df = df[~df[\"book_name\"].isin(split)]\n",
    "#             validation_df = df[df[\"book_name\"].isin(split)]\n",
    "#             train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "#             train_y = train_df[\"y\"].values.ravel()\n",
    "#             validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "#             validation_y = validation_df[\"y\"].values.ravel()\n",
    "#             train_X, validation_X = self._impute(train_X, validation_X)\n",
    "#             #if self.verbose:\n",
    "#             #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "#             train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "#             #if self.verbose:\n",
    "#             #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "#             if self.model == \"xgboost\":\n",
    "#                 train_book_names = train_df[\"book_name\"].values.reshape(-1, 1)\n",
    "#                 best_parameters = self._get_model(self.model_param, train_X, train_y, train_book_names, task_type=\"binary_classification\")\n",
    "#                 class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1], y=train_y.astype(int).tolist())))\n",
    "#                 dtrain = xgboost.DMatrix(train_X, label=train_y.astype(int), weight=[class_weights[int(i)] for i in train_y])\n",
    "#                 num_boost_round = best_parameters[\"num_boost_round\"]\n",
    "#                 best_parameters.pop(\"nested_cv_score\")\n",
    "#                 best_parameters.pop(\"num_boost_round\")\n",
    "#                 model = xgboost.train(best_parameters,\n",
    "#                                       dtrain,\n",
    "#                                       num_boost_round=num_boost_round,\n",
    "#                                       verbose_eval=False)\n",
    "#             else:\n",
    "#                 model = self._get_model(self.model_param)\n",
    "#                 model.fit(train_X, train_y)\n",
    "            \n",
    "#             train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "#             validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            \n",
    "#             if self.model == \"xgboost\":\n",
    "#                 train_books[\"yhat\"] = model.predict(xgboost.DMatrix(train_X))\n",
    "#                 validation_books[\"yhat\"] = model.predict(xgboost.DMatrix(validation_X))\n",
    "#             else:\n",
    "#                 train_books[\"yhat\"] = model.predict(train_X)\n",
    "#                 validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "\n",
    "#             train_acc = self._aggregate_chunk_predictions(train_books)\n",
    "#             validation_acc = self._aggregate_chunk_predictions(validation_books)\n",
    "            \n",
    "#             all_validation_books.append(validation_books)\n",
    "            \n",
    "#             train_accs.append(train_acc)\n",
    "#             validation_accs.append(validation_acc)\n",
    "        \n",
    "#         # Save y and y_pred for examples\n",
    "#         all_validation_books = pd.concat(all_validation_books)\n",
    "#         all_validation_books.to_csv(f\"{results_dir}valiationbooks-{self.params_to_use}.csv\", index=False)\n",
    "        \n",
    "#         print(confusion_matrix(all_validation_books[\"y\"], all_validation_books[\"yhat\"]))\n",
    "#         print(pd.crosstab(all_validation_books[\"y\"], all_validation_books[\"yhat\"], rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "#         train_accs = pd.DataFrame(train_accs)\n",
    "#         validation_accs = pd.DataFrame(validation_accs)\n",
    "\n",
    "#         mean_train_mode_acc = train_accs[\"mode_acc\"].mean()\n",
    "#         mean_train_book_acc = train_accs[\"book_acc\"].mean()\n",
    "#         mean_train_chunk_acc = train_accs[\"chunk_acc\"].mean()\n",
    "#         mean_validation_mode_acc = validation_accs[\"mode_acc\"].mean()\n",
    "#         mean_validation_book_acc = validation_accs[\"book_acc\"].mean()\n",
    "#         mean_validation_chunk_acc = validation_accs[\"chunk_acc\"].mean()\n",
    "#         print('validation mode, book, and chunk acc', mean_validation_mode_acc, mean_validation_book_acc, mean_validation_chunk_acc)\n",
    "\n",
    "#         return mean_train_book_acc, mean_validation_book_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f655dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' \n",
    "# Classification into in library/not in library\n",
    "# '''\n",
    "\n",
    "# class LibraryClassification(TwoclassClassification):\n",
    "#     def _combine_df_labels(self, df):\n",
    "#         df = df.merge(right=self.labels, on=\"book_name\", how=\"left\", validate=\"one_to_one\")\n",
    "#         df[\"y\"] = df[\"y\"].fillna(0)\n",
    "#         #Select books written after year first one appeared in a library catalogues\n",
    "#         df[\"year\"] = df[\"book_name\"].str.replace('-', '_').str.split('_').str[-1].astype('int64')\n",
    "#         helper_df = df.loc[df[\"y\"]!=0]\n",
    "#         first_library_year = min(helper_df[\"y\"])\n",
    "#         df = df.loc[df[\"year\"]>=first_library_year]\n",
    "#         df = df.drop(columns=\"year\")\n",
    "#         return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62768d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Classification into not reviewed/negative/not classified/positive\n",
    "# '''\n",
    "\n",
    "# class MulticlassClassification(TwoclassClassification):\n",
    "#     def __init__(self, language, features, drop_columns, dimensionality_reduction, model_param, model, verbose, params_to_use):\n",
    "#         super().__init__(language, features, drop_columns, dimensionality_reduction, model_param, model, verbose, params_to_use)\n",
    "\n",
    "#     def _check_class_specific_assertions(self):\n",
    "#         assert model in [\"svc\", \"xgboost\"]\n",
    "#         assert features in [\"book\", \"book_and_averaged_chunk\"]#, \"chunk_and_copied_book\", \"chunk\"]\n",
    "                \n",
    "#     def _combine_df_labels(self, df):\n",
    "#         #Reviews zum englischen Korpus beginnnen mit 1759 und decken alles bis 1914 ab\n",
    "#         df = df.merge(right=self.labels, on=\"book_name\", how=\"left\", validate=\"many_to_one\")\n",
    "#         df[\"y\"] = df[\"y\"].fillna(value=0)\n",
    "#         #Select books written after year of first review\n",
    "#         year = df[\"book_name\"].str.replace('-', '_').str.split('_').str[-1].astype('int64')\n",
    "#         df = df.loc[year>=min(year)]\n",
    "#         return df\n",
    "    \n",
    "#     def _evaluate_predictions(self, df):\n",
    "#         score = f1_score(df[\"y\"], df[\"yhat\"], average='macro')\n",
    "#         return score\n",
    "            \n",
    "        \n",
    "#     def run(self):\n",
    "#         train_f1s = []\n",
    "#         validation_f1s = []\n",
    "\n",
    "#         df = self.df\n",
    "#         df = self._combine_df_labels(df)\n",
    "#         book_names_split_stratified = self._split_booknames_stratified(df, nr_splits=5, return_indices=False)\n",
    "#         all_validation_books = []\n",
    "\n",
    "#         for index, split in enumerate(book_names_split_stratified):\n",
    "#             train_df = df[~df[\"book_name\"].isin(split)]\n",
    "#             validation_df = df[df[\"book_name\"].isin(split)]\n",
    "#             print(\"class distribution over dfs\")\n",
    "#             print(train_df[\"y\"].value_counts())\n",
    "#             print(validation_df[\"y\"].value_counts())\n",
    "#             #print(train_df.loc[train_df[\"y\"]==1])\n",
    "#             print(validation_df.loc[validation_df[\"y\"]==1])\n",
    "            \n",
    "#             train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "#             train_y = train_df[\"y\"].values.ravel()\n",
    "#             validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "#             validation_y = validation_df[\"y\"].values.ravel()\n",
    "#             train_X, validation_X = self._impute(train_X, validation_X)\n",
    "#             #if self.verbose:\n",
    "#             #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "#             train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "#             #if self.verbose:\n",
    "#             #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "#             if self.model == \"xgboost\":\n",
    "#                 train_book_names = train_df[\"book_name\"].values.reshape(-1, 1)\n",
    "#                 best_parameters = self._get_model(self.model_param, train_X, train_y, train_book_names, task_type=\"multiclass_classification\")\n",
    "#                 class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1, 2, 3], y=train_y.astype(int).tolist())))\n",
    "#                 dtrain = xgboost.DMatrix(train_X, label=train_y.astype(int), weight=[class_weights[int(i)] for i in train_y])\n",
    "#                 num_boost_round = best_parameters[\"num_boost_round\"]\n",
    "#                 best_parameters.pop(\"nested_cv_score\")\n",
    "#                 best_parameters.pop(\"num_boost_round\")\n",
    "#                 model = xgboost.train(best_parameters,\n",
    "#                                       dtrain,\n",
    "#                                       num_boost_round=num_boost_round,\n",
    "#                                       verbose_eval=False)\n",
    "#             else:\n",
    "#                 model = self._get_model(self.model_param)\n",
    "#                 model.fit(train_X, train_y)\n",
    "            \n",
    "#             train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "#             validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            \n",
    "#             if self.model == \"xgboost\":\n",
    "#                 train_books[\"yhat\"] = model.predict(xgboost.DMatrix(train_X))\n",
    "#                 validation_books[\"yhat\"] = model.predict(xgboost.DMatrix(validation_X))\n",
    "#             else:\n",
    "#                 train_books[\"yhat\"] = model.predict(train_X)\n",
    "#                 validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "            \n",
    "#             train_f1 = self._evaluate_predictions(train_books)\n",
    "#             validation_f1 = self._evaluate_predictions(validation_books)\n",
    "#             all_validation_books.append(validation_books)\n",
    "            \n",
    "#             train_f1s.append(train_f1)\n",
    "#             validation_f1s.append(validation_f1)\n",
    "#             if self.verbose:\n",
    "#                 print(f\"Fold: {index+1}, TrainF1: {np.round(train_f1, 3)}, ValF1: {np.round(validation_f1, 3)}\")\n",
    "        \n",
    "#         # Save y and y_pred for examples\n",
    "#         all_validation_books = pd.concat(all_validation_books)\n",
    "#        all_validation_books.to_csv(f\"{results_dir}valiationbooks-{self.params_to_use}.csv\", index=False)        \n",
    "#         print(confusion_matrix(all_validation_books[\"y\"], all_validation_books[\"yhat\"]))\n",
    "#         print(pd.crosstab(all_validation_books[\"y\"], all_validation_books[\"yhat\"], rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "#         mean_train_f1 = statistics.mean(train_f1s)\n",
    "#         mean_validation_f1 = statistics.mean(validation_f1s)\n",
    "        \n",
    "#         if self.verbose:\n",
    "#             print(f\"\"\"TrainF1: {np.round(mean_train_f1, 3)}, ValidationF1: {np.round(mean_validation_f1, 3)}\"\"\")\n",
    "#             print(\"\\n---------------------------------------------------\\n\")\n",
    "#         return mean_train_f1, mean_validation_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0889634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Run two-class classification\n",
    "# '''\n",
    "# results = []\n",
    "# params_to_use = \"library\" #\"twoclass\"\n",
    "# for lang in [lang]:\n",
    "#     if params_to_use == \"testing\":\n",
    "#         param_dict = testing_params\n",
    "#     elif params_to_use == \"twoclass\":\n",
    "#         param_dict = twoclass_params\n",
    "#     elif params_to_use == \"library\":\n",
    "#         param_dict = library_params\n",
    "#     #Eng: 606 books, 14146 chunks, 13170 chunks of books published after 1759\n",
    "#     book_df = pd.read_csv(f\"{features_dir}book_df.csv\")\n",
    "#     book_and_averaged_chunk_df = pd.read_csv(f\"{features_dir}book_and_averaged_chunk_df.csv\")\n",
    "#     chunk_df = pd.read_csv(f\"{features_dir}chunk_df.csv\")\n",
    "#     chunk_and_copied_book_df = pd.read_csv(f\"{features_dir}chunk_and_copied_book_df.csv\")\n",
    "\n",
    "#     for model in [] + param_dict['model']:\n",
    "#         print(model)\n",
    "#         model_param = model_params[model]\n",
    "#         for model_param in model_param:\n",
    "#             for dimensionality_reduction in param_dict[\"dimensionality_reduction\"]:\n",
    "#                 for features in param_dict[\"features\"]:\n",
    "#                     for drop_columns in [[\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"pos\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\", \"pos\"]]:\n",
    "#                         if params_to_use ==\"library\":\n",
    "#                             labels = library_scores\n",
    "#                             experiment = LibraryClassification(\n",
    "#                                 language=lang,\n",
    "#                                 features=features,\n",
    "#                                 drop_columns=drop_columns,\n",
    "#                                 dimensionality_reduction=dimensionality_reduction,\n",
    "#                                 model_param=model_param,\n",
    "#                                 model=model,\n",
    "#                                 verbose=True,\n",
    "#                                 params_to_use = params_to_use\n",
    "#                             )\n",
    "#                         else:\n",
    "#                             experiment = TwoclassClassification(\n",
    "#                                 language=lang,\n",
    "#                                 features=features,\n",
    "#                                 drop_columns=drop_columns,\n",
    "#                                 dimensionality_reduction=dimensionality_reduction,\n",
    "#                                 model_param=model_param,\n",
    "#                                 model=model,\n",
    "#                                 verbose=True,\n",
    "#                                 params_to_use = params_to_use\n",
    "#                             )\n",
    "#                         mean_train_book_acc, mean_validation_book_acc = experiment.run()\n",
    "#                         print(lang, model, features, drop_columns, dimensionality_reduction, 'param=', model_param, mean_train_book_acc, mean_validation_book_acc)\n",
    "#                         print('\\n-----------------------------------------------------------\\n')\n",
    "#                         results.append((lang, model, features, drop_columns, dimensionality_reduction, model_param, mean_train_book_acc, mean_validation_book_acc))\n",
    "\n",
    "#     results_df = pd.DataFrame(results, columns=[\"lang\", \"model\", \"features\", \"drop_columns\", \n",
    "#     \"dimensionality_reduction\", \"model_param\", \"mean_train_book_acc\", \"mean_validation_book_acc\"])\n",
    "#     results_df.to_csv(f\"{results_dir}results-{lang}-{params_to_use}'.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86da650",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# Run Multiclass Classification\n",
    "# '''\n",
    "# results = []\n",
    "# params_to_use = \"multiclass\" \n",
    "# for lang in [lang]:    \n",
    "#     if params_to_use == \"testing\":\n",
    "#         param_dict = testing_params\n",
    "#         model_params = {\"svr\": [1], \"lasso\": [1], \"xgboost\": [None], \"svc\": [1]} \n",
    "#     elif params_to_use == \"multiclass\":\n",
    "#         param_dict = multiclass_params\n",
    "    \n",
    "#     #Eng: 606 books, 14146 chunks, 13170 chunks of books published after 1759\n",
    "#     book_df = pd.read_csv(f\"{features_dir}book_df.csv\")\n",
    "#     book_and_averaged_chunk_df = pd.read_csv(f\"{features_dir}book_and_averaged_chunk_df.csv\")\n",
    "#     #chunk_df = pd.read_csv(f\"{features_dir}chunk_df.csv\")\n",
    "#     #chunk_and_copied_book_df = pd.read_csv(f\"{features_dir}chunk_and_copied_book_df.csv\")\n",
    "    \n",
    "#     for model in [] + param_dict['model']:\n",
    "#         model_param = model_params[model]\n",
    "#         for model_param in model_param:\n",
    "#             for dimensionality_reduction in param_dict[\"dimensionality_reduction\"]:\n",
    "#                 for features in param_dict[\"features\"]:\n",
    "#                     for drop_columns in [[\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"pos\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\", \"pos\"]]:\n",
    "#                                                 #try:\n",
    "#                         print(params_to_use, lang, model, features, drop_columns, dimensionality_reduction, 'param=', model_param)\n",
    "#                         experiment = MulticlassClassification(\n",
    "#                             language=lang,\n",
    "#                             features=features,\n",
    "#                             drop_columns=drop_columns,\n",
    "#                             dimensionality_reduction=dimensionality_reduction,\n",
    "#                             model_param=model_param,\n",
    "#                             model=model,\n",
    "#                             verbose=True,\n",
    "#                             params_to_use = params_to_use \n",
    "#                         )\n",
    "#                         mean_train_f1, mean_validation_f1 = experiment.run()\n",
    "#                         results.append((lang, model, features, drop_columns, dimensionality_reduction, model_param, mean_train_f1, mean_validation_f1))\n",
    "#                         print(params_to_use, lang, model, features, drop_columns, dimensionality_reduction, 'param=', model_param, mean_train_f1, mean_validation_f1)\n",
    "#     results_df = pd.DataFrame(results, columns=[\"lang\", \"model\", \"features\", \"drop_columns\", \n",
    "#     \"dimensionality_reduction\", \"model_param\", \"mean_train_f1\", \"mean_validation_f1\"])\n",
    "#     results_df.to_csv(f\"{results_dir}results-{lang}-{params_to_use}'.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06cf62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f1fbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240b3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ef4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
