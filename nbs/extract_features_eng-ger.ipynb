{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../src/\")\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from handcrafted_features import DocBasedFeatureExtractor, Doc2VecChunkVectorizer, CorpusBasedFeatureExtractor\n",
    "from utils import get_doc_paths, read_labels\n",
    "import pickle\n",
    "\n",
    "raw_docs_dir = \"../data/raw_docs/\"\n",
    "labels_dir = \"../data/labels/\"\n",
    "extracted_features_dir = \"../data/extracted_features/\"\n",
    "feature_extraction_path = '/home/annina/scripts/great_unread_nlp/data/feature_extraction/'\n",
    "\n",
    "lang = \"eng\" #\"ger\", \"eng\"\n",
    "doc_paths = get_doc_paths(raw_docs_dir, lang)\n",
    "sentences_per_chunk = 200\n",
    "#d2vcv = Doc2VecChunkVectorizer(lang, sentences_per_chunk)\n",
    "#d2vcv.fit_transform(reduced_doc_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name_mapper = {'eng':['Blackmore_R-D_Lorna-Doone_1869',\n",
    "#                     'Bulwer-Lytton_Edward_Paul-Clifford_1830',\n",
    "#                     'Conrad_Joseph_The-Secret-Sharer_1910',\n",
    "#                     'Parsons_Eliza_The-Castle-of-Wolfenbach_1793',\n",
    "#                     'Richardson_Samuel_Sir-Charles-Grandison_1753',\n",
    "#                     'Austen_Jane_Northanger-Abbey_1818',\n",
    "#                     'Cleland_John_Fanny-Hill_1748',\n",
    "#                     'Defoe_Daniel_Roxana_1724',\n",
    "#                     'Fielding_Henry_Amelia_1752',\n",
    "#                     'Kingsley_Charles_The-Water-Babies_1863',\n",
    "#                     'Le-Queux_William_The-Invasion-of-1910_1906',\n",
    "#                     'Surtees_Robert_Jorrocks-Jaunts-and-Jollities_1831'],\n",
    "#                     'ger': ['Eichendorff_Joseph_Auch-ich-war-in-Arkadien_1832',\n",
    "#                     'Eichendorff_Joseph_Die-Gluecksritter_1841',\n",
    "#                     'Eichendorff_Joseph_Libertas-und-ihrer-Freier_1848',\n",
    "#                     'Eichendorff_Joseph_Viel-Laermen-um-Nichts_1832',\n",
    "#                     'Goethe_Johann-Wolfgang_Unterhaltungen-deutscher-Ausgewanderten_1795',\n",
    "#                     'Zschokke_Johann_Addrich-im-Moos_1825',\n",
    "#                     'Zschokke_Johann_Der-Freihof-von-Aarau_1823',\n",
    "#                     'Zschokke_Johann_Die-Rose-von-Disentis_1844']}\n",
    "# for lang in [\"eng\", \"ger\"]:\n",
    "#     file_names = file_name_mapper[lang]\n",
    "#     print(file_names)\n",
    "#     doc_paths = ['../data/raw_docs/' + lang + '/' + filename + '.txt' for filename in file_names]\n",
    "#     print(doc_paths)\n",
    "\n",
    "    #d2vcv = Doc2VecChunkVectorizer(lang, sentences_per_chunk)\n",
    "    #d2vcv.fit_transform(reduced_doc_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_chunk_based_features = []\n",
    "# all_book_based_features = []\n",
    "# all_average_sbert_sentence_embeddings = []\n",
    "# all_doc2vec_chunk_embeddings = []\n",
    "# for index, doc_path in enumerate(doc_paths):\n",
    "#     #if there is an error with chunk_index, rerun the Doc2VecChunkVectorizer for the book where the error occurs\n",
    "#     fe = DocBasedFeatureExtractor(lang, doc_path, sentences_per_chunk)\n",
    "#     chunk_based_features, book_based_features, average_sbert_sentence_embeddings, doc2vec_chunk_embeddings = fe.get_all_features()\n",
    "#     all_chunk_based_features.extend(chunk_based_features)\n",
    "#     all_book_based_features.append(book_based_features)\n",
    "#     all_average_sbert_sentence_embeddings.append(average_sbert_sentence_embeddings)\n",
    "#     all_doc2vec_chunk_embeddings.append(doc2vec_chunk_embeddings)\n",
    "#     if index > 0 and index % 10 == 0:\n",
    "#         f = open(feature_extraction_path + lang+ '/features_' + str(index) + '.pkl', 'wb')  \n",
    "#         pickle.dump([all_chunk_based_features, all_book_based_features, all_average_sbert_sentence_embeddings, all_doc2vec_chunk_embeddings], f, -1)\n",
    "#         f.close()\n",
    "#         print('pickle', index)\n",
    "        \n",
    "# f = open(feature_extraction_path + lang+ '/all_chunk_based_features' + '.pkl', 'wb')  \n",
    "# pickle.dump(all_chunk_based_features, f, -1)\n",
    "# f.close()\n",
    "\n",
    "# f = open(feature_extraction_path + lang+ '/all_book_based_features' + '.pkl', 'wb')  \n",
    "# pickle.dump(all_book_based_features, f, -1)\n",
    "# f.close()\n",
    "\n",
    "# f = open(feature_extraction_path + lang+ '/all_average_sbert_sentence_embeddings' + '.pkl', 'wb')  \n",
    "# pickle.dump(all_average_sbert_sentence_embeddings, f, -1)\n",
    "# f.close()\n",
    "\n",
    "# f = open(feature_extraction_path + lang+ '/all_doc2vec_chunk_embeddings' + '.pkl', 'wb')  \n",
    "# pickle.dump(all_doc2vec_chunk_embeddings, f, -1)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(feature_extraction_path + lang + '/all_chunk_based_features' + '.pkl', 'rb')  \n",
    "all_chunk_based_features = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(feature_extraction_path + lang + '/all_book_based_features' + '.pkl', 'rb')  \n",
    "all_book_based_features = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(feature_extraction_path + lang + '/all_average_sbert_sentence_embeddings' + '.pkl', 'rb')  \n",
    "all_average_sbert_sentence_embeddings = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(feature_extraction_path + lang + '/all_doc2vec_chunk_embeddings' + '.pkl', 'rb')  \n",
    "all_doc2vec_chunk_embeddings = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# f = open(feature_extraction_path + lang + '/all_corpus_based_features_' + '.pkl', 'rb')  \n",
    "# all_corpus_based_features = pickle.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 606/606 [00:41<00:00, 14.70it/s]\n",
      "100%|██████████| 606/606 [02:34<00:00,  3.93it/s]\n",
      "100%|██████████| 606/606 [01:50<00:00,  5.47it/s]\n",
      "100%|██████████| 606/606 [02:34<00:00,  3.93it/s]\n",
      "100%|██████████| 606/606 [01:55<00:00,  5.24it/s]\n",
      "100%|██████████| 606/606 [00:44<00:00, 13.76it/s]\n",
      "100%|██████████| 606/606 [00:47<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606, 907)\n"
     ]
    }
   ],
   "source": [
    "cbfe = CorpusBasedFeatureExtractor(lang, doc_paths, all_average_sbert_sentence_embeddings, all_doc2vec_chunk_embeddings)\n",
    "all_corpus_based_features = cbfe.get_all_features(k=100)\n",
    "\n",
    "print(all_corpus_based_features.shape)#ger 30: (547, 726), 50: (547, 1181) #eng 30: (606, 907)\n",
    "\n",
    "all_corpus_based_features.columns\n",
    "\n",
    "f = open(feature_extraction_path + lang+ '/all_corpus_based_features' + '.pkl', 'wb')  \n",
    "pickle.dump(all_corpus_based_features, f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15330, 1800)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "book_df = pd.DataFrame(all_book_based_features)\n",
    "book_df = book_df.merge(all_corpus_based_features, on=\"book_name\")\n",
    "book_and_averaged_chunk_df = book_df.merge(pd.DataFrame(all_chunk_based_features).groupby(\"book_name\").mean().reset_index(drop=False), on=\"book_name\")\n",
    "\n",
    "chunk_df = pd.DataFrame(all_chunk_based_features)\n",
    "chunk_and_copied_book_df = chunk_df.merge(pd.DataFrame(all_book_based_features), on=\"book_name\")\n",
    "chunk_and_copied_book_df = chunk_and_copied_book_df.merge(all_corpus_based_features, on=\"book_name\")\n",
    "\n",
    "#os.makedirs(f\"{extracted_features_dir}/{lang}\", exist_ok=True)\n",
    "book_df.to_csv(f\"{extracted_features_dir}/{lang}/book_df.csv\", index=False)\n",
    "book_and_averaged_chunk_df.to_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\", index=False)\n",
    "chunk_df.to_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\", index=False)\n",
    "chunk_and_copied_book_df.to_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\", index=False)\n",
    "\n",
    "chunk_and_copied_book_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
