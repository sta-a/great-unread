{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66befa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../src/\")\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from handcrafted_features import DocBasedFeatureExtractor, Doc2VecChunkVectorizer, CorpusBasedFeatureExtractor\n",
    "from utils import get_doc_paths, read_labels\n",
    "import pickle\n",
    "\n",
    "raw_docs_dir = \"../data/raw_docs/\"\n",
    "labels_dir = \"../data/labels/\"\n",
    "extracted_features_dir = \"../data/extracted_features/\"\n",
    "\n",
    "lang = \"eng\" #\"ger\"\n",
    "doc_paths = get_doc_paths(raw_docs_dir, lang)\n",
    "\n",
    "sentences_per_chunk = 200\n",
    "#d2vcv = Doc2VecChunkVectorizer(lang, sentences_per_chunk)\n",
    "#d2vcv.fit_transform(reduced_doc_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50091bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle 10\n",
      "pickle 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annina/anaconda3/envs/nlp/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/annina/anaconda3/envs/nlp/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle 30\n",
      "pickle 40\n",
      "pickle 50\n",
      "pickle 60\n",
      "pickle 70\n",
      "pickle 80\n",
      "pickle 90\n",
      "pickle 100\n",
      "pickle 110\n",
      "pickle 120\n",
      "pickle 130\n",
      "pickle 140\n",
      "pickle 150\n",
      "pickle 160\n",
      "pickle 170\n",
      "pickle 180\n",
      "pickle 190\n",
      "pickle 200\n",
      "pickle 210\n",
      "pickle 220\n",
      "pickle 230\n",
      "pickle 240\n",
      "pickle 250\n",
      "pickle 260\n",
      "pickle 270\n",
      "pickle 280\n",
      "pickle 290\n",
      "pickle 300\n",
      "pickle 310\n",
      "pickle 320\n",
      "pickle 330\n",
      "pickle 340\n",
      "pickle 350\n",
      "pickle 360\n",
      "pickle 370\n",
      "pickle 380\n",
      "pickle 390\n",
      "pickle 400\n",
      "pickle 410\n",
      "pickle 420\n",
      "pickle 430\n",
      "pickle 440\n",
      "pickle 450\n",
      "pickle 460\n",
      "pickle 470\n",
      "pickle 480\n",
      "pickle 490\n",
      "pickle 500\n",
      "pickle 510\n",
      "pickle 520\n",
      "pickle 530\n",
      "pickle 540\n",
      "pickle 550\n",
      "pickle 560\n"
     ]
    }
   ],
   "source": [
    "# file_name_mapper = {'eng':['Blackmore_R-D_Lorna-Doone_1869',\n",
    "#                     'Bulwer-Lytton_Edward_Paul-Clifford_1830',\n",
    "#                     'Conrad_Joseph_The-Secret-Sharer_1910',\n",
    "#                     'Parsons_Eliza_The-Castle-of-Wolfenbach_1793',\n",
    "#                     'Richardson_Samuel_Sir-Charles-Grandison_1753',\n",
    "#                     'Austen_Jane_Northanger-Abbey_1818',\n",
    "#                     'Cleland_John_Fanny-Hill_1748',\n",
    "#                     'Defoe_Daniel_Roxana_1724',\n",
    "#                     'Fielding_Henry_Amelia_1752',\n",
    "#                     'Kingsley_Charles_The-Water-Babies_1863',\n",
    "#                     'Le-Queux_William_The-Invasion-of-1910_1906',\n",
    "#                     'Surtees_Robert_Jorrocks-Jaunts-and-Jollities_1831'],\n",
    "#                     'ger': ['Eichendorff_Joseph_Auch-ich-war-in-Arkadien_1832',\n",
    "#                     'Eichendorff_Joseph_Die-Gluecksritter_1841',\n",
    "#                     'Eichendorff_Joseph_Libertas-und-ihrer-Freier_1848',\n",
    "#                     'Eichendorff_Joseph_Viel-Laermen-um-Nichts_1832',\n",
    "#                     'Goethe_Johann-Wolfgang_Unterhaltungen-deutscher-Ausgewanderten_1795',\n",
    "#                     'Zschokke_Johann_Addrich-im-Moos_1825',\n",
    "#                     'Zschokke_Johann_Der-Freihof-von-Aarau_1823',\n",
    "#                     'Zschokke_Johann_Die-Rose-von-Disentis_1844']}\n",
    "# for lang in [\"eng\", \"ger\"]:\n",
    "#     file_names = file_name_mapper[lang]\n",
    "#     print(file_names)\n",
    "#     doc_paths = ['../data/raw_docs/' + lang + '/' + filename + '.txt' for filename in file_names]\n",
    "#     print(doc_paths)\n",
    "\n",
    "    #d2vcv = Doc2VecChunkVectorizer(lang, sentences_per_chunk)\n",
    "    #d2vcv.fit_transform(reduced_doc_paths)\n",
    "\n",
    "all_chunk_based_features = []\n",
    "all_book_based_features = []\n",
    "all_average_sbert_sentence_embeddings = []\n",
    "all_doc2vec_chunk_embeddings = []\n",
    "for index, doc_path in enumerate(doc_paths):\n",
    "    #if there is an error with chunk_index, rerun the Doc2VecChunkVectorizer for the book where the error occurs\n",
    "    fe = DocBasedFeatureExtractor(lang, doc_path, sentences_per_chunk)\n",
    "    chunk_based_features, book_based_features, average_sbert_sentence_embeddings, doc2vec_chunk_embeddings = fe.get_all_features()\n",
    "    all_chunk_based_features.extend(chunk_based_features)\n",
    "    all_book_based_features.append(book_based_features)\n",
    "    all_average_sbert_sentence_embeddings.append(average_sbert_sentence_embeddings)\n",
    "    all_doc2vec_chunk_embeddings.append(doc2vec_chunk_embeddings)\n",
    "    if index > 0 and index % 10 == 0:\n",
    "        feature_extraction_path = '/home/annina/scripts/great_unread_nlp/data/feature_extraction/'\n",
    "        f = open(feature_extraction_path + lang+ '/features_' + str(index) + '.pkl', 'wb')  \n",
    "        pickle.dump([all_chunk_based_features, all_book_based_features, all_average_sbert_sentence_embeddings, all_doc2vec_chunk_embeddings], f, -1)\n",
    "        f.close()\n",
    "        print('pickle', index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387226f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfe = CorpusBasedFeatureExtractor(lang, doc_paths, all_average_sbert_sentence_embeddings, all_doc2vec_chunk_embeddings)\n",
    "all_corpus_based_features = cbfe.get_all_features()\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "book_df = pd.DataFrame(all_book_based_features)\n",
    "book_df = book_df.merge(all_corpus_based_features, on=\"book_name\")\n",
    "book_and_averaged_chunk_df = book_df.merge(pd.DataFrame(all_chunk_based_features).groupby(\"book_name\").mean().reset_index(drop=False), on=\"book_name\")\n",
    "\n",
    "chunk_df = pd.DataFrame(all_chunk_based_features)\n",
    "chunk_and_copied_book_df = chunk_df.merge(pd.DataFrame(all_book_based_features), on=\"book_name\")\n",
    "chunk_and_copied_book_df = chunk_and_copied_book_df.merge(all_corpus_based_features, on=\"book_name\")\n",
    "\n",
    "os.makedirs(f\"{extracted_features_dir}/{lang}\", exist_ok=True)\n",
    "book_df.to_csv(f\"{extracted_features_dir}/{lang}/book_df.csv\", index=False)\n",
    "book_and_averaged_chunk_df.to_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\", index=False)\n",
    "chunk_df.to_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\", index=False)\n",
    "chunk_and_copied_book_df.to_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
