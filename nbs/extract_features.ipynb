{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35a9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "sys.path.insert(0, \"../src/\")\n",
    "sys.path.insert(0, \"../src/feature_extraction/\")\n",
    "from doc2vec_chunk_vectorizer import Doc2VecChunkVectorizer\n",
    "from production_rule_extractor import ProductionRuleExtractor\n",
    "from doc_based_feature_extractor import DocBasedFeatureExtractor\n",
    "from corpus_based_feature_extractor import CorpusBasedFeatureExtractor\n",
    "from utils import get_doc_paths, read_labels\n",
    "\n",
    "raw_docs_dir = \"../data/raw_docs/\"\n",
    "labels_dir = \"../data/labels/\"\n",
    "extracted_features_dir = \"../data/extracted_features/\"\n",
    "\n",
    "lang = \"eng\" #\"ger\"\n",
    "doc_paths = get_doc_paths(raw_docs_dir, lang)\n",
    "\n",
    "sentences_per_chunk = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4278e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lang in [\"eng\", \"ger\"]:\n",
    "#     doc_paths = get_doc_paths(raw_docs_dir, lang)\n",
    "#     d2vcv = Doc2VecChunkVectorizer(lang, sentences_per_chunk)\n",
    "#     d2vcv.fit_transform(doc_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33cf4656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 23/606 [01:10<19:12,  1.98s/it]/home/annina/anaconda3/envs/nlp/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/annina/anaconda3/envs/nlp/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 606/606 [26:59<00:00,  2.67s/it]  \n"
     ]
    }
   ],
   "source": [
    "all_chunk_based_features = []\n",
    "all_book_based_features = []\n",
    "all_average_sbert_sentence_embeddings = []\n",
    "all_doc2vec_chunk_embeddings = []\n",
    "\n",
    "for doc_path in tqdm(doc_paths):\n",
    "    fe = DocBasedFeatureExtractor(lang, doc_path, sentences_per_chunk)\n",
    "    chunk_based_features, book_based_features, average_sbert_sentence_embeddings, doc2vec_chunk_embeddings = fe.get_all_features()\n",
    "        \n",
    "    all_chunk_based_features.extend(chunk_based_features)\n",
    "    all_book_based_features.append(book_based_features)\n",
    "    all_average_sbert_sentence_embeddings.append(average_sbert_sentence_embeddings)\n",
    "    all_doc2vec_chunk_embeddings.append(doc2vec_chunk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eae2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(extracted_features_dir + lang + '/all_chunk_based_features' + '.pkl', 'wb')  \n",
    "pickle.dump(all_chunk_based_features, f, -1)\n",
    "f.close()\n",
    "\n",
    "f = open(extracted_features_dir + lang + '/all_book_based_features' + '.pkl', 'wb')  \n",
    "pickle.dump(all_book_based_features, f, -1)\n",
    "f.close()\n",
    "\n",
    "f = open(extracted_features_dir + lang + '/all_average_sbert_sentence_embeddings' + '.pkl', 'wb')  \n",
    "pickle.dump(all_average_sbert_sentence_embeddings, f, -1)\n",
    "f.close()\n",
    "\n",
    "# f = open(extracted_features_dir + lang + '/all_doc2vec_chunk_embeddings' + '.pkl', 'wb')  \n",
    "# pickle.dump(all_doc2vec_chunk_embeddings, f, -1)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e664b",
   "metadata": {},
   "source": [
    "f = open(extracted_features_dir + lang + '/all_chunk_based_features' + '.pkl', 'rb')  \n",
    "all_chunk_based_features = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(extracted_features_dir + lang + '/all_book_based_features' + '.pkl', 'rb')  \n",
    "all_book_based_features = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(extracted_features_dir + lang + '/all_average_sbert_sentence_embeddings' + '.pkl', 'rb')  \n",
    "all_average_sbert_sentence_embeddings = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(extracted_features_dir + lang + '/all_doc2vec_chunk_embeddings' + '.pkl', 'rb')  \n",
    "all_doc2vec_chunk_embeddings = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(extracted_features_dir + lang + '/all_corpus_based_features_' + '.pkl', 'rb')  \n",
    "all_corpus_based_features = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5013085d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 606/606 [00:52<00:00, 11.58it/s]\n",
      "  0%|          | 0/606 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram function called\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 606/606 [03:19<00:00,  3.04it/s]\n",
      "  0%|          | 0/606 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call bigram function from k most common ngrams\n",
      "bigram function called\n",
      "call bigram function from k most common ngrams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 606/606 [00:49<00:00, 12.12it/s]\n",
      "100%|██████████| 606/606 [00:53<00:00, 11.24it/s]\n",
      "3it [00:05,  1.87s/it]"
     ]
    }
   ],
   "source": [
    "cbfe = CorpusBasedFeatureExtractor(lang, doc_paths, all_average_sbert_sentence_embeddings, all_doc2vec_chunk_embeddings)\n",
    "all_corpus_based_features = cbfe.get_all_features(k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a140b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(extracted_features_dir + lang + '/all_corpus_based_features' + '.pkl', 'wb')\n",
    "pickle.dump(all_corpus_based_features, f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "book_df = pd.DataFrame(all_book_based_features)\n",
    "book_df = book_df.merge(all_corpus_based_features, on=\"book_name\")\n",
    "book_and_averaged_chunk_df = book_df.merge(pd.DataFrame(all_chunk_based_features).groupby(\"book_name\").mean().reset_index(drop=False), on=\"book_name\")\n",
    "\n",
    "chunk_df = pd.DataFrame(all_chunk_based_features)\n",
    "chunk_and_copied_book_df = chunk_df.merge(pd.DataFrame(all_book_based_features), on=\"book_name\")\n",
    "chunk_and_copied_book_df = chunk_and_copied_book_df.merge(all_corpus_based_features, on=\"book_name\")\n",
    "\n",
    "os.makedirs(f\"{extracted_features_dir}/{lang}\", exist_ok=True)\n",
    "book_df.to_csv(f\"{extracted_features_dir}/{lang}/book_df.csv\", index=False)\n",
    "book_and_averaged_chunk_df.to_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\", index=False)\n",
    "chunk_df.to_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\", index=False)\n",
    "chunk_and_copied_book_df.to_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
