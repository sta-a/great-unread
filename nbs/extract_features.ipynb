{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f514f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "lang = \"eng\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "sys.path.insert(0, \"../src/\")\n",
    "from feature_extraction.doc2vec_chunk_vectorizer import Doc2VecChunkVectorizer\n",
    "from feature_extraction.doc_based_feature_extractor import DocBasedFeatureExtractor\n",
    "from feature_extraction.corpus_based_feature_extractor import CorpusBasedFeatureExtractor\n",
    "from utils import get_doc_paths\n",
    "from chunk import Chunk\n",
    "\n",
    "raw_docs_dir = f\"../data/raw_docs/{lang}/\"\n",
    "labels_dir = \"../data/labels/\"\n",
    "features_dir = f\"../data/features-exp/{lang}/\"\n",
    "\n",
    "if not os.path.exists(features_dir):\n",
    "    os.makedirs(features_dir)\n",
    "\n",
    "doc_paths = get_doc_paths(raw_docs_dir, lang)\n",
    "\n",
    "sentences_per_chunk = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571bf7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_per_chunk = None\n",
    "# for lang in [\"eng\", \"ger\"]:\n",
    "#     doc_paths = get_doc_paths(raw_docs_dir, lang)\n",
    "#     d2vcv = Doc2VecChunkVectorizer(lang, sentences_per_chunk)\n",
    "#     d2vcv.fit_transform(doc_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f43a9-e5a2-46b8-8582-c9a3002d382d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Document-based features\n",
    "all_chunk_features = []\n",
    "all_book_features = [] #Features that must be calculated on whole book\n",
    "all_average_sbert_sentence_embeddings = []\n",
    "all_doc2vec_chunk_embeddings = []\n",
    "\n",
    "for doc_path in tqdm(doc_paths):\n",
    "    fe = DocBasedFeatureExtractor(lang, doc_path, sentences_per_chunk)\n",
    "    chunk_features, book_features, average_sbert_sentence_embeddings, doc2vec_chunk_embeddings = fe.get_all_features()  \n",
    "    all_chunk_features.extend(chunk_features)\n",
    "    all_book_features.append(book_features)\n",
    "    all_average_sbert_sentence_embeddings.append(average_sbert_sentence_embeddings)\n",
    "    all_doc2vec_chunk_embeddings.append(doc2vec_chunk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedcf76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate the chunk features for the whole book, which is considered as one chunk\n",
    "all_chunk_features_fulltext = [] #Chunk features calculated for whole book\n",
    "all_average_sbert_sentence_embeddings_fulltext = []\n",
    "all_doc2vec_chunk_embeddings_fulltext = []\n",
    "\n",
    "for doc_path in tqdm(doc_paths):\n",
    "    fe = DocBasedFeatureExtractor(lang, doc_path, sentences_per_chunk=None)\n",
    "    chunk_features_fulltext, _, average_sbert_sentence_embeddings, doc2vec_chunk_embeddings = fe.get_all_features()\n",
    "    all_chunk_features_fulltext.extend(chunk_features_fulltext)\n",
    "    all_average_sbert_sentence_embeddings_fulltext.append(average_sbert_sentence_embeddings)\n",
    "    all_doc2vec_chunk_embeddings_fulltext.append(doc2vec_chunk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad9896-9667-470a-b195-9c5908f2de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle document-based features\n",
    "with open(features_dir + 'all_chunk_features' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(all_chunk_features, f, -1)\n",
    "\n",
    "with open(features_dir + 'all_book_features' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(all_book_features, f, -1)\n",
    "\n",
    "with open(features_dir + 'all_average_sbert_sentence_embeddings' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(all_average_sbert_sentence_embeddings, f, -1)\n",
    "\n",
    "with open(features_dir + 'all_doc2vec_chunk_embeddings' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(all_doc2vec_chunk_embeddings, f, -1)\n",
    "\n",
    "with open(features_dir + 'all_chunk_features_fulltext' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(all_chunk_features_fulltext, f, -1)\n",
    "\n",
    "with open(features_dir + 'all_average_sbert_sentence_embeddings_fulltext' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(all_average_sbert_sentence_embeddings_fulltext, f, -1)\n",
    "\n",
    "with open(features_dir + 'all_doc2vec_chunk_embeddings_fulltext' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(all_doc2vec_chunk_embeddings_fulltext, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f94ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load document-based features  \n",
    "# with open(features_dir + 'all_chunk_features' + '.pkl', 'rb') as f:\n",
    "#     all_chunk_features = pickle.load(f)\n",
    "\n",
    "# with open(features_dir + 'all_book_features' + '.pkl', 'rb') as f:\n",
    "#     all_book_features = pickle.load(f)\n",
    "\n",
    "# with open(features_dir + 'all_average_sbert_sentence_embeddings' + '.pkl', 'rb') as f:\n",
    "#     all_average_sbert_sentence_embeddings = pickle.load(f)\n",
    "\n",
    "# with open(features_dir + 'all_doc2vec_chunk_embeddings' + '.pkl', 'rb') as f:\n",
    "#     all_doc2vec_chunk_embeddings = pickle.load(f)\n",
    "\n",
    "# with open(features_dir + 'all_chunk_features_fulltext' + '.pkl', 'rb') as f:\n",
    "#     all_chunk_features_fulltext = pickle.load(f)\n",
    "\n",
    "# with open(features_dir + 'all_average_sbert_sentence_embeddings_fulltext' + '.pkl', 'rb') as f:\n",
    "#     all_average_sbert_sentence_embeddings_fulltext = pickle.load(f)\n",
    "\n",
    "# with open(features_dir + 'all_doc2vec_chunk_embeddings_fulltext' + '.pkl', 'rb') as f:\n",
    "#     all_doc2vec_chunk_embeddings_fulltext = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0fc00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Corpus-based features\n",
    "cbfe = CorpusBasedFeatureExtractor(lang, doc_paths, all_average_sbert_sentence_embeddings, all_doc2vec_chunk_embeddings, sentences_per_chunk, nr_features=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94975638-f149-45ab-8961-414fc763f229",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_chunk_features, corpus_book_features = cbfe.get_all_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ff328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle corpus-based features\n",
    "with open(features_dir + 'corpus_chunk_features' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus_chunk_features, f, -1)\n",
    "\n",
    "with open(features_dir + 'corpus_book_features' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus_book_features, f, -1)\n",
    "\n",
    "# # Aggregate embeddings from chunks in cbfe instead of returning them via functions from dbfe\n",
    "# for o,n in zip(cbfe.all_average_sbert_sentence_embeddings, cbfe.new_sbert):\n",
    "#     print(len(o))\n",
    "#     print(len(n))  \n",
    "#     print(np.array_equal(o,n))\n",
    "\n",
    "# # Aggregate embeddings from chunks in cbfe instead of returning them via functions from dbfe\n",
    "# for o,n in zip(cbfe.all_doc2vec_chunk_embeddings, cbfe.new_doc2vec):\n",
    "#     print(len(o), len(n))\n",
    "#     print(np.array_equal(o,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d9926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recalculate the chunk features for the whole book, which is considered as one chunk\n",
    "cbfe_fulltext = CorpusBasedFeatureExtractor(lang, doc_paths, all_average_sbert_sentence_embeddings_fulltext, all_doc2vec_chunk_embeddings_fulltext, sentences_per_chunk=None, nr_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chunk_features_fulltext, _ = cbfe_fulltext.get_all_features()\n",
    "with open(features_dir + 'corpus_chunk_features_fulltext' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus_chunk_features_fulltext, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36caa602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load corpus-based features  \n",
    "with open(features_dir + 'corpus_chunk_features' + '.pkl', 'rb') as f:\n",
    "    corpus_chunk_features = pickle.load(f)\n",
    "\n",
    "with open(features_dir + 'corpus_book_features' + '.pkl', 'rb') as f:\n",
    "    corpus_book_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa96065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(features_dir + 'corpus_chunk_features_fulltext' + '.pkl', 'rb') as f:\n",
    "    # corpus_chunk_features_fulltext = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74664ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# book features\n",
    "all_book_features = pd.DataFrame(all_book_features)\n",
    "all_chunk_features_fulltext = pd.DataFrame(all_chunk_features_fulltext)\n",
    "book_df = all_book_features\\\n",
    "            .merge(right=all_chunk_features_fulltext, on='book_name', how='outer', validate='one_to_one')\\\n",
    "            .merge(right=corpus_book_features, on='book_name', validate='one_to_one')\\\n",
    "            .merge(right=corpus_chunk_features_fulltext, on='book_name', validate='one_to_one')\n",
    "\n",
    "#chunk features\n",
    "all_chunk_features = pd.DataFrame(all_chunk_features)\n",
    "chunk_df = all_chunk_features.merge(right=corpus_chunk_features, on='book_name', how='outer', validate='one_to_one')\n",
    "chunk_df\n",
    "\n",
    "chunk_df['book_name'] = chunk_df['book_name'].str.split('_').str[:4].str.join('_')\n",
    "\n",
    "# Combine book features and averages of chunksaveraged chunk features\n",
    "book_and_averaged_chunk_df = book_df.merge(chunk_df.groupby(\"book_name\").mean().reset_index(drop=False), on=\"book_name\")\n",
    "book_and_averaged_chunk_df\n",
    "\n",
    "chunk_and_copied_book_df = chunk_df.merge(right=book_df, on='book_name', how='outer', validate='many_to_one')\n",
    "chunk_and_copied_book_df\n",
    "\n",
    "print(book_df.shape, chunk_df.shape, book_and_averaged_chunk_df.shape, chunk_and_copied_book_df.shape)\n",
    "\n",
    "dfs = {'book_df': book_df, 'book_and_averaged_chunk_df': book_and_averaged_chunk_df, 'chunk_df': chunk_df, 'chunk_and_copied_book_df': chunk_and_copied_book_df}\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    df = df.sort_values(by='book_name', axis=0, ascending=True, na_position='first')\n",
    "    df.to_csv(f\"{features_dir}{name}.csv\", index=False)\n",
    "    \n",
    "    print(df.isnull().values.any())\n",
    "    print(df.columns[df.isna().any()].tolist())\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d1442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc37b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c3877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe7d8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822ac8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f519b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f74ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
