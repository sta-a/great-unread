{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "lang = \"eng\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "sys.path.insert(0, \"../src/\")\n",
    "from doc2vec_chunk_vectorizer import Doc2VecChunkVectorizer\n",
    "from production_rule_extractor import ProductionRuleExtractor\n",
    "from doc_based_feature_extractor import DocBasedFeatureExtractor\n",
    "from corpus_based_feature_extractor import CorpusBasedFeatureExtractor\n",
    "from utils import get_doc_paths\n",
    "\n",
    "raw_docs_dir = f\"../data/raw_docs/{lang}/\"\n",
    "labels_dir = \"../data/labels/\"\n",
    "features_dir = f\"../data/features/{lang}/\"\n",
    "\n",
    "if not os.path.exists(features_dir):\n",
    "    os.makedirs(features_dir)\n",
    "\n",
    "doc_paths = get_doc_paths(raw_docs_dir, lang)[:1]\n",
    "\n",
    "sentences_per_chunk = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_per_chunk = None\n",
    "# for lang in [\"eng\", \"ger\"]:\n",
    "#     doc_paths = get_doc_paths(raw_docs_dir, lang)\n",
    "#     d2vcv = Doc2VecChunkVectorizer(lang, sentences_per_chunk)\n",
    "#     d2vcv.fit_transform(doc_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Document-based features\n",
    "all_chunk_features = []\n",
    "all_book_features = [] #Features that must be calculated on whole book\n",
    "all_average_sbert_sentence_embeddings = []\n",
    "all_doc2vec_chunk_embeddings = []\n",
    "\n",
    "for doc_path in tqdm(doc_paths):\n",
    "    fe = DocBasedFeatureExtractor(lang, doc_path, sentences_per_chunk)\n",
    "    chunk_features, book_features, average_sbert_sentence_embeddings, doc2vec_chunk_embeddings = fe.get_all_features()  \n",
    "    all_chunk_features.extend(chunk_features)\n",
    "    all_book_features.append(book_features)\n",
    "    all_average_sbert_sentence_embeddings.append(average_sbert_sentence_embeddings)\n",
    "    all_doc2vec_chunk_embeddings.append(doc2vec_chunk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate the chunk features for the whole book, which is considered as one chunk\n",
    "all_chunk_features_fulltext = [] #Chunk features calculated for whole book\n",
    "all_average_sbert_sentence_embeddings_fulltext = []\n",
    "all_doc2vec_chunk_embeddings_fulltext = []\n",
    "\n",
    "for doc_path in tqdm(doc_paths):\n",
    "    fe = DocBasedFeatureExtractor(lang, doc_path, sentences_per_chunk=None)\n",
    "    chunk_features_fulltext, _, average_sbert_sentence_embeddings, doc2vec_chunk_embeddings = fe.get_all_features()\n",
    "    all_chunk_features_fulltext.extend(chunk_features_fulltext)\n",
    "    all_average_sbert_sentence_embeddings_fulltext.append(average_sbert_sentence_embeddings)\n",
    "    all_doc2vec_chunk_embeddings_fulltext.append(doc2vec_chunk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load document-based features\n",
    "# f = open(features_dir + 'all_chunk_features' + '.pkl', 'rb')  \n",
    "# all_chunk_features = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# f = open(features_dir + 'all_book_features' + '.pkl', 'rb')  \n",
    "# all_book_features = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# f = open(features_dir + 'all_average_sbert_sentence_embeddings' + '.pkl', 'rb')  \n",
    "# all_average_sbert_sentence_embeddings = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# f = open(features_dir + 'all_doc2vec_chunk_embeddings' + '.pkl', 'rb')  \n",
    "# all_doc2vec_chunk_embeddings = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# f = open(features_dir + 'all_chunk_features_fulltext' + '.pkl', 'rb')  \n",
    "# all_chunk_features_fulltext = pickle.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Corpus-based features\n",
    "cbfe = CorpusBasedFeatureExtractor(lang, doc_paths, all_average_sbert_sentence_embeddings, all_doc2vec_chunk_embeddings, sentences_per_chunk, nr_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chunk_features, corpus_book_features = cbfe.get_all_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate embeddings from chunks in cbfe instead of returning them via functions from dbfe\n",
    "for o,n in zip(cbfe.all_average_sbert_sentence_embeddings, cbfe.new_sbert):\n",
    "    print(len(o), len(n))\n",
    "    print(np.array_equal(o,n))\n",
    "\n",
    "# Aggregate embeddings from chunks in cbfe instead of returning them via functions from dbfe\n",
    "for o,n in zip(cbfe.all_doc2vec_chunk_embeddings, cbfe.new_doc2vec):\n",
    "    print(len(o), len(n))\n",
    "    print(np.array_equal(o,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recalculate the chunk features for the whole book, which is considered as one chunk\n",
    "cbfe_fulltext = CorpusBasedFeatureExtractor(lang, doc_paths, all_average_sbert_sentence_embeddings_fulltext, all_doc2vec_chunk_embeddings_fulltext, sentences_per_chunk=None, nr_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chunk_features_fulltext, _ = cbfe_fulltext.get_all_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate embeddings from chunks in cbfe instead of returning them via functions from dbfe\n",
    "for o,n in zip(cbfe_fulltext.all_average_sbert_sentence_embeddings, cbfe_fulltext.new_sbert):\n",
    "    print(len(o))\n",
    "    print(len(n))\n",
    "    print(np.array_equal(o,n))\n",
    "\n",
    "# Aggregate embeddings from chunks in cbfe instead of returning them via functions from dbfe\n",
    "for o,n in zip(cbfe_fulltext.all_doc2vec_chunk_embeddings, cbfe_fulltext.new_doc2vec):\n",
    "    print(len(o), len(n))\n",
    "    print(np.array_equal(o,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle corpus-based features\n",
    "f = open(features_dir + 'corpus_chunk_features' + '.pkl', 'wb')\n",
    "pickle.dump(corpus_chunk_features, f, -1)\n",
    "f.close()\n",
    "\n",
    "f = open(features_dir + 'corpus_book_features' + '.pkl', 'wb')\n",
    "pickle.dump(corpus_book_features, f, -1)\n",
    "f.close()\n",
    "\n",
    "f = open(features_dir + 'corpus_chunk_features_fulltext' + '.pkl', 'wb')\n",
    "pickle.dump(corpus_chunk_features_fulltext, f, -1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load corpus-based features\n",
    "# f = open(features_dir + 'corpus_chunk_features' + '.pkl', 'rb')  \n",
    "# corpus_chunk_features = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# f = open(features_dir + 'corpus_book_features' + '.pkl', 'rb')  \n",
    "# corpus_book_features = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# f = open(features_dir + 'corpus_chunk_features_fulltext' + '.pkl', 'rb')  \n",
    "# corpus_chunk_features_fulltext = pickle.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book features\n",
    "all_book_features = pd.DataFrame(all_book_features)\n",
    "all_chunk_features_fulltext = pd.DataFrame(all_chunk_features_fulltext)\n",
    "book_df = all_book_features\\\n",
    "            .merge(right=all_chunk_features_fulltext, on='book_name', how='outer', validate='one_to_one')\\\n",
    "            .merge(right=corpus_book_features, on='book_name', validate='one_to_one')\\\n",
    "            .merge(right=corpus_chunk_features_fulltext, on='book_name', validate='one_to_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df = pd.DataFrame(all_chunk_features_fulltext)\n",
    "book_df.to_csv(f\"{features_dir}/book_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk features\n",
    "all_chunk_features = pd.DataFrame(all_chunk_features)\n",
    "chunk_df = all_chunk_features.merge(right=corpus_chunk_features, on='book_name', how='outer', validate='one_to_one')\n",
    "chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_df['book_name'] = chunk_df['book_name'].str.split('_').str[:4].str.join('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine book features and averages of chunksaveraged chunk features\n",
    "book_and_averaged_chunk_df = book_df.merge(chunk_df.groupby(\"book_name\").mean().reset_index(drop=False), on=\"book_name\")\n",
    "book_and_averaged_chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_and_copied_book_df = chunk_df.merge(right=book_df, on='book_name', how='outer', validate='many_to_one')\n",
    "chunk_and_copied_book_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book_df.shape, chunk_df.shape, book_and_averaged_chunk_df.shape, chunk_and_copied_book_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {'book_df': book_df, 'book_and_averaged_chunk_df': book_and_averaged_chunk_df, 'chunk_df': chunk_df, 'chunk_and_copied_book_df': chunk_and_copied_book_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(book_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
