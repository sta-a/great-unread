{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vectorize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9641b7b8a33c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2VecProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvectorize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2VecVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_doc_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vectorize'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../src/\")\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from process import Doc2VecProcessor\n",
    "from vectorize import Doc2VecVectorizer\n",
    "from utils import get_doc_paths, read_labels\n",
    "\n",
    "raw_docs_dir = \"../data/raw_docs/\"\n",
    "labels_dir = \"../data/labels/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"eng\"\n",
    "raw_doc_paths = get_doc_paths(raw_docs_dir, lang)\n",
    "dp = Doc2VecProcessor(lang=lang, processed_chunk_sentence_count=None, stride=None)\n",
    "dp.process(raw_doc_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"ger\"\n",
    "raw_doc_paths = get_doc_paths(raw_docs_dir, lang)\n",
    "dp = Doc2VecProcessor(lang=lang, processed_chunk_sentence_count=None, stride=None)\n",
    "dp.process(raw_doc_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"eng\"\n",
    "raw_doc_paths = get_doc_paths(raw_docs_dir, lang)\n",
    "dp = Doc2VecProcessor(lang=lang, processed_chunk_sentence_count=500, stride=500)\n",
    "dp.process(raw_doc_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"ger\"\n",
    "raw_doc_paths = get_doc_paths(raw_docs_dir, lang)\n",
    "dp = Doc2VecProcessor(lang=lang, processed_chunk_sentence_count=500, stride=500)\n",
    "dp.process(raw_doc_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full documents + Doc2VecDMM + SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"eng\"\n",
    "processed_full_doc_paths = get_doc_paths(\"../data/processed_doc2vec_full/\", lang)\n",
    "d2vv = Doc2VecVectorizer(dm=1, dm_mean=1)\n",
    "d2vv.fit(processed_full_doc_paths)\n",
    "df = d2vv.get_doc_vectors()\n",
    "labels = read_labels(\"eng\")\n",
    "df['y'] = df[\"doc_path\"].apply(lambda x: labels[x.split(\"/\")[-1][:-4]])\n",
    "df = df.drop(columns=['doc_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "X = df.drop(columns=['y']).values\n",
    "y = df[\"y\"].values.ravel()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for index, (train_indices, validation_indices) in enumerate(kf.split(X)):\n",
    "    train_X = X[train_indices, :]\n",
    "    train_y = y[train_indices]\n",
    "    validation_X = X[validation_indices, :]\n",
    "    validation_y = y[validation_indices]\n",
    "    \n",
    "    model = SVR(C=30)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_yhat = model.predict(train_X)\n",
    "    validation_yhat = model.predict(validation_X)\n",
    "    all_labels.extend(validation_y.tolist())\n",
    "    all_predictions.extend(validation_yhat.tolist())\n",
    "    train_mse = mean_squared_error(train_y, train_yhat)\n",
    "    train_mae = mean_absolute_error(train_y, train_yhat)\n",
    "    validation_mse = mean_squared_error(validation_y, validation_yhat)\n",
    "    validation_mae = mean_absolute_error(validation_y, validation_yhat)\n",
    "    print(f\"Fold: {index+1}, TrainMSE: {train_mse}, TrainMAE: {train_mae}, ValMSE: {validation_mse}, ValMAE: {validation_mae}\")\n",
    "all_labels = np.array(all_labels)\n",
    "all_predictions = np.array(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.hist(all_labels)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "plt.scatter(all_labels, all_predictions)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunks of 500 Sentences + Doc2VecDMM + SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"eng\"\n",
    "processed_sc_500_st_500_doc_paths = get_doc_paths(\"../data/processed_doc2vec_sc_500_st_500/\", lang)\n",
    "d2vv = Doc2VecVectorizer(dm=1, dm_mean=1)\n",
    "d2vv.fit(processed_sc_500_st_500_doc_paths)\n",
    "df = d2vv.get_doc_vectors()\n",
    "labels = read_labels(\"eng\")\n",
    "df['y'] = df[\"doc_path\"].apply(lambda x: labels[x.split(\"/\")[-1][:-4].split(\"_pt\")[0]])\n",
    "df['book_name'] = df['doc_path'].apply(lambda x: x.split(\"/\")[-1][:-4].split(\"_pt\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "book_names = df['book_name'].unique()\n",
    "book_names_splitted = np.array_split(book_names, 10)\n",
    "\n",
    "for index, split in enumerate(book_names_splitted):\n",
    "    train_X = df[~df[\"book_name\"].isin(split)].drop(columns=[\"y\", \"doc_path\", \"book_name\"]).values\n",
    "    train_y = df[~df[\"book_name\"].isin(split)][\"y\"].values.ravel()\n",
    "    validation_X = df[df[\"book_name\"].isin(split)].drop(columns=[\"y\", \"doc_path\", \"book_name\"]).values\n",
    "    validation_y = df[df[\"book_name\"].isin(split)][\"y\"].values.ravel()\n",
    "    \n",
    "    model = SVR(C=30)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_yhat = model.predict(train_X)\n",
    "    validation_yhat = model.predict(validation_X)\n",
    "    all_labels.extend(validation_y.tolist())\n",
    "    all_predictions.extend(validation_yhat.tolist())\n",
    "    train_mse = mean_squared_error(train_y, train_yhat)\n",
    "    train_mae = mean_absolute_error(train_y, train_yhat)\n",
    "    validation_mse = mean_squared_error(validation_y, validation_yhat)\n",
    "    validation_mae = mean_absolute_error(validation_y, validation_yhat)\n",
    "    print(f\"Fold: {index+1}, TrainMSE: {train_mse}, TrainMAE: {train_mae}, ValMSE: {validation_mse}, ValMAE: {validation_mae}\")\n",
    "all_labels = np.array(all_labels)\n",
    "all_predictions = np.array(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.scatter(all_labels, all_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full documents + Doc2VecDBOW + SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"eng\"\n",
    "processed_full_doc_paths = get_doc_paths(\"../data/processed_doc2vec_full/\", lang)\n",
    "d2vv = Doc2VecVectorizer(dm=0, dm_mean=0)\n",
    "d2vv.fit(processed_full_doc_paths)\n",
    "df = d2vv.get_doc_vectors()\n",
    "labels = read_labels(\"eng\")\n",
    "df['y'] = df[\"doc_path\"].apply(lambda x: labels[x.split(\"/\")[-1][:-4]])\n",
    "df = df.drop(columns=['doc_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "X = df.drop(columns=['y']).values\n",
    "y = df[\"y\"].values.ravel()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for index, (train_indices, validation_indices) in enumerate(kf.split(X)):\n",
    "    train_X = X[train_indices, :]\n",
    "    train_y = y[train_indices]\n",
    "    validation_X = X[validation_indices, :]\n",
    "    validation_y = y[validation_indices]\n",
    "    \n",
    "    model = MLPRegressor(hidden_layer_sizes=(80, 50, 30, 10), activation=\"relu\", max_iter=50)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_yhat = model.predict(train_X)\n",
    "    validation_yhat = model.predict(validation_X)\n",
    "    all_labels.extend(validation_y.tolist())\n",
    "    all_predictions.extend(validation_yhat.tolist())\n",
    "    train_mse = mean_squared_error(train_y, train_yhat)\n",
    "    train_mae = mean_absolute_error(train_y, train_yhat)\n",
    "    validation_mse = mean_squared_error(validation_y, validation_yhat)\n",
    "    validation_mae = mean_absolute_error(validation_y, validation_yhat)\n",
    "    print(f\"Fold: {index+1}, TrainMSE: {train_mse}, TrainMAE: {train_mae}, ValMSE: {validation_mse}, ValMAE: {validation_mae}\")\n",
    "all_labels = np.array(all_labels)\n",
    "all_predictions = np.array(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.scatter(all_labels, all_predictions)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
