{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# sys.path.insert(0, \"../src/\")\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# from handcrafted_features import DocBasedFeatureExtractor, Doc2VecChunkVectorizer, CorpusBasedFeatureExtractor\n",
    "# from utils import get_doc_paths, read_labels\n",
    "# \n",
    "# raw_docs_dir = \"../data/raw_docs/\"\n",
    "# labels_dir = \"../data/labels/\"\n",
    "# extracted_features_dir = \"../data/extracted_features/\"\n",
    "# \n",
    "# lang = \"eng\"\n",
    "# doc_paths = get_doc_paths(raw_docs_dir, lang)\n",
    "# \n",
    "# sentences_per_chunk = 200\n",
    "# d2vcv = Doc2VecChunkVectorizer(lang, sentences_per_chunk)\n",
    "# d2vcv.fit_transform(doc_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 13/599 [01:01<33:34,  3.44s/it]  /Users/arda/conda_root/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 599/599 [41:55<00:00,  4.20s/it]  \n"
     ]
    }
   ],
   "source": [
    "# all_chunk_based_features = []\n",
    "# all_book_based_features = []\n",
    "# all_average_sbert_sentence_embeddings = []\n",
    "# all_doc2vec_chunk_embeddings = []\n",
    "# for doc_path in tqdm(doc_paths):\n",
    "#     fe = DocBasedFeatureExtractor(lang, doc_path, sentences_per_chunk)\n",
    "#     chunk_based_features, book_based_features, average_sbert_sentence_embeddings, doc2vec_chunk_embeddings = fe.get_all_features()\n",
    "#     all_chunk_based_features.extend(chunk_based_features)\n",
    "#     all_book_based_features.append(book_based_features)\n",
    "#     all_average_sbert_sentence_embeddings.append(average_sbert_sentence_embeddings)\n",
    "#     all_doc2vec_chunk_embeddings.append(doc2vec_chunk_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 599/599 [05:15<00:00,  1.90it/s]\n",
      "100%|██████████| 599/599 [02:54<00:00,  3.43it/s]\n",
      "100%|██████████| 599/599 [00:47<00:00, 12.64it/s]\n",
      "100%|██████████| 599/599 [00:52<00:00, 11.47it/s]\n",
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.1\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.1\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamulticore:running online LDA training, 10 topics, 2 passes over the supplied corpus of 599 documents, updating every 6000 documents, evaluating every ~599 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamulticore:training LDA model using 3 processes\n",
      "INFO:gensim.models.ldamulticore:PROGRESS: pass 0, dispatched chunk #0 = documents up to #599/599, outstanding queue size 1\n",
      "DEBUG:gensim.models.ldamodel:updating topics\n",
      "INFO:gensim.models.ldamodel:topic #6 (0.100): 0.011*\"cecilia\" + 0.007*\"helen\" + 0.004*\"lucy\" + 0.004*\"jones\" + 0.003*\"sophia\" + 0.003*\"tho\" + 0.003*\"emily\" + 0.002*\"charlotte\" + 0.002*\"allan\" + 0.002*\"aurora\"\n",
      "INFO:gensim.models.ldamodel:topic #9 (0.100): 0.013*\"laura\" + 0.008*\"molly\" + 0.006*\"guy\" + 0.005*\"philip\" + 0.004*\"amy\" + 0.003*\"harriet\" + 0.003*\"roger\" + 0.003*\"caroline\" + 0.003*\"alice\" + 0.003*\"catherine\"\n",
      "INFO:gensim.models.ldamodel:topic #8 (0.100): 0.019*\"margaret\" + 0.003*\"maggie\" + 0.003*\"carlyle\" + 0.003*\"lucy\" + 0.003*\"edwin\" + 0.003*\"hale\" + 0.002*\"isabel\" + 0.002*\"flora\" + 0.002*\"ruth\" + 0.002*\"michael\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.100): 0.004*\"lucy\" + 0.003*\"felix\" + 0.003*\"sophia\" + 0.003*\"kate\" + 0.003*\"oliver\" + 0.003*\"camilla\" + 0.002*\"bella\" + 0.002*\"constance\" + 0.002*\"hugh\" + 0.002*\"catherine\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.100): 0.009*\"alice\" + 0.006*\"emily\" + 0.004*\"holmes\" + 0.003*\"walter\" + 0.003*\"kate\" + 0.003*\"catherine\" + 0.003*\"betsy\" + 0.003*\"carlyle\" + 0.002*\"mowbray\" + 0.002*\"flora\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.671270, rho=1.000000\n",
      "DEBUG:gensim.models.ldamodel:bound: at document #0\n",
      "INFO:gensim.models.ldamodel:-9.360 per-word bound, 656.9 perplexity estimate based on a held-out corpus of 599 documents with 2271969 words\n",
      "INFO:gensim.models.ldamulticore:PROGRESS: pass 1, dispatched chunk #0 = documents up to #599/599, outstanding queue size 1\n",
      "DEBUG:gensim.models.ldamodel:updating topics\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.100): 0.007*\"orlando\" + 0.007*\"clara\" + 0.006*\"lucy\" + 0.005*\"hae\" + 0.004*\"marquis\" + 0.004*\"emily\" + 0.004*\"mortimer\" + 0.003*\"nancy\" + 0.003*\"ned\" + 0.002*\"michael\"\n",
      "INFO:gensim.models.ldamodel:topic #8 (0.100): 0.030*\"margaret\" + 0.006*\"carlyle\" + 0.006*\"maggie\" + 0.005*\"norman\" + 0.005*\"flora\" + 0.005*\"isabel\" + 0.004*\"blanche\" + 0.004*\"patrick\" + 0.004*\"arnold\" + 0.004*\"barbara\"\n",
      "INFO:gensim.models.ldamodel:topic #7 (0.100): 0.006*\"vincent\" + 0.005*\"hae\" + 0.004*\"edwin\" + 0.004*\"weel\" + 0.004*\"maggie\" + 0.004*\"tae\" + 0.003*\"magdalen\" + 0.003*\"ann\" + 0.003*\"juliet\" + 0.003*\"susan\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.100): 0.006*\"kate\" + 0.006*\"lucy\" + 0.006*\"sophia\" + 0.005*\"felix\" + 0.005*\"nicholas\" + 0.005*\"constance\" + 0.005*\"oliver\" + 0.004*\"toby\" + 0.004*\"bella\" + 0.003*\"harold\"\n",
      "INFO:gensim.models.ldamodel:topic #6 (0.100): 0.014*\"cecilia\" + 0.008*\"helen\" + 0.006*\"jones\" + 0.006*\"lucy\" + 0.006*\"tho\" + 0.004*\"allan\" + 0.004*\"sophia\" + 0.004*\"harriet\" + 0.003*\"charlotte\" + 0.003*\"emily\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.253940, rho=0.659452\n",
      "DEBUG:gensim.models.ldamodel:bound: at document #0\n",
      "INFO:gensim.models.ldamodel:-9.170 per-word bound, 576.1 perplexity estimate based on a held-out corpus of 599 documents with 2271969 words\n",
      "DEBUG:gensim.utils:starting a new internal lifecycle event log for LdaMulticore\n",
      "INFO:gensim.utils:LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=18478, num_topics=10, decay=0.5, chunksize=2000) in 30.84s', 'datetime': '2021-06-23T22:06:35.898717', 'gensim': '4.0.1', 'python': '3.8.5 (default, Sep  4 2020, 02:22:02) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.15.4-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# cbfe = CorpusBasedFeatureExtractor(lang, doc_paths, all_average_sbert_sentence_embeddings, all_doc2vec_chunk_embeddings)\n",
    "# all_corpus_based_features = cbfe.get_all_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# \n",
    "# book_df = pd.DataFrame(all_book_based_features)\n",
    "# book_df = book_df.merge(all_corpus_based_features, on=\"book_name\")\n",
    "# book_and_averaged_chunk_df = book_df.merge(pd.DataFrame(all_chunk_based_features).groupby(\"book_name\").mean().reset_index(drop=False), on=\"book_name\")\n",
    "# \n",
    "# chunk_df = pd.DataFrame(all_chunk_based_features)\n",
    "# chunk_and_copied_book_df = chunk_df.merge(pd.DataFrame(all_book_based_features), on=\"book_name\")\n",
    "# chunk_and_copied_book_df = chunk_and_copied_book_df.merge(all_corpus_based_features, on=\"book_name\")\n",
    "# \n",
    "# os.makedirs(f\"{extracted_features_dir}/{lang}\", exist_ok=True)\n",
    "# book_df.to_csv(f\"{extracted_features_dir}/{lang}/book_df.csv\", index=False)\n",
    "# book_and_averaged_chunk_df.to_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\", index=False)\n",
    "# chunk_df.to_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\", index=False)\n",
    "# chunk_and_copied_book_df.to_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "extracted_features_dir = \"../data/extracted_features/\"\n",
    "labels_dir = \"../data/labels/\"\n",
    "lang = \"eng\"\n",
    "\n",
    "book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_df.csv\")\n",
    "book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from utils import read_labels\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "labels = read_labels(labels_dir, lang)\n",
    "\n",
    "class Experiment(object):\n",
    "    def __init__(self, features, drop_columns_including, dimensionality_reduction, model, verbose):\n",
    "        assert features in [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "        assert isinstance(drop_columns_including, list)\n",
    "        for i in drop_columns_including:\n",
    "            assert isinstance(i, str)\n",
    "        assert model in [\"xgboost\", \"svr\", \"lasso\"]\n",
    "        assert (dimensionality_reduction in [\"ss_pca_0_95\", \"k_best_f_reg_0_10\", \"k_best_mutual_info_0_10\"]) or (dimensionality_reduction is None)\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.drop_columns_including = drop_columns_including\n",
    "        self.dimensionality_reduction = dimensionality_reduction\n",
    "        self.model = model\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if features == \"book\":\n",
    "            self.df = deepcopy(book_df)\n",
    "        elif features == \"chunk\":\n",
    "            self.df = deepcopy(chunk_df)\n",
    "        elif features == \"chunk_and_copied_book\":\n",
    "            self.df = deepcopy(chunk_and_copied_book_df)\n",
    "        elif features == \"book_and_averaged_chunk\":\n",
    "            self.df = deepcopy(book_and_averaged_chunk_df)\n",
    "\n",
    "        columns_before_drop = set(self.df.columns)\n",
    "        self.df = self.df[[column for column in self.df.columns if not self._drop_column(column)]].reset_index(drop=True)\n",
    "        columns_after_drop = set(self.df.columns)\n",
    "        if self.verbose:\n",
    "            print(f\"Dropped {len(columns_before_drop - columns_after_drop)} columns.\")\n",
    "        self.df.loc[:, \"y\"] = self.df.book_name.apply(lambda x: self.labels[x]).tolist()\n",
    "\n",
    "    def _drop_column(self, column):\n",
    "        for string in self.drop_columns_including:\n",
    "            if string in column:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def _custom_pca(self, train_X):\n",
    "        for i in range(5, train_X.shape[1], int((train_X.shape[1] - 5) / 10)):\n",
    "            pca = PCA(n_components=i)\n",
    "            new_train_X = pca.fit_transform(train_X)\n",
    "            if pca.explained_variance_ratio_.sum() >= 0.95:\n",
    "                break\n",
    "        return new_train_X, pca\n",
    "\n",
    "    def _select_features(self, train_X, train_y, validation_X):\n",
    "        if self.dimensionality_reduction == \"ss_pca_0_95\":\n",
    "            ss = StandardScaler()\n",
    "            train_X = ss.fit_transform(train_X)\n",
    "            validation_X = ss.transform(validation_X)\n",
    "            train_X, pca = self._custom_pca(train_X)\n",
    "            validation_X = pca.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_f_reg_0_10\":\n",
    "            k_best = SelectKBest(f_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_mutual_info_0_10\":\n",
    "            k_best = SelectKBest(mutual_info_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction is None:\n",
    "            pass\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _impute(self, train_X, validation_X):\n",
    "        imputer = KNNImputer()\n",
    "        train_X = imputer.fit_transform(train_X)\n",
    "        validation_X = imputer.transform(validation_X)\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _get_model(self):\n",
    "        # if any of these performs better than others, we can try to tune the hyperparameters\n",
    "        # but I think for now it's more important to see which approach performs better\n",
    "        # chunk based or doc based\n",
    "        # use dimensionality reduction or not...\n",
    "        if self.model == \"xgboost\":\n",
    "            return XGBRegressor()\n",
    "        elif self.model == \"svr\":\n",
    "            return SVR()\n",
    "        elif self.model == \"lasso\":\n",
    "            return Lasso()\n",
    "            \n",
    "    def run(self):\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        train_mses = []\n",
    "        train_maes = []\n",
    "        validation_mses = []\n",
    "        validation_maes = []\n",
    "\n",
    "        df = self.df\n",
    "        book_names = df['book_name'].unique()\n",
    "        book_names_splitted = np.array_split(book_names, 10)\n",
    "        for index, split in enumerate(book_names_splitted):\n",
    "            train_df = df[~df[\"book_name\"].isin(split)]\n",
    "            validation_df = df[df[\"book_name\"].isin(split)]\n",
    "            train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            train_y = train_df[\"y\"].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            validation_y = validation_df[\"y\"].values.ravel()\n",
    "            train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            if self.verbose:\n",
    "                print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "            if self.verbose:\n",
    "                print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            model = self._get_model()\n",
    "            model.fit(train_X, train_y)\n",
    "            \n",
    "            train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "            train_books[\"yhat\"] = model.predict(train_X)\n",
    "            validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "            \n",
    "            train_books = train_books.groupby(\"book_name\").mean()\n",
    "            validation_books = validation_books.groupby(\"book_name\").mean()\n",
    "            \n",
    "            train_y = train_books[\"y\"].tolist()\n",
    "            train_yhat = train_books[\"yhat\"].tolist()\n",
    "            validation_y = validation_books[\"y\"].tolist()\n",
    "            validation_yhat = validation_books[\"yhat\"].tolist()\n",
    "            \n",
    "            all_labels.extend(validation_y)\n",
    "            all_predictions.extend(validation_yhat)\n",
    "            \n",
    "            train_mse = mean_squared_error(train_y, train_yhat)\n",
    "            train_mae = mean_absolute_error(train_y, train_yhat)\n",
    "            validation_mse = mean_squared_error(validation_y, validation_yhat)\n",
    "            validation_mae = mean_absolute_error(validation_y, validation_yhat)\n",
    "            train_mses.append(train_mse)\n",
    "            train_maes.append(train_mae)\n",
    "            validation_mses.append(validation_mse)\n",
    "            validation_maes.append(validation_mae)\n",
    "            if self.verbose:\n",
    "                print(f\"Fold: {index+1}, TrainMSE: {np.round(train_mse, 3)}, TrainMAE: {np.round(train_mae, 3)}, ValMSE: {np.round(validation_mse, 3)}, ValMAE: {np.round(validation_mae, 3)}\")\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "\n",
    "        mean_train_mse = np.mean(train_mses)\n",
    "        mean_train_mae = np.mean(train_maes)\n",
    "        mean_validation_mse = np.mean(validation_mses)\n",
    "        mean_validation_mae = np.mean(validation_maes)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"------\")\n",
    "            print(f\"Mean scores, TrainMSE: {np.round(mean_train_mse, 3)}, TrainMAE: {np.round(mean_train_mae, 3)}, ValMSE: {np.round(mean_validation_mse, 3)}, ValMAE: {np.round(mean_validation_mae, 3)}\")\n",
    "\n",
    "            plt.figure(figsize=(18, 6))\n",
    "            plt.scatter(all_labels, all_predictions)\n",
    "            plt.xlabel(\"Ground Truths\")\n",
    "            plt.ylabel(\"Predictions\")\n",
    "\n",
    "            plt.show();\n",
    "        return mean_train_mse, mean_train_mae, mean_validation_mse, mean_validation_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost book ['doc2vec_chunk_embedding'] k_best_f_reg_0_10 0.0001331986638318581 0.00781272138513123 497.4080857867046 17.596386708955617\n",
      "xgboost book ['doc2vec_chunk_embedding'] ss_pca_0_95 1.9417901707297977e-07 0.00030323384584537473 548.6991066855837 18.76817986940835\n",
      "xgboost book ['doc2vec_chunk_embedding'] k_best_mutual_info_0_10 0.0001803956307178797 0.00867681814712686 522.5112248261811 18.108052991031606\n",
      "xgboost book ['doc2vec_chunk_embedding'] None 2.3294693816108048e-07 0.0003448382162276392 526.6013000346694 18.112050449598218\n",
      "xgboost book ['average_sentence_embedding'] k_best_f_reg_0_10 0.0001331986638318581 0.00781272138513123 497.4080857867046 17.596386708955617\n",
      "xgboost book ['average_sentence_embedding'] ss_pca_0_95 1.8744597843288232e-07 0.0002992014086068399 555.2859491844622 19.1250903861175\n",
      "xgboost book ['average_sentence_embedding'] k_best_mutual_info_0_10 0.00015988937505826816 0.008340205812670382 519.8295855023355 17.982910718196113\n",
      "xgboost book ['average_sentence_embedding'] None 2.3294693816108048e-07 0.0003448382162276392 526.6013000346694 18.112050449598218\n",
      "xgboost book ['average_sentence_embedding', 'doc2vec_chunk_embedding'] k_best_f_reg_0_10 0.0001331986638318581 0.00781272138513123 497.4080857867046 17.596386708955617\n",
      "xgboost book ['average_sentence_embedding', 'doc2vec_chunk_embedding'] ss_pca_0_95 2.0100682815112649e-07 0.00030979019050155393 526.7558928133119 18.45558923632582\n",
      "xgboost book ['average_sentence_embedding', 'doc2vec_chunk_embedding'] k_best_mutual_info_0_10 0.00016771115096667156 0.008796961611295348 534.7709412725848 17.99882564638905\n",
      "xgboost book ['average_sentence_embedding', 'doc2vec_chunk_embedding'] None 2.3294693816108048e-07 0.0003448382162276392 526.6013000346694 18.112050449598218\n",
      "xgboost book [] k_best_f_reg_0_10 0.0001331986638318581 0.00781272138513123 497.4080857867046 17.596386708955617\n",
      "xgboost book [] ss_pca_0_95 1.9834624125785702e-07 0.0003050852163104542 521.3293953183779 18.29225787353952\n",
      "xgboost book [] k_best_mutual_info_0_10 0.0001736054527427806 0.008907382525809377 534.4924773607072 17.814090449406876\n",
      "xgboost book [] None 2.3294693816108048e-07 0.0003448382162276392 526.6013000346694 18.112050449598218\n",
      "xgboost chunk ['doc2vec_chunk_embedding'] k_best_f_reg_0_10 9.944931248912381 2.4040864654070546 530.2001763607252 18.6708195109341\n",
      "xgboost chunk ['doc2vec_chunk_embedding'] ss_pca_0_95 20.167413312581964 3.402723642218897 559.3393134061695 19.363748325527407\n",
      "xgboost chunk ['doc2vec_chunk_embedding'] k_best_mutual_info_0_10 9.944931248912381 2.4040864654070546 530.2001763607252 18.6708195109341\n",
      "xgboost chunk ['doc2vec_chunk_embedding'] None 9.944931248912381 2.4040864654070546 530.2001763607252 18.6708195109341\n",
      "xgboost chunk ['average_sentence_embedding'] k_best_f_reg_0_10 27.3390483023823 3.889641437494194 445.77287502133976 17.126175214664848\n",
      "xgboost chunk ['average_sentence_embedding'] ss_pca_0_95 31.585095559378892 4.23336599223618 476.37771112881654 17.64726521295174\n",
      "xgboost chunk ['average_sentence_embedding'] k_best_mutual_info_0_10 27.3390483023823 3.889641437494194 445.77287502133976 17.126175214664848\n",
      "xgboost chunk ['average_sentence_embedding'] None 27.3390483023823 3.889641437494194 445.77287502133976 17.126175214664848\n",
      "xgboost chunk ['average_sentence_embedding', 'doc2vec_chunk_embedding'] k_best_f_reg_0_10 135.01023291714023 9.127762389663397 575.3000509729166 19.690168565436373\n",
      "xgboost chunk ['average_sentence_embedding', 'doc2vec_chunk_embedding'] ss_pca_0_95 196.43717134500767 11.173617764317331 593.2482253220518 20.18421561324135\n",
      "xgboost chunk ['average_sentence_embedding', 'doc2vec_chunk_embedding'] k_best_mutual_info_0_10 135.01023291714023 9.127762389663397 575.3000509729166 19.690168565436373\n",
      "xgboost chunk ['average_sentence_embedding', 'doc2vec_chunk_embedding'] None 135.01023291714023 9.127762389663397 575.3000509729166 19.690168565436373\n",
      "xgboost chunk [] k_best_f_reg_0_10 8.640003728918975 2.203708195709749 480.172871233295 17.684973110774216\n",
      "xgboost chunk [] ss_pca_0_95 14.726949960171122 2.8926969007227172 519.2126896489506 18.685185709997675\n",
      "xgboost chunk [] k_best_mutual_info_0_10 8.640003728918975 2.203708195709749 480.172871233295 17.684973110774216\n",
      "xgboost chunk [] None 8.640003728918975 2.203708195709749 480.172871233295 17.684973110774216\n",
      "xgboost book_and_averaged_chunk ['doc2vec_chunk_embedding'] k_best_f_reg_0_10 2.7860316351036073e-05 0.00335252648372485 557.3638167099305 18.39403181708851\n",
      "xgboost book_and_averaged_chunk ['doc2vec_chunk_embedding'] ss_pca_0_95 1.4000996536262036e-07 0.000262359601570046 583.3937444142796 19.44662059681686\n",
      "xgboost book_and_averaged_chunk ['doc2vec_chunk_embedding'] k_best_mutual_info_0_10 0.0003026414552308815 0.011200846060887757 475.23785396509794 17.186033464256372\n",
      "xgboost book_and_averaged_chunk ['doc2vec_chunk_embedding'] None 1.1566599906765159e-07 0.0002475606991854105 515.2144455829268 17.904899765239275\n",
      "xgboost book_and_averaged_chunk ['average_sentence_embedding'] k_best_f_reg_0_10 0.0001475553993088927 0.00797923405351043 527.9253377261086 18.10762515951226\n",
      "xgboost book_and_averaged_chunk ['average_sentence_embedding'] ss_pca_0_95 1.7945981038143955e-07 0.0002918097193971788 514.2579469365695 18.040574427099227\n",
      "xgboost book_and_averaged_chunk ['average_sentence_embedding'] k_best_mutual_info_0_10 0.00018310352168808439 0.009158016469350504 503.84726057696054 17.62622566062872\n",
      "xgboost book_and_averaged_chunk ['average_sentence_embedding'] None 2.0713287421688625e-07 0.0003266869823194151 468.17357539949825 16.85706093837228\n",
      "xgboost book_and_averaged_chunk ['average_sentence_embedding', 'doc2vec_chunk_embedding'] k_best_f_reg_0_10 0.00016903357464159502 0.008723572461838134 498.16083136262785 17.46634694691665\n",
      "xgboost book_and_averaged_chunk ['average_sentence_embedding', 'doc2vec_chunk_embedding'] ss_pca_0_95 1.858995651030883e-07 0.00029575132270508197 516.193622006975 18.104190296532344\n",
      "xgboost book_and_averaged_chunk ['average_sentence_embedding', 'doc2vec_chunk_embedding'] k_best_mutual_info_0_10 0.00016839724664405783 0.008873730713041086 513.743368836478 17.929319221636685\n",
      "xgboost book_and_averaged_chunk ['average_sentence_embedding', 'doc2vec_chunk_embedding'] None 1.9239365373467861e-07 0.0003180516369672234 507.6090949217831 17.59580631071422\n",
      "xgboost book_and_averaged_chunk [] k_best_f_reg_0_10 3.1610153374693246e-05 0.0036424337385251686 524.0677791227336 18.195716759489805\n",
      "xgboost book_and_averaged_chunk [] ss_pca_0_95 1.3390562341129796e-07 0.00025518597188888437 553.3322552347047 19.06496496410699\n",
      "xgboost book_and_averaged_chunk [] k_best_mutual_info_0_10 0.00015826587007625645 0.008316592220634826 506.5118511938672 17.414327765083662\n",
      "xgboost book_and_averaged_chunk [] None 1.1383328996196473e-07 0.00024773886145432476 502.7611871355986 17.581957506152285\n",
      "xgboost chunk_and_copied_book ['doc2vec_chunk_embedding'] k_best_f_reg_0_10 8.116994939289012e-05 0.005225416021141796 535.0101567410436 18.183783702290977\n",
      "xgboost chunk_and_copied_book ['doc2vec_chunk_embedding'] ss_pca_0_95 1.1745095684086073 0.7984319566141769 511.77368273803177 18.456869966055812\n",
      "xgboost chunk_and_copied_book ['doc2vec_chunk_embedding'] k_best_mutual_info_0_10 8.116994939289012e-05 0.005225416021141796 535.0101567410436 18.183783702290977\n",
      "xgboost chunk_and_copied_book ['doc2vec_chunk_embedding'] None 8.116994939289012e-05 0.005225416021141796 535.0101567410436 18.183783702290977\n",
      "xgboost chunk_and_copied_book ['average_sentence_embedding'] k_best_f_reg_0_10 5.013367173266967e-05 0.004061764150831211 528.3579629492403 18.100262433892365\n",
      "xgboost chunk_and_copied_book ['average_sentence_embedding'] ss_pca_0_95 0.2053975246911087 0.30848776088040114 491.8979316992119 17.544971444350953\n",
      "xgboost chunk_and_copied_book ['average_sentence_embedding'] k_best_mutual_info_0_10 5.013367173266967e-05 0.004061764150831211 528.3579629492403 18.100262433892365\n",
      "xgboost chunk_and_copied_book ['average_sentence_embedding'] None 5.013367173266967e-05 0.004061764150831211 528.3579629492403 18.100262433892365\n",
      "xgboost chunk_and_copied_book ['average_sentence_embedding', 'doc2vec_chunk_embedding'] k_best_f_reg_0_10 4.2247544900260794e-05 0.0037746997660272367 533.9720578643366 18.22458935880265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost chunk_and_copied_book ['average_sentence_embedding', 'doc2vec_chunk_embedding'] ss_pca_0_95 0.058006408082086215 0.1546833677470367 530.7842588294244 18.5082530411777\n",
      "xgboost chunk_and_copied_book ['average_sentence_embedding', 'doc2vec_chunk_embedding'] k_best_mutual_info_0_10 4.2247544900260794e-05 0.0037746997660272367 533.9720578643366 18.22458935880265\n",
      "xgboost chunk_and_copied_book ['average_sentence_embedding', 'doc2vec_chunk_embedding'] None 4.2247544900260794e-05 0.0037746997660272367 533.9720578643366 18.22458935880265\n",
      "xgboost chunk_and_copied_book [] k_best_f_reg_0_10 9.940350901856796e-05 0.005869913853120751 528.7035362252121 18.072510760377778\n",
      "xgboost chunk_and_copied_book [] ss_pca_0_95 1.341308862411226 0.8556929400277353 489.1217990042008 18.08186397315982\n",
      "xgboost chunk_and_copied_book [] k_best_mutual_info_0_10 9.940350901856796e-05 0.005869913853120751 528.7035362252121 18.072510760377778\n",
      "xgboost chunk_and_copied_book [] None 9.940350901856796e-05 0.005869913853120751 528.7035362252121 18.072510760377778\n",
      "lasso book ['doc2vec_chunk_embedding'] k_best_f_reg_0_10 547.3455481609965 19.390907135087303 569.65531117966 19.765545234823104\n",
      "lasso book ['doc2vec_chunk_embedding'] ss_pca_0_95 392.5465843584011 15.770771583114376 485.0481390643463 17.676155731950175\n",
      "lasso book ['doc2vec_chunk_embedding'] k_best_mutual_info_0_10 573.3812647539227 19.768003594580943 593.7630084325851 20.070827349347645\n",
      "lasso book ['doc2vec_chunk_embedding'] None 470.698982495357 17.513889936901766 560.6522295592468 19.15321176579791\n",
      "lasso book ['average_sentence_embedding'] k_best_f_reg_0_10 547.3455481609965 19.390907135087303 569.65531117966 19.765545234823104\n",
      "lasso book ['average_sentence_embedding'] ss_pca_0_95 393.3624513573726 15.798202459591158 484.8462329198358 17.678216075115024\n",
      "lasso book ['average_sentence_embedding'] k_best_mutual_info_0_10 572.6311307754539 19.744410677324858 595.2476685866743 20.137000684177416\n",
      "lasso book ['average_sentence_embedding'] None 470.698982495357 17.513889936901766 560.6522295592468 19.15321176579791\n",
      "lasso book ['average_sentence_embedding', 'doc2vec_chunk_embedding'] k_best_f_reg_0_10 547.3455481609965 19.390907135087303 569.65531117966 19.765545234823104\n",
      "lasso book ['average_sentence_embedding', 'doc2vec_chunk_embedding'] ss_pca_0_95 392.1050458832591 15.760986144600057 483.5719940865658 17.685219564466713\n",
      "lasso book ['average_sentence_embedding', 'doc2vec_chunk_embedding'] k_best_mutual_info_0_10 574.2046891637829 19.78395669738445 601.8322964928622 20.19566603766234\n",
      "lasso book ['average_sentence_embedding', 'doc2vec_chunk_embedding'] None 470.698982495357 17.513889936901766 560.6522295592468 19.15321176579791\n",
      "lasso book [] k_best_f_reg_0_10 547.3455481609965 19.390907135087303 569.65531117966 19.765545234823104\n",
      "lasso book [] ss_pca_0_95 393.5679736667306 15.801637494930205 484.6218517586667 17.66455894147833\n",
      "lasso book [] k_best_mutual_info_0_10 573.3063426369656 19.76363150236437 597.3364464995251 20.16072202952177\n",
      "lasso book [] None 470.698982495357 17.513889936901766 560.6522295592468 19.15321176579791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2233083.3903988823, tolerance: 936.6783161665354\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3617004.5691486043, tolerance: 909.2665373937298\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3874698.2968934886, tolerance: 895.5732188672007\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4259974.234359392, tolerance: 953.6964428225517\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3620246.747877972, tolerance: 946.2256321460626\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4467386.872723441, tolerance: 968.4685304674942\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2592152.6002968424, tolerance: 894.168169237138\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4367832.420741685, tolerance: 951.8926030983527\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4273028.245521807, tolerance: 958.4191927614013\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4604364.784896015, tolerance: 955.5856155350697\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso chunk ['doc2vec_chunk_embedding'] k_best_f_reg_0_10 616.8146790970343 20.337788681452786 624.7833594620961 20.444842754414584\n",
      "lasso chunk ['doc2vec_chunk_embedding'] ss_pca_0_95 524.6255215574992 18.715786392848347 589.1074095231891 19.782115858963046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2233083.3903988823, tolerance: 936.6783161665354\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3617004.5691486043, tolerance: 909.2665373937298\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3874698.2968934886, tolerance: 895.5732188672007\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4259974.234359392, tolerance: 953.6964428225517\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3620246.747877972, tolerance: 946.2256321460626\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4467386.872723441, tolerance: 968.4685304674942\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2592152.6002968424, tolerance: 894.168169237138\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4367832.420741685, tolerance: 951.8926030983527\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4273028.245521807, tolerance: 958.4191927614013\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4604364.784896015, tolerance: 955.5856155350697\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso chunk ['doc2vec_chunk_embedding'] k_best_mutual_info_0_10 616.8146790970343 20.337788681452786 624.7833594620961 20.444842754414584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2233083.3903988823, tolerance: 936.6783161665354\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3617004.5691486043, tolerance: 909.2665373937298\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3874698.2968934886, tolerance: 895.5732188672007\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4259974.234359392, tolerance: 953.6964428225517\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3620246.747877972, tolerance: 946.2256321460626\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4467386.872723441, tolerance: 968.4685304674942\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2592152.6002968424, tolerance: 894.168169237138\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4367832.420741685, tolerance: 951.8926030983527\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4273028.245521807, tolerance: 958.4191927614013\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4604364.784896015, tolerance: 955.5856155350697\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso chunk ['doc2vec_chunk_embedding'] None 616.8146790970343 20.337788681452786 624.7833594620961 20.444842754414584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3382895.13761769, tolerance: 936.6783161665354\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3764846.7895311993, tolerance: 909.2665373937298\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3641989.833068648, tolerance: 895.5732188672007\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3866217.020066582, tolerance: 953.6964428225517\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3789275.4174117735, tolerance: 946.2256321460626\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3914821.900338321, tolerance: 968.4685304674942\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3603807.650105422, tolerance: 894.168169237138\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3870388.379603817, tolerance: 951.8926030983527\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3884374.063083692, tolerance: 958.4191927614013\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3907507.6631239806, tolerance: 955.5856155350697\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso chunk ['average_sentence_embedding'] k_best_f_reg_0_10 501.54820776487514 18.321786727999918 547.9135475592132 19.153264187234544\n",
      "lasso chunk ['average_sentence_embedding'] ss_pca_0_95 513.1052381178275 18.615270498369515 548.5368119483397 19.255866307682528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3382895.13761769, tolerance: 936.6783161665354\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/arda/conda_root/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3764846.7895311993, tolerance: 909.2665373937298\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for model in [\"xgboost\", \"lasso\", \"svr\"]:\n",
    "    for features in [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]:\n",
    "        for drop_columns_including in [[\"doc2vec_chunk_embedding\"], [\"average_sentence_embedding\"], [\"average_sentence_embedding\", \"doc2vec_chunk_embedding\"], []]:\n",
    "            for dimensionality_reduction in [\"k_best_f_reg_0_10\", \"ss_pca_0_95\", \"k_best_mutual_info_0_10\", None]:\n",
    "                try:\n",
    "                    experiment = Experiment(\n",
    "                        features=features,\n",
    "                        drop_columns_including=drop_columns_including,\n",
    "                        dimensionality_reduction=dimensionality_reduction,\n",
    "                        model=model,\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    train_mse, train_mae, validation_mse, validation_mae = experiment.run()\n",
    "                    results.append((model, features, drop_columns_including, dimensionality_reduction, train_mse, train_mae, validation_mse, validation_mae))\n",
    "                    print(model, features, drop_columns_including, dimensionality_reduction, train_mse, train_mae, validation_mse, validation_mae)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in {model}, {features}, {drop_columns_including}, {dimensionality_reduction}\")\n",
    "                    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
