{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "import numpy as np; np.random.seed(random_seed)\n",
    "import xgboost\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class HyperParameterOptimizer(object):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.feature_importances_ = None\n",
    "    \n",
    "    def fit(self, _train_X, _train_y):\n",
    "        print(f\"Current shape of feature matrix:\", _train_X.shape)\n",
    "        num_boost_round = 99999\n",
    "\n",
    "        def feval(preds, train_data):\n",
    "            labels = train_data.get_label()\n",
    "            return 'mae', mean_absolute_error(labels, preds)\n",
    "        \n",
    "        all_results = []\n",
    "        for max_depth in [5]: # [5, 10, 15, 20, 25]\n",
    "            for learning_rate in [0.01]:\n",
    "                for colsample_bytree in [0.5]:\n",
    "                    for min_child_weight in [6]: # 3, 6, 8, 12\n",
    "                        train_X = deepcopy(_train_X)\n",
    "                        train_y = deepcopy(_train_y)\n",
    "                        dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "                        params = {\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"min_child_weight\": min_child_weight, \"n_jobs\": -1}\n",
    "                        cv_results = xgboost.cv(\n",
    "                            params,\n",
    "                            dtrain,\n",
    "                            num_boost_round=num_boost_round,\n",
    "                            seed=random_seed,\n",
    "                            nfold=10,\n",
    "                            feval=feval,\n",
    "                            early_stopping_rounds=20,\n",
    "                            verbose_eval=False\n",
    "                        )\n",
    "                        print(train_X.shape[1], max_depth, learning_rate, colsample_bytree, min_child_weight, np.round(float(cv_results[\"test-mae-mean\"].iloc[len(cv_results[\"test-mae-mean\"])-1]), 4))\n",
    "                        all_results.append((max_depth, learning_rate, colsample_bytree, min_child_weight, cv_results))\n",
    "        all_results_df = pd.DataFrame(all_results, columns=[\"max_depth\", \"learning_rate\", \"colsample_bytree\", \"min_child_weight\", \"cv_results\"])\n",
    "        all_results_df[\"best_validation_f1\"] = all_results_df[\"cv_results\"].apply(lambda x: x.iloc[len(x)-1][\"test-mae-mean\"])\n",
    "        all_results_df = all_results_df.sort_values(by=\"best_validation_f1\", ascending=False).reset_index(drop=True)\n",
    "        best_parameters = all_results_df.iloc[0]\n",
    "        best_max_depth = int(best_parameters[\"max_depth\"])\n",
    "        best_learning_rate = float(best_parameters[\"learning_rate\"])\n",
    "        best_colsample_bytree = float(best_parameters[\"colsample_bytree\"])\n",
    "        best_min_child_weight = float(best_parameters[\"min_child_weight\"])\n",
    "        best_validation_f1 = float(best_parameters[\"best_validation_f1\"])\n",
    "        best_num_boost_round = int(len(best_parameters[\"cv_results\"]))\n",
    "        \n",
    "        print(\"Current best max_depth:\", best_max_depth)\n",
    "        print(\"Current best learning_rate:\", best_learning_rate)\n",
    "        print(\"Current best colsample_bytree:\", best_colsample_bytree)\n",
    "        print(\"Current best min_child_weight:\", best_min_child_weight)\n",
    "        print(\"Current best num_boost_round:\", best_num_boost_round)\n",
    "        params = {\"max_depth\": best_max_depth, \"learning_rate\": best_learning_rate, \"colsample_bytree\": best_colsample_bytree, \"min_child_weight\": best_min_child_weight, \"n_jobs\": -1}\n",
    "        \n",
    "        train_X = deepcopy(_train_X)\n",
    "        train_y = deepcopy(_train_y)\n",
    "        dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "        model = xgboost.train(params,\n",
    "                              dtrain,\n",
    "                              num_boost_round=best_num_boost_round,\n",
    "                              verbose_eval=False)\n",
    "        feature_importances = model.get_score(importance_type='gain')\n",
    "        feature_importances_list = []\n",
    "        for i in range(train_X.shape[1]):\n",
    "            current_key = f'f{i}'\n",
    "            if current_key in feature_importances:\n",
    "                feature_importances_list.append(feature_importances[current_key])\n",
    "            else:\n",
    "                feature_importances_list.append(0.0)\n",
    "        print(f\"Current best validation f1 score is {np.round(best_validation_f1, 4)}\")\n",
    "        print(\"############################\")\n",
    "        self.model = model\n",
    "        self.feature_importances_ = np.array(feature_importances_list)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(xgboost.DMatrix(X))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.predict(X)\n",
    "\n",
    "    def score(self, X=None, y=None):\n",
    "        return 0.0\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        return self\n",
    "\n",
    "    def _get_tags(self):\n",
    "        return {\"allow_nan\": True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shape of feature matrix: (547, 526)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "import sys\n",
    "sys.path.insert(0, \"../src/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import read_labels\n",
    "\n",
    "labels_dir = \"../data/labels/\"\n",
    "lang = \"ger\"\n",
    "labels = read_labels(labels_dir)\n",
    "\n",
    "extracted_features_dir = \"../data/extracted_features/\"\n",
    "\n",
    "book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "\n",
    "train_X = book_and_averaged_chunk_df.drop(columns=[\"book_name\"])\n",
    "train_X = train_X[[column for column in train_X.columns if \"average_sentence_embedding\" not in column]].values\n",
    "train_y = book_and_averaged_chunk_df[\"book_name\"].apply(lambda x: labels[x]).values\n",
    "\n",
    "hpo = HyperParameterOptimizer()\n",
    "rfe = RFE(hpo, step=10, n_features_to_select=10, verbose=0)\n",
    "rfe.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = 734"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "book_and_averaged_chunk_df = book_and_averaged_chunk_df.drop(columns=[\"book_name\"])\n",
    "book_and_averaged_chunk_df = book_and_averaged_chunk_df[[column for column in book_and_averaged_chunk_df.columns if \"average_sentence_embedding\" not in column]]\n",
    "best_features = book_and_averaged_chunk_df.columns[np.argwhere((rfe.ranking_ <= 10) == True).T[0]].tolist()\n",
    "print(len(best_features))\n",
    "print(best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "book_names = book_and_averaged_chunk_df[\"book_name\"].values\n",
    "\n",
    "book_and_averaged_chunk_df = book_and_averaged_chunk_df.drop(columns=[\"book_name\"])\n",
    "book_and_averaged_chunk_df = book_and_averaged_chunk_df[[column for column in book_and_averaged_chunk_df.columns if \"average_sentence_embedding\" not in column]]\n",
    "best_features = book_and_averaged_chunk_df.columns[np.argwhere((rfe.ranking_ <= 10) == True).T[0]].tolist()\n",
    "book_and_averaged_chunk_df = book_and_averaged_chunk_df[best_features]\n",
    "\n",
    "df = book_and_averaged_chunk_df\n",
    "df[\"y\"] = np.array([labels[book_name] for book_name in book_names])\n",
    "df[\"book_name\"] = book_names\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "train_mses = []\n",
    "train_maes = []\n",
    "validation_mses = []\n",
    "validation_maes = []\n",
    "\n",
    "book_names_splitted = np.array_split(book_names, 10)\n",
    "for index, split in enumerate(book_names_splitted):\n",
    "    train_df = df[~df[\"book_name\"].isin(split)]\n",
    "    validation_df = df[df[\"book_name\"].isin(split)]\n",
    "    train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "    train_y = train_df[\"y\"].values.ravel()\n",
    "    validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "    validation_y = validation_df[\"y\"].values.ravel()\n",
    "    dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "    params = {\"max_depth\": 5, \"learning_rate\": 0.01, \"colsample_bytree\": 0.5, \"min_child_weight\": 6.0, \"n_jobs\": -1}\n",
    "    model = xgboost.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round\n",
    "    )\n",
    "\n",
    "    train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "    train_books[\"yhat\"] = model.predict(xgboost.DMatrix(train_X))\n",
    "    validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "    validation_books[\"yhat\"] = model.predict(xgboost.DMatrix(validation_X))\n",
    "\n",
    "    train_books = train_books.groupby(\"book_name\").mean()\n",
    "    validation_books = validation_books.groupby(\"book_name\").mean()\n",
    "\n",
    "    train_y = train_books[\"y\"].tolist()\n",
    "    train_yhat = train_books[\"yhat\"].tolist()\n",
    "    validation_y = validation_books[\"y\"].tolist()\n",
    "    validation_yhat = validation_books[\"yhat\"].tolist()\n",
    "\n",
    "    all_labels.extend(validation_y)\n",
    "    all_predictions.extend(validation_yhat)\n",
    "\n",
    "    train_mse = mean_squared_error(train_y, train_yhat)\n",
    "    train_mae = mean_absolute_error(train_y, train_yhat)\n",
    "    validation_mse = mean_squared_error(validation_y, validation_yhat)\n",
    "    validation_mae = mean_absolute_error(validation_y, validation_yhat)\n",
    "    train_mses.append(train_mse)\n",
    "    train_maes.append(train_mae)\n",
    "    validation_mses.append(validation_mse)\n",
    "    validation_maes.append(validation_mae)\n",
    "    print(f\"Fold: {index+1}, TrainMSE: {np.round(train_mse, 3)}, TrainMAE: {np.round(train_mae, 3)}, ValMSE: {np.round(validation_mse, 3)}, ValMAE: {np.round(validation_mae, 3)}\")\n",
    "all_labels = np.array(all_labels)\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "mean_train_mse = np.mean(train_mses)\n",
    "mean_train_mae = np.mean(train_maes)\n",
    "mean_validation_mse = np.mean(validation_mses)\n",
    "mean_validation_mae = np.mean(validation_maes)\n",
    "\n",
    "\n",
    "print(\"------\")\n",
    "print(f\"Mean scores, TrainMSE: {np.round(mean_train_mse, 3)}, TrainMAE: {np.round(mean_train_mae, 3)}, ValMSE: {np.round(mean_validation_mse, 3)}, ValMAE: {np.round(mean_validation_mae, 3)}\")\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.scatter(all_labels, all_predictions)\n",
    "plt.xlabel(\"Ground Truths\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
