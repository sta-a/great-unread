{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d78fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "random_seed = 42\n",
    "import numpy as np; np.random.seed(random_seed)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../src/\")\n",
    "import pandas as pd\n",
    "\n",
    "features_dir = \"../data/features/\"\n",
    "results_dir = \"../data/results_canon/\"\n",
    "sentiment_dir = \"../data/labels_sentiment/\"\n",
    "canonization_labels_dir = \"../data/labels_canon/\"\n",
    "lang = \"eng\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc0016f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFE\n",
    "import xgboost\n",
    "from copy import deepcopy\n",
    "from utils import read_canon_labels#, read_extreme_cases\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "labels = read_canon_labels(canonization_labels_dir)\n",
    "# canonization scores ground truths (extreme cases based on which canonoization scores were built)\n",
    "#extreme_cases_df = read_extreme_cases(labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa9d1d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_name</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ainsworth_William-Harrison_Rookwood_1834</td>\n",
       "      <td>0.140018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amory_Thomas_The-Life-of-John-Buncle_1756</td>\n",
       "      <td>0.140018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anonymous_Anonymous_Little-Goody-Two-Shoes_1765</td>\n",
       "      <td>0.346666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anonymous_Anonymous_The-Adventures-of-Anthony-...</td>\n",
       "      <td>0.140018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anonymous_Anonymous_The-Triumph-Prudence-Over-...</td>\n",
       "      <td>0.140018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>Ziegler_Heinrich_Asiatische-Bansie_1689</td>\n",
       "      <td>0.625028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>Zschokke_Johann_Das-Goldmacherdorf_1817</td>\n",
       "      <td>0.724274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>Zschokke_Johann_Addrich-im-Moos_1825</td>\n",
       "      <td>0.295803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>Zschokke_Johann_Der-Freihof-von-Aarau_1823</td>\n",
       "      <td>0.295803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>Zschokke_Johann_Die-Rose-von-Disentis_1844</td>\n",
       "      <td>0.295803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1153 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              book_name         y\n",
       "0              Ainsworth_William-Harrison_Rookwood_1834  0.140018\n",
       "1             Amory_Thomas_The-Life-of-John-Buncle_1756  0.140018\n",
       "2       Anonymous_Anonymous_Little-Goody-Two-Shoes_1765  0.346666\n",
       "3     Anonymous_Anonymous_The-Adventures-of-Anthony-...  0.140018\n",
       "4     Anonymous_Anonymous_The-Triumph-Prudence-Over-...  0.140018\n",
       "...                                                 ...       ...\n",
       "1148            Ziegler_Heinrich_Asiatische-Bansie_1689  0.625028\n",
       "1149            Zschokke_Johann_Das-Goldmacherdorf_1817  0.724274\n",
       "1150               Zschokke_Johann_Addrich-im-Moos_1825  0.295803\n",
       "1151         Zschokke_Johann_Der-Freihof-von-Aarau_1823  0.295803\n",
       "1152         Zschokke_Johann_Die-Rose-von-Disentis_1844  0.295803\n",
       "\n",
       "[1153 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab0fbbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "global_list = []\n",
    "\n",
    "class HyperParameterOptimizer(object):\n",
    "    def __init__(self, book_names=None):\n",
    "        self.book_names = book_names\n",
    "        # self.model = None\n",
    "        # self.feature_importances_ = None\n",
    "    \n",
    "    def fit(self, _train_X, _train_y):\n",
    "        global global_list\n",
    "        n_fold = 5\n",
    "        df = np.hstack((self.book_names, _train_X))\n",
    "        df = pd.DataFrame(df, columns=[\"book_name\"] + [f\"col_{i}\" for i in range(_train_X.shape[1])])\n",
    "        \n",
    "        print(f\"Current shape of feature matrix:\", _train_X.shape)\n",
    "        num_boost_round = 99999\n",
    "\n",
    "        def feval(preds, train_data):\n",
    "            labels = train_data.get_label()\n",
    "            return 'r2', r2_score(labels, preds)\n",
    "        \n",
    "        all_results = []\n",
    "        for max_depth in [4]:\n",
    "            for learning_rate in [0.03]:\n",
    "                for colsample_bytree in [0.33]:\n",
    "                    for min_child_weight in [6]:\n",
    "                        train_X = deepcopy(_train_X)\n",
    "                        train_y = deepcopy(_train_y)\n",
    "                        dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "                        params = {\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"min_child_weight\": min_child_weight, \"n_jobs\": -1}\n",
    "                        cv_results = xgboost.cv(\n",
    "                            params,\n",
    "                            dtrain,\n",
    "                            num_boost_round=num_boost_round,\n",
    "                            seed=random_seed,\n",
    "                            nfold=n_fold,\n",
    "                            folds=_split_booknames(df, n_fold, return_indices=True),\n",
    "                            feval=feval,\n",
    "                            maximize=True,\n",
    "                            early_stopping_rounds=5,\n",
    "                            verbose_eval=False\n",
    "                        )\n",
    "                        print(train_X.shape[1], max_depth, learning_rate, colsample_bytree, min_child_weight, np.round(float(cv_results[\"test-r2-mean\"].iloc[len(cv_results[\"test-r2-mean\"])-1]), 4))\n",
    "                        all_results.append((max_depth, learning_rate, colsample_bytree, min_child_weight, cv_results))\n",
    "        all_results_df = pd.DataFrame(all_results, columns=[\"max_depth\", \"learning_rate\", \"colsample_bytree\", \"min_child_weight\", \"cv_results\"])\n",
    "        all_results_df[\"best_validation_r2\"] = all_results_df[\"cv_results\"].apply(lambda x: x.iloc[len(x)-1][\"test-r2-mean\"])\n",
    "        all_results_df = all_results_df.sort_values(by=\"best_validation_r2\", ascending=False).reset_index(drop=True)\n",
    "        best_parameters = all_results_df.iloc[0]\n",
    "        best_max_depth = int(best_parameters[\"max_depth\"])\n",
    "        best_learning_rate = float(best_parameters[\"learning_rate\"])\n",
    "        best_colsample_bytree = float(best_parameters[\"colsample_bytree\"])\n",
    "        best_min_child_weight = float(best_parameters[\"min_child_weight\"])\n",
    "        best_validation_r2 = float(best_parameters[\"best_validation_r2\"])\n",
    "        best_num_boost_round = int(len(best_parameters[\"cv_results\"]))\n",
    "        \n",
    "        print(\"Current best max_depth:\", best_max_depth)\n",
    "        print(\"Current best learning_rate:\", best_learning_rate)\n",
    "        print(\"Current best colsample_bytree:\", best_colsample_bytree)\n",
    "        print(\"Current best min_child_weight:\", best_min_child_weight)\n",
    "        print(\"Current best num_boost_round:\", best_num_boost_round)\n",
    "        print(f\"Current best validation r2 score is {np.round(best_validation_r2, 4)}\")\n",
    "        print(\"############################\")\n",
    "        params = {\"max_depth\": best_max_depth, \"learning_rate\": best_learning_rate, \"colsample_bytree\": best_colsample_bytree, \"min_child_weight\": best_min_child_weight, \"n_jobs\": -1}\n",
    "        \n",
    "        train_X = deepcopy(_train_X)\n",
    "        train_y = deepcopy(_train_y)\n",
    "        dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "        model = xgboost.train(params,\n",
    "                              dtrain,\n",
    "                              num_boost_round=best_num_boost_round,\n",
    "                              verbose_eval=False)\n",
    "        feature_importances = model.get_score(importance_type='gain')\n",
    "        feature_importances_list = []\n",
    "        for i in range(train_X.shape[1]):\n",
    "            current_key = f'f{i}'\n",
    "            if current_key in feature_importances:\n",
    "                feature_importances_list.append(feature_importances[current_key])\n",
    "            else:\n",
    "                feature_importances_list.append(0.0)\n",
    "        self.model = model\n",
    "        self.feature_importances_ = np.array(feature_importances_list)\n",
    "        \n",
    "        global_list.append((best_validation_r2, train_X.shape[1], best_num_boost_round, params))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(xgboost.DMatrix(X))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.predict(X)\n",
    "\n",
    "    def score(self, X=None, y=None):\n",
    "        return 0.0\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {'book_names': self.book_names}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.book_names = params['book_names']\n",
    "        return self\n",
    "\n",
    "    def _get_tags(self):\n",
    "        return {\"allow_nan\": True}\n",
    "\n",
    "\n",
    "def _split_booknames(df, nr_splits, return_indices=False):\n",
    "    '''\n",
    "    Distribute book names over splits.\n",
    "    All works of an author are in the same split.\n",
    "    '''\n",
    "    book_names = df['book_name'].unique()\n",
    "    authors = []\n",
    "    booknames_authors_mapping = {}\n",
    "\n",
    "    #Get authors\n",
    "    for book_name in book_names:\n",
    "        author = '_'.join(book_name.split('_')[:2])\n",
    "        authors.append(author)\n",
    "        if author in booknames_authors_mapping:\n",
    "            booknames_authors_mapping[author].append(book_name)\n",
    "        else:\n",
    "            booknames_authors_mapping[author] = []\n",
    "            booknames_authors_mapping[author].append(book_name)\n",
    "    #Distribute authors over splits so that each split has approximately the same number of books\n",
    "    works_per_author = Counter(authors)\n",
    "    goal_sum = round(len(book_names)/nr_splits)\n",
    "    tolerance = 0.03\n",
    "    lower_threshold = goal_sum - round(tolerance*goal_sum)\n",
    "    upper_threshold = goal_sum + round(tolerance*goal_sum)\n",
    "    author_splits = []\n",
    "    for i in range (0, nr_splits-1):\n",
    "        works_in_split = 0\n",
    "        split = []\n",
    "        while works_in_split < upper_threshold:\n",
    "            curr_author = random.choice(list(works_per_author.keys()))\n",
    "            curr_author_workcount = works_per_author.pop(curr_author)\n",
    "            #Reinsert into dict if value is too high\n",
    "            if works_in_split + curr_author_workcount > upper_threshold:\n",
    "                works_per_author[curr_author] = curr_author_workcount\n",
    "            else:\n",
    "                split.append(curr_author)\n",
    "                works_in_split += curr_author_workcount\n",
    "                if works_in_split >= lower_threshold:\n",
    "                    break\n",
    "        author_splits.append(split)\n",
    "    #Create last split directly from remaining dict\n",
    "    works_in_last_split = sum(works_per_author.values())\n",
    "    split = list(works_per_author.keys())\n",
    "    author_splits.append(split)\n",
    "\n",
    "    if not return_indices:\n",
    "        #Map author splits to book names\n",
    "        book_splits = []\n",
    "        for author_split in author_splits:\n",
    "            book_split = []\n",
    "            for author in author_split:\n",
    "                book_split.extend(booknames_authors_mapping[author])\n",
    "            book_splits.append(book_split)\n",
    "    else:\n",
    "        book_name_idx_mapping = dict((book_name, index) for index, book_name in enumerate(book_names))\n",
    "        book_splits = []\n",
    "        for author_split in author_splits:\n",
    "            test_split = []\n",
    "            for author in author_split:\n",
    "                test_split.extend([book_name_idx_mapping[book_name] for book_name in booknames_authors_mapping[author]])\n",
    "            train_split = list(set(book_name_idx_mapping.values()) - set(test_split))\n",
    "            book_splits.append((train_split, test_split))\n",
    "    return book_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6661a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(object):\n",
    "    def __init__(self, features, drop_columns_including, dimensionality_reduction, include_data, model, verbose):\n",
    "        assert features in [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "        assert isinstance(drop_columns_including, list)\n",
    "        for i in drop_columns_including:\n",
    "            assert isinstance(i, str)\n",
    "        assert model in [\"xgboost\", \"svr\", \"lasso\"]\n",
    "        assert (dimensionality_reduction in [\"ss_pca_0_95\", \"k_best_f_reg_0_10\", \"k_best_mutual_info_0_10\", \"rfe\"]) or (dimensionality_reduction is None)\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.drop_columns_including = drop_columns_including\n",
    "        self.include_data = include_data\n",
    "        self.dimensionality_reduction = dimensionality_reduction\n",
    "        self.model = model\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if self.include_data != \"train_reduced_test_reduced\":\n",
    "            if self.features == \"book\":\n",
    "                self.df = deepcopy(book_df)\n",
    "            elif self.features == \"chunk\":\n",
    "                self.df = deepcopy(chunk_df)\n",
    "            elif self.features == \"chunk_and_copied_book\":\n",
    "                self.df = deepcopy(chunk_and_copied_book_df)\n",
    "            elif self.features == \"book_and_averaged_chunk\":\n",
    "                self.df = deepcopy(book_and_averaged_chunk_df)\n",
    "        else:\n",
    "            if self.features == \"book\":\n",
    "                curr_book_df = deepcopy(book_df)\n",
    "                self.df = curr_book_df.merge(extreme_cases_df, how='inner', left_on='book_name', right_on=\"file_name\", validate=\"one_to_one\").drop(columns=[\"file_name\"])\n",
    "            elif features == \"chunk\":\n",
    "                curr_chunk_df = deepcopy(chunk_df)\n",
    "                self.df = curr_chunk_df.merge(extreme_cases_df, how='inner', left_on='book_name', right_on=\"file_name\", validate=\"many_to_one\").drop(columns=[\"file_name\"])\n",
    "            elif features == \"chunk_and_copied_book\":\n",
    "                curr_chunk_and_copied_book_df = deepcopy(chunk_and_copied_book_df)\n",
    "                self.df = curr_chunk_and_copied_book_df.merge(extreme_cases_df, how='inner', left_on='book_name', right_on=\"file_name\", validate=\"many_to_one\").drop(columns=[\"file_name\"])\n",
    "            elif features == \"book_and_averaged_chunk\":\n",
    "                curr_book_and_averaged_chunk_df = deepcopy(book_and_averaged_chunk_df)\n",
    "                self.df = curr_book_and_averaged_chunk_df.merge(extreme_cases_df, how='inner', left_on='book_name', right_on=\"file_name\", validate=\"one_to_one\").drop(columns=[\"file_name\"])\n",
    "\n",
    "        columns_before_drop = set(self.df.columns)\n",
    "        self.df = self.df[[column for column in self.df.columns if not self._drop_column(column)]].reset_index(drop=True)\n",
    "        columns_after_drop = set(self.df.columns)\n",
    "        if self.verbose:\n",
    "            print(f\"Dropped {len(columns_before_drop - columns_after_drop)} columns.\")\n",
    "        self.df.loc[:, \"y\"] = self.df.book_name.apply(lambda x: self.labels[x]).tolist()\n",
    "\n",
    "    def _drop_column(self, column):\n",
    "        for string in self.drop_columns_including:\n",
    "            if string in column:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def _custom_pca(self, train_X):\n",
    "        for i in range(5, train_X.shape[1], int((train_X.shape[1] - 5) / 10)):\n",
    "            pca = PCA(n_components=i)\n",
    "            new_train_X = pca.fit_transform(train_X)\n",
    "            if pca.explained_variance_ratio_.sum() >= 0.95:\n",
    "                break\n",
    "        return new_train_X, pca\n",
    "\n",
    "    def _select_features(self, train_X, train_y, validation_X, train_book_names):\n",
    "        global global_list\n",
    "        best_num_boost_round = None\n",
    "        if self.dimensionality_reduction == \"ss_pca_0_95\":\n",
    "            ss = StandardScaler()\n",
    "            train_X = ss.fit_transform(train_X)\n",
    "            validation_X = ss.transform(validation_X)\n",
    "            train_X, pca = self._custom_pca(train_X)\n",
    "            validation_X = pca.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_f_reg_0_10\":\n",
    "            k_best = SelectKBest(f_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_mutual_info_0_10\":\n",
    "            k_best = SelectKBest(mutual_info_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"rfe\":\n",
    "            hpo = HyperParameterOptimizer(book_names=train_book_names)\n",
    "            rfe = RFE(hpo, step=50, n_features_to_select=50, verbose=0) # change\n",
    "            rfe.fit(train_X, train_y)\n",
    "            global_list = sorted(global_list, key=lambda x: -x[0])\n",
    "            best_feature_count = global_list[0][1]\n",
    "            best_num_boost_round = global_list[0][2]\n",
    "            best_params = global_list[0][3]\n",
    "            for ranking in range(rfe.ranking_.max()):\n",
    "                if (rfe.ranking_ <= ranking).sum() == best_feature_count:\n",
    "                    break\n",
    "            train_X = train_X[:, np.argwhere((rfe.ranking_ <= ranking) == True).T[0]]\n",
    "            validation_X = validation_X[:, np.argwhere((rfe.ranking_ <= ranking) == True).T[0]]\n",
    "            global_list = []\n",
    "        elif self.dimensionality_reduction is None:\n",
    "            pass\n",
    "        return train_X, validation_X, best_num_boost_round, best_params\n",
    "    \n",
    "    def _impute(self, train_X, validation_X):\n",
    "        imputer = KNNImputer()\n",
    "        train_X = imputer.fit_transform(train_X)\n",
    "        validation_X = imputer.transform(validation_X)\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    # def _get_model(self, num_boost_round, params):\n",
    "    #     if self.model == \"xgboost\":\n",
    "    #         return XGBRegressor(n_estimators=1000, max_depth=4, learning_rate=0.01, colsample_bytree=0.33, min_child_weight=6)\n",
    "    #     elif self.model == \"svr\":\n",
    "    #         return SVR()\n",
    "    #     elif self.model == \"lasso\":\n",
    "    #         return Lasso()\n",
    "\n",
    "    def run(self):\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        train_mses = []\n",
    "        train_maes = []\n",
    "        validation_mses = []\n",
    "        validation_maes = []\n",
    "        validation_r2s = []\n",
    "\n",
    "        df = self.df\n",
    "        book_names_split = _split_booknames(df=df, nr_splits=5) # change\n",
    "        #book_names = df['book_name'].unique()\n",
    "        #book_names_split = np.array_split(book_names, 10)\n",
    "        for index, split in enumerate(book_names_split):\n",
    "            train_df = df[~df[\"book_name\"].isin(split)]\n",
    "            validation_df = df[df[\"book_name\"].isin(split)]\n",
    "            #print(validation_df.head)\n",
    "            if self.include_data == \"train_full_test_reduced\":\n",
    "                #print(validation_df.shape)\n",
    "                validation_df = validation_df.merge(extreme_cases_df, how='inner', left_on=\"book_name\", right_on=\"file_name\").drop(columns=[\"file_name\"])\n",
    "                #print(validation_df.shape)\n",
    "            train_book_names = train_df[\"book_name\"].values.reshape(-1, 1)\n",
    "            train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            train_y = train_df[\"y\"].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            validation_y = validation_df[\"y\"].values.ravel()\n",
    "            train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            train_X, validation_X, num_boost_round, params = self._select_features(train_X, train_y, validation_X, train_book_names)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            # model = self._get_model(num_boost_round, params)\n",
    "            # model.fit(train_X, train_y)\n",
    "            \n",
    "            dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "            model = xgboost.train(params,\n",
    "                                  dtrain,\n",
    "                                  num_boost_round=num_boost_round,\n",
    "                                  verbose_eval=False)\n",
    "            \n",
    "            train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "            train_books[\"yhat\"] = model.predict(xgboost.DMatrix(train_X))\n",
    "            validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            validation_books[\"yhat\"] = model.predict(xgboost.DMatrix(validation_X))\n",
    "            \n",
    "            train_books = train_books.groupby(\"book_name\").mean()\n",
    "            validation_books = validation_books.groupby(\"book_name\").mean()\n",
    "            \n",
    "            train_y = train_books[\"y\"].tolist()\n",
    "            train_yhat = train_books[\"yhat\"].tolist()\n",
    "            validation_y = validation_books[\"y\"].tolist()\n",
    "            validation_yhat = validation_books[\"yhat\"].tolist()\n",
    "            \n",
    "            all_labels.extend(validation_y)\n",
    "            all_predictions.extend(validation_yhat)\n",
    "            \n",
    "            train_mse = mean_squared_error(train_y, train_yhat)\n",
    "            train_mae = mean_absolute_error(train_y, train_yhat)\n",
    "            validation_mse = mean_squared_error(validation_y, validation_yhat)\n",
    "            validation_mae = mean_absolute_error(validation_y, validation_yhat)\n",
    "            validation_r2 = r2_score(validation_y, validation_yhat)\n",
    "            train_mses.append(train_mse)\n",
    "            train_maes.append(train_mae)\n",
    "            validation_mses.append(validation_mse)\n",
    "            validation_maes.append(validation_mae)\n",
    "            validation_r2s.append(validation_r2)\n",
    "            \n",
    "            #if self.verbose:\n",
    "                #print(f\"Fold: {index+1}, TrainMSE: {np.round(train_mse, 3)}, TrainMAE: {np.round(train_mae, 3)}, ValMSE: {np.round(validation_mse, 3)}, ValMAE: {np.round(validation_mae, 3)}, ValR2: {np.round(validation_r2, 3)}\")\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "\n",
    "        mean_train_mse = np.mean(train_mses)\n",
    "        mean_train_mae = np.mean(train_maes)\n",
    "        mean_validation_mse = np.mean(validation_mses)\n",
    "        mean_validation_rmse = np.mean([sqrt(x) for x in validation_mses])\n",
    "        mean_validation_mae = np.mean(validation_maes)\n",
    "        mean_validation_r2 = np.mean(validation_r2s)\n",
    "        print(\"mean_validation_r2:\", mean_validation_r2)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"------\")\n",
    "            print(f\"Mean scores, TrainMSE: {np.round(mean_train_mse, 3)}, TrainMAE: {np.round(mean_train_mae, 3)}, ValMSE: {np.round(mean_validation_mse, 3)}, ValRMSE: {np.round(mean_validation_rmse, 3)}, ValMAE: {np.round(mean_validation_mae, 3)}, ValR2: {np.round(mean_validation_r2, 3)}\")\n",
    "\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.xticks(fontsize=20)\n",
    "            plt.yticks(fontsize=20)\n",
    "            plt.scatter(all_labels, all_predictions)\n",
    "            plt.xlabel(\"Canonization Scores\", fontsize=20)\n",
    "            plt.ylabel(\"Predicted Scores\", fontsize=20)\n",
    "            plt.savefig(\"../data/results/figures/\" + lang + '-' + self.model + '-' + self.dimensionality_reduction + '-' + self.features + '-' + self.include_data + '-' + 'author_split' + '.png', dpi=400)\n",
    "            plt.show();\n",
    "        return mean_train_mse, mean_train_mae, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f492590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language eng\n",
      "book\n",
      "Data included: full\n",
      "Dropped 0 columns.\n",
      "Error in eng, xgboost, book, ['average_sentence_embedding'], rfe, full\n",
      "'Radcliffe_Ann_The-Italian_1797'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Radcliffe_Ann_The-Italian_1797'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Radcliffe_Ann_The-Italian_1797'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b93fede6a9c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error in {lang}, {model}, {features}, {drop_columns_including}, {dimensionality_reduction}, {include_data}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-b93fede6a9c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data included:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                             experiment = Experiment(\n\u001b[0m\u001b[1;32m     28\u001b[0m                                 \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                 \u001b[0mdrop_columns_including\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_columns_including\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a19f05407564>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, features, drop_columns_including, dimensionality_reduction, include_data, model, verbose)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dropped {len(columns_before_drop - columns_after_drop)} columns.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_drop_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a19f05407564>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dropped {len(columns_before_drop - columns_after_drop)} columns.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_drop_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Radcliffe_Ann_The-Italian_1797'"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "eng_params = {\"features\": [\"book_and_averaged_chunk\"]}\n",
    "ger_params = {\"features\": [\"chunk\"]}\n",
    "\n",
    "for lang in [\"eng\", \"ger\"]: \n",
    "    print(\"Language\", lang)\n",
    "    if lang == \"eng\":\n",
    "        param_dir = eng_params\n",
    "    else:\n",
    "        param_dir = ger_params\n",
    "        \n",
    "    book_df = pd.read_csv(f\"{features_dir}/{lang}/book_df.csv\")\n",
    "    book_and_averaged_chunk_df = pd.read_csv(f\"{features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "    chunk_df = pd.read_csv(f\"{features_dir}/{lang}/chunk_df.csv\")\n",
    "    chunk_and_copied_book_df = pd.read_csv(f\"{features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "\n",
    "    for model in [\"xgboost\"]:\n",
    "        for features in [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]: #param_dir[\"features\"]: #\n",
    "            print(features)\n",
    "            for drop_columns_including in [[\"average_sentence_embedding\"]]:\n",
    "                for dimensionality_reduction in [\"rfe\"]:\n",
    "                    # train and test with either full dataset or only extreme cases\n",
    "                    for include_data in [\"full\"]: #, \"train_full_test_reduced\", \"train_reduced_test_reduced\"]:\n",
    "                        print('Data included:', include_data)\n",
    "                        try:\n",
    "                            experiment = Experiment(\n",
    "                                features=features,\n",
    "                                drop_columns_including=drop_columns_including,\n",
    "                                dimensionality_reduction=dimensionality_reduction,\n",
    "                                include_data=include_data,\n",
    "                                model=model,\n",
    "                                verbose=True\n",
    "                            )\n",
    "                            mean_train_mse, mean_train_mae, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2 = experiment.run()\n",
    "                            results.append((lang, model, features, drop_columns_including, dimensionality_reduction, include_data, mean_train_mse, mean_train_mae, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2))\n",
    "                            print(lang, model, features, drop_columns_including, dimensionality_reduction, include_data, mean_train_mse, mean_train_mae, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in {lang}, {model}, {features}, {drop_columns_including}, {dimensionality_reduction}, {include_data}\")\n",
    "                            print(e)\n",
    "                            raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
