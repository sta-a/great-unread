{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b64b32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../src/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "extracted_features_dir = \"../data/extracted_features/\"\n",
    "results_dir = \"../data/results/classification_library/\"\n",
    "sentiment_dir = \"../data/evaluationscore/\"\n",
    "canonization_labels_dir = \"../data/labels/\"\n",
    "lang = \"eng\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b8e470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR, SVC\n",
    "import xgboost\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBRegressor\n",
    "from copy import deepcopy\n",
    "from scipy.stats import pearsonr\n",
    "from utils import read_sentiment_scores, read_library_scores\n",
    "from math import sqrt\n",
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\") # %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import statistics\n",
    "import random\n",
    "random.seed(8)\n",
    "\n",
    "labels = read_sentiment_scores(sentiment_dir, canonization_labels_dir, lang)\n",
    "library_scores = read_library_scores(sentiment_dir, canonization_labels_dir, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4edf27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_name</th>\n",
       "      <th>c</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ainsworth_William-Harrison_Rookwood_1834</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amory_Thomas_The-Life-of-John-Buncle_1756</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anonymous_Anonymous_Little-Goody-Two-Shoes_1765</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anonymous_Anonymous_The-Adventures-of-Anthony-...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anonymous_Anonymous_The-Triumph-Prudence-Over-...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>Wilde_Oscar_The-Picture-of-Dorian-Gray_1890</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>Wollstonecraft_Mary_Mary_1788</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>Wood_Ellen_East-Lynne_1861</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>Yonge_Charlotte_The-Daisy-Chain_1856</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>Yonge_Charlotte_The-Heir-of-Redclyffe_1853</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>605 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             book_name  c  y\n",
       "0             Ainsworth_William-Harrison_Rookwood_1834  1  0\n",
       "1            Amory_Thomas_The-Life-of-John-Buncle_1756  1  0\n",
       "2      Anonymous_Anonymous_Little-Goody-Two-Shoes_1765  0  0\n",
       "3    Anonymous_Anonymous_The-Adventures-of-Anthony-...  1  0\n",
       "4    Anonymous_Anonymous_The-Triumph-Prudence-Over-...  0  0\n",
       "..                                                 ... .. ..\n",
       "600        Wilde_Oscar_The-Picture-of-Dorian-Gray_1890  0  0\n",
       "601                      Wollstonecraft_Mary_Mary_1788  1  0\n",
       "602                         Wood_Ellen_East-Lynne_1861  1  0\n",
       "603               Yonge_Charlotte_The-Daisy-Chain_1856  1  0\n",
       "604         Yonge_Charlotte_The-Heir-of-Redclyffe_1853  1  0\n",
       "\n",
       "[605 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "library_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed2eda57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_name</th>\n",
       "      <th>y</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austen_Jane_Pride-and-Prejudice_1813</td>\n",
       "      <td>0.029731</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austen_Jane_Sense-and-Sensibility_1811</td>\n",
       "      <td>0.049103</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barrie_J-M_Auld-Licht-Idylls_1888</td>\n",
       "      <td>0.016107</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barrie_J-M_Sentimental-Tommy_1896</td>\n",
       "      <td>0.038327</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Beerbohm_Max_Zuleika-Dobson_1911</td>\n",
       "      <td>0.053656</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Wollstonecraft_Mary_Mary_1788</td>\n",
       "      <td>0.040233</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Scott_Walter_The-Black-Dwarf_1816</td>\n",
       "      <td>-0.021840</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Beckford_William_Vathek_1786</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Radcliffe_Ann_Udolpho_1794</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>Smollett_Tobias_Humphry-Clinker_1771</td>\n",
       "      <td>0.010482</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  book_name         y  c\n",
       "1      Austen_Jane_Pride-and-Prejudice_1813  0.029731  3\n",
       "2    Austen_Jane_Sense-and-Sensibility_1811  0.049103  3\n",
       "3         Barrie_J-M_Auld-Licht-Idylls_1888  0.016107  3\n",
       "4         Barrie_J-M_Sentimental-Tommy_1896  0.038327  3\n",
       "5          Beerbohm_Max_Zuleika-Dobson_1911  0.053656  3\n",
       "..                                      ...       ... ..\n",
       "148           Wollstonecraft_Mary_Mary_1788  0.040233  3\n",
       "170       Scott_Walter_The-Black-Dwarf_1816 -0.021840  2\n",
       "177            Beckford_William_Vathek_1786  0.006286  2\n",
       "234              Radcliffe_Ann_Udolpho_1794  0.004301  2\n",
       "241    Smollett_Tobias_Humphry-Clinker_1771  0.010482  2\n",
       "\n",
       "[191 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a2338fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels[\"c\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "837d64ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 in labels[\"c\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f48ece3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD4CAYAAAD2FnFTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWGklEQVR4nO3df4xlZXnA8e8jSIQdhFrKAAs62mywlK2UHdGG/pixlSwrldrSlg2xrNWutJK0KSZutWlJjQltg1aDEdZKFNsy9BeVwqpF0nFLooVdC8yiWBG27e4QtiguDhDp6tM/7hnnOrx35syPc+/cme8nuZlz3vO+933OO3f22fPjvicyE0mSZntBrwOQJK1MJghJUpEJQpJUZIKQJBWZICRJRUf3OoDldNJJJ+XQ0FCjfTz99NOsW7eu0T76hWMxw7FocRxm9MtY7N2794nM/JHStlWVIIaGhtizZ0+jfYyPjzMyMtJoH/3CsZjhWLQ4DjP6ZSwi4r86bfMUkySpyAQhSSoyQUiSikwQkqQiE4QkqcgEIUkqMkFIkopMEJKkIhOEJKloVX2TWlqphnbcUSzff80buhyJVJ9HEJKkosaOICLiRuAi4FBmnl2V3QKcWVU5EfhWZp5TaLsf+DbwXeBIZg43FackqazJU0wfB64DbpouyMxfn16OiGuBw3O0H83MJxqLTpI0p8YSRGbujoih0raICODXgNc11b8kaWkiM5t781aCuH36FFNb+c8C7+906igiHgWeBBK4ITN3ztHHdmA7wODg4KaxsbFlir5samqKgYGBRvvoF47FjPnGYuJg+WB54/oTmgqpJ/xMzOiXsRgdHd3b6d/iXt3FtBW4eY7t52fmZEScDNwZEQ9l5u5SxSp57AQYHh7Opudf75c53rvBsZgx31hs63QX02Wd2/QjPxMzVsNYdP0upog4Gvhl4JZOdTJzsvp5CLgVOK870UmSpvXiNtdfAB7KzAOljRGxLiKOn14GLgD2dTE+SRINJoiIuBn4AnBmRByIiLdWmy5l1umliDgtInZVq4PA3RFxP3APcEdmfqapOCVJZU3exbS1Q/m2QtkksKVafgR4VVNxSZLqcaoNaQVyag6tBE61IUkqMkFIkopMEJKkIhOEJKnIBCFJKjJBSJKKTBCSpCIThCSpyAQhSSoyQUiSikwQkqQi52KSVIvzQ609HkFIkopMEJKkIhOEJKnIBCFJKjJBSJKKTBCSpCIThCSpqLEEERE3RsShiNjXVnZ1RByMiPuq15YObTdHxFcj4uGI2NFUjJKkzpo8gvg4sLlQ/oHMPKd67Zq9MSKOAj4MXAicBWyNiLMajFOSVNBYgsjM3cA3F9H0PODhzHwkM58DxoCLlzU4SdK8IjObe/OIIeD2zDy7Wr8a2AY8BewBrsrMJ2e1uQTYnJlvq9bfDLwmM6/s0Md2YDvA4ODgprGxsUb2ZdrU1BQDAwON9tEvHIsZ843FxMHDxfKN60/oSf3FqNOHn4kZ/TIWo6OjezNzuLSt23MxfQR4L5DVz2uB35xVJwrtOmaxzNwJ7AQYHh7OkZGRZQm0k/HxcZruo184FjPmG4ttneYxuqzcpun6i1GnDz8TM1bDWHT1LqbMfDwzv5uZ3wM+Sut00mwHgDPa1k8HJrsRnyRpRlcTRESc2rb6JmBfodq9wIaIeHlEHANcCtzWjfgkSTMaO8UUETcDI8BJEXEA+GNgJCLOoXXKaD/w9qruacBfZuaWzDwSEVcCnwWOAm7MzAebilOSVNZYgsjMrYXij3WoOwlsaVvfBTzvFlhJUvf4TWpJUpEJQpJUZIKQJBWZICRJRSYISVKRCUKSVGSCkCQVmSAkSUUmCElSkQlCklRkgpAkFZkgJElFJghJUpEJQpJUZIKQJBWZICRJRSYISVKRCUKSVGSCkCQVmSAkSUWNJYiIuDEiDkXEvrayP4+IhyLigYi4NSJO7NB2f0RMRMR9EbGnqRglSZ01eQTxcWDzrLI7gbMz8yeA/wT+YI72o5l5TmYONxSfJGkOjSWIzNwNfHNW2b9k5pFq9YvA6U31L0lamsjM5t48Ygi4PTPPLmz7Z+CWzPyrwrZHgSeBBG7IzJ1z9LEd2A4wODi4aWxsbJmiL5uammJgYKDRPvqFYzFjvrGYOHi4WL5x/Qk9qb8YdfrwMzGjX8ZidHR0b6czNUd3OxiAiHgPcAT46w5Vzs/MyYg4GbgzIh6qjkiep0oeOwGGh4dzZGSkiZC/b3x8nKb76BeOxYz5xmLbjjuK5fsvK7dpuv5i1OnDz8SM1TAWXb+LKSIuBy4CLssOhy+ZOVn9PATcCpzXvQglSdDlBBERm4F3AW/MzGc61FkXEcdPLwMXAPtKdSVJzWnyNtebgS8AZ0bEgYh4K3AdcDyt00b3RcT1Vd3TImJX1XQQuDsi7gfuAe7IzM80FackqayxaxCZubVQ/LEOdSeBLdXyI8CrmopLklRPTy5SS+qtoQ4XnAH2X/OGLkailcypNiRJRSYISVKRCUKSVGSCkCQVmSAkSUW1EkREPG8uJUnS6lb3COL6iLgnIn6n0zMcJEmrS60EkZk/DVwGnAHsiYi/iYjXNxqZJKmnal+DyMyvAX9Iay6lnwM+VD0d7pebCk6S1Dt1r0H8RER8APgK8DrgFzPzx6rlDzQYnySpR+pOtXEd8FHg3Zn57HRh9cyGP2wkMmkNmGvKC6nX6iaILcCzmfldgIh4AfCizHwmMz/ZWHSSpJ6pew3ic8CxbevHVWWSpFWqboJ4UWZOTa9Uy8c1E5IkaSWomyCejohzp1ciYhPw7Bz1JUl9ru41iN8D/i4iJqv1U4FfbyQiSdKKUCtBZOa9EfFK4EwggIcy8/8ajUyS1FMLeaLcq4Ghqs1PRgSZeVMjUUmSeq5WgoiITwI/CtwHfLcqTsAEIUmrVN0jiGHgrMzMum8cETcCFwGHMvPsquwlwC20jkT2A7+WmU8W2m4GPggcBfxlZl5Tt19J0vKoexfTPuCUBb73x4HNs8p2AHdl5gbgrmr9B0TEUcCHgQuBs4CtEXHWAvuWJC1R3SOIk4AvR8Q9wHemCzPzjZ0aZObuiBiaVXwxMFItfwIYpzX5X7vzgIcz8xGAiBir2n25ZqySpGUQdc4aRcTPlcoz8/PztBsCbm87xfStzDyxbfuTmflDs9pcAmzOzLdV628GXpOZV3boYzuwHWBwcHDT2NjYvPuzFFNTUwwMDDTaR79wLGbMNxYTBw93MZoZG9efUCyfK56Ftmmv72diRr+Mxejo6N7MHC5tq3ub6+cj4mXAhsz8XEQcR+v6QBOiFMIcse0EdgIMDw/nyMhIQ2G1jI+P03Qf/cKxmDHfWGzr0aR8+y8bKZbPFc9C27TX9zMxYzWMRd3pvn8L+HvghqpoPfBPi+jv8Yg4tXrPU4FDhToHaD2YaNrpwGShniSpQXUvUr8DOB94Cr7/8KCTF9HfbcDl1fLlwKcKde4FNkTEyyPiGODSqp0kqYvqJojvZOZz0ysRcTRznPap6twMfAE4MyIORMRbgWuA10fE14DXV+tExGkRsQsgM48AVwKfpfWAor/NzAcXtluSpKWqexfT5yPi3cCx1bOofwf457kaZObWDpt+vlB3ktYzJ6bXdwG7asYmSWpA3SOIHcD/AhPA22n94+2T5CRpFat7F9P3aD1y9KPNhiNJWinqzsX0KIVrDpn5imWPSJK0IixkLqZpLwJ+FXjJ8ocjSVopal2DyMxvtL0OZuZfAK9rNjRJUi/VPcV0btvqC2gdURzfSERSHxia9a3iqzYeYduOO9h/zRt6FNHymb1vWrvqnmK6tm35CNVU3csejSRpxah7F9No04FIklaWuqeYfn+u7Zn5/uUJR5K0UizkLqZXMzMn0i8Cu4H/aSIoSVLvLeSBQedm5rcBIuJq4O+mn9kgSVp96k618VLgubb152g9V1qStErVPYL4JHBPRNxK6xvVbwJuaiwqSVLP1b2L6X0R8WngZ6qit2TmfzQXliSp1+qeYgI4DngqMz8IHIiIlzcUkyRpBaj7yNE/Bt4F/EFV9ELgr5oKSpLUe3WPIN4EvBF4Gr7/gB+n2pCkVazuRernMjMjIgEiYl2DMUnqI+1zN03PSQWsinmp1rq6RxB/GxE3ACdGxG8Bn8OHB0nSqjbvEUREBHAL8ErgKeBM4I8y887FdBgRZ1bvN+0V1fv9RVudEeBTwKNV0T9m5p8spj9J0uLMmyCqU0v/lJmbgEUlhVnv91XgHICIOAo4CNxaqPpvmXnRUvuTJC1O3VNMX4yIVzfQ/88DX8/M/2rgvSVJS1A3QYzSShJfj4gHImIiIh5Yhv4vBW7usO2nIuL+iPh0RPz4MvQlSVqAyMzOGyNempn/HREvK21fyv/8I+IYYBL48cx8fNa2FwPfy8ypiNgCfDAzN3R4n+3AdoDBwcFNY2Njiw2plqmpKQYGBhrto1+s5bGYOHj4B9YHj4XHn4WN60+oVb9buh3P9DjM1fda0S9/H6Ojo3szc7i0bb4E8aXMPLda/ofM/JXlCioiLgbekZkX1Ki7HxjOzCfmqjc8PJx79uxZpgjLxsfHGRkZabSPfrGWx6L0yNFrJ47ueGtnrx7j2e14psdhrr7Xin75+4iIjglivlNM0bb8iuULCYCtdDi9FBGnVHdPERHn0YrzG8vcvyRpDvPdxZQdlpckIo4DXg+8va3sCoDMvB64BPjtiDgCPAtcmnMd6kiSlt18CeJVEfEUrSOJY6tlqvXMzBcvptPMfAb44Vll17ctXwdct5j3liQtjzkTRGYe1a1ApJVooefqe3WtoZOVFo/6y0Km+5YkrSEmCElSkQlCklRkgpAkFZkgJElFJghJUpEJQpJUZIKQJBWZICRJRSYISVKRCUKSVDTvM6mltcA5i6Tn8whCklRkgpAkFZkgJElFJghJUpEJQpJUZIKQJBWZICRJRT1JEBGxPyImIuK+iNhT2B4R8aGIeDgiHoiIc3sRpyStZb38otxoZj7RYduFwIbq9RrgI9VPSVKXrNRTTBcDN2XLF4ETI+LUXgclSWtJZGb3O414FHgSSOCGzNw5a/vtwDWZeXe1fhfwrswsnY7aDmwHGBwc3DQ2NtZo7FNTUwwMDDTaR79YTWMxcfDwktoPHguPP7tMwfSx9nHYuP6E3gbTY/3y9zE6Oro3M4dL23p1iun8zJyMiJOBOyPioczc3bY9Cm2KmaxKLjsBhoeHc2RkZNmDbTc+Pk7TffSL1TQW25Y4F9NVG49w7YRTm7WPw/7LRnobTI+thr+PnpxiyszJ6uch4FbgvFlVDgBntK2fDkx2JzpJEvQgQUTEuog4fnoZuADYN6vabcBvVHczvRY4nJmPdTlUSVrTenFMPAjcGhHT/f9NZn4mIq4AyMzrgV3AFuBh4BngLT2IU5LWtK4niMx8BHhVofz6tuUE3tHNuCRJP2il3uYqSeoxE4QkqcgEIUkqMkFIkopMEJKkIhOEJKnIuQG06gwtcdoMrSydfp/7r3lDlyNZezyCkCQVmSAkSUUmCElSkQlCklRkgpAkFZkgJElFJghJUpEJQpJUZIKQJBWZICRJRU61oRXPqTNWF3+f/cMjCElSkQlCklTU9QQREWdExL9GxFci4sGI+N1CnZGIOBwR91WvP+p2nJK01vXiGsQR4KrM/FJEHA/sjYg7M/PLs+r9W2Ze1IP4JEn04AgiMx/LzC9Vy98GvgKs73YckqS5RWb2rvOIIWA3cHZmPtVWPgL8A3AAmATemZkPdniP7cB2gMHBwU1jY2ONxjw1NcXAwECjffSLbo3FxMHDjfexVIPHwuPP9jqK3msfh43rTyjWWa7fZ6f3Xyn65d+K0dHRvZk5XNrWswQREQPA54H3ZeY/ztr2YuB7mTkVEVuAD2bmhvnec3h4OPfs2dNMwJXx8XFGRkYa7aNfdGss+uG2yKs2HuHaCe8abx+HTk98W67f50p/oly//FsRER0TRE/uYoqIF9I6Qvjr2ckBIDOfysypankX8MKIOKnLYUrSmtaLu5gC+Bjwlcx8f4c6p1T1iIjzaMX5je5FKUnqxTHx+cCbgYmIuK8qezfwUoDMvB64BPjtiDgCPAtcmr28WCJJa1DXE0Rm3g3EPHWuA67rTkSSpBKvqi1SpwttK/3C2UrWDxejVV/Tv0//BpvnVBuSpCIThCSpyAQhSSoyQUiSikwQkqQiE4QkqcgEIUkqMkFIkopMEJKkIhOEJKnIqTYqTX9tfyVOC7DQmGbXv2rjEbbtuGPB++CUGup3vfp77na/HkFIkopMEJKkIhOEJKnIBCFJKjJBSJKKTBCSpCIThCSpyAQhSSrqSYKIiM0R8dWIeDgidhS2R0R8qNr+QESc24s4JWkt63qCiIijgA8DFwJnAVsj4qxZ1S4ENlSv7cBHuhqkJKknRxDnAQ9n5iOZ+RwwBlw8q87FwE3Z8kXgxIg4tduBStJaFpnZ3Q4jLgE2Z+bbqvU3A6/JzCvb6twOXJOZd1frdwHvysw9hffbTusoA+BM4KsN78JJwBMN99EvHIsZjkWL4zCjX8biZZn5I6UNvZisLwpls7NUnTqtwsydwM6lBlVXROzJzOFu9beSORYzHIsWx2HGahiLXpxiOgCc0bZ+OjC5iDqSpAb1IkHcC2yIiJdHxDHApcBts+rcBvxGdTfTa4HDmflYtwOVpLWs66eYMvNIRFwJfBY4CrgxMx+MiCuq7dcDu4AtwMPAM8Bbuh3nHLp2OqsPOBYzHIsWx2FG349F1y9SS5L6g9+kliQVmSAkSUUmiIKIeElE3BkRX6t+/lCHejdGxKGI2LeY9v1gAWNRnD4lIq6OiIMRcV/12tK96JduKdPCzNe23yxxLPZHxET1GXje95n6SY1xeGVEfCEivhMR71xI2xUnM33NegF/BuyolncAf9qh3s8C5wL7FtO+H1519oXWzQZfB14BHAPcD5xVbbsaeGev92OR+95xv9rqbAE+Teu7O68F/r1u2356LWUsqm37gZN6vR9dGoeTgVcD72v/7PfjZ8IjiLKLgU9Uy58AfqlUKTN3A99cbPs+UWdf6kyf0o+WMi3MahsTp8hpmXccMvNQZt4L/N9C2640Joiyway+d1H9PLnL7VeSOvuyHviftvUDVdm0K6tTDjf22em2+fZrrjp12vaTpYwFtGZC+JeI2FtNj9OvlvJ77bvPRC+m2lgRIuJzwCmFTe/pdiy9tgxjMdfUKB8B3lutvxe4FvjNhcbYI0uZFqb2dDF9YqlT5JyfmZMRcTJwZ0Q8VB2B95ul/F777jOxZhNEZv5Cp20R8XhEnJqZj1WHyIcW+PZLbd9VyzAWHadGyczH297ro8DtyxN1VyxlWphjarTtJ0uaIiczp38eiohbaZ1u6ccEsZRpgPpuCiFPMZXdBlxeLV8OfKrL7VeSOvvScfqUWeeg3wTsK7RfqZYyLUydtv1k0WMREesi4niAiFgHXEB/fQ7aLeX32n+fiV5fJV+JL+CHgbuAr1U/X1KVnwbsaqt3M/AYrYtRB4C3ztW+H18LGIstwH/SukvjPW3lnwQmgAdo/TGc2ut9WuD+P2+/gCuAK6rloPUArK9X+zk835j062uxY0Hrrp37q9eD/T4WNcbhlOrfg6eAb1XLL+7Hz4RTbUiSijzFJEkqMkFIkopMEJKkIhOEJKnIBCFJKjJBSJKKTBCSpKL/B7mzLQqK6z5UAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Labels statistics\n",
    "print(len(pd.unique(labels[\"book_name\"]))) #197\n",
    "# 254 labels, 197 different book_names -> 57 second/third... reviews\n",
    "# 36 book_names with more than 1 label, these 36 book_names have 93 labels\n",
    "# 93 = 36 first reviews + 57 second/third... reviews\n",
    "# 6 texts have opposing reviews (13 reviews are opposing)\n",
    "# 191 texts after aggregating (without opposing reviews)\n",
    "\n",
    "labels[\"y\"].plot.hist(grid=True, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e960d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression(object):\n",
    "    def __init__(self, language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose):\n",
    "        assert isinstance(drop_columns_including, list)\n",
    "        for i in drop_columns_including:\n",
    "            assert isinstance(i, str)\n",
    "        assert (dimensionality_reduction in [\"k_best_f_reg_0_10\", \"k_best_mutual_info_0_10\", \"ss_pca_0_95\"]) or (dimensionality_reduction is None)\n",
    "        self._check_class_specific_assertions()\n",
    "        \n",
    "        self.language = language\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.labels = self._prepare_labels()\n",
    "        self.drop_columns_including = drop_columns_including\n",
    "        self.dimensionality_reduction = dimensionality_reduction\n",
    "        self.model_param = model_param\n",
    "        self.model = model\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if self.features == \"book\":\n",
    "            self.df = deepcopy(book_df)\n",
    "        elif self.features == \"chunk\":\n",
    "            self.df = deepcopy(chunk_df)\n",
    "        elif self.features == \"chunk_and_copied_book\":\n",
    "            self.df = deepcopy(chunk_and_copied_book_df)\n",
    "        elif self.features == \"book_and_averaged_chunk\":\n",
    "            self.df = deepcopy(book_and_averaged_chunk_df)\n",
    "\n",
    "        columns_before_drop = set(self.df.columns)\n",
    "        if self.drop_columns_including:\n",
    "            self.df = self.df[[column for column in self.df.columns if not self._drop_column(column)]].reset_index(drop=True)\n",
    "        columns_after_drop = set(self.df.columns)\n",
    "        if self.verbose:\n",
    "            print(f\"Dropped {len(columns_before_drop - columns_after_drop)} columns.\")\n",
    "            \n",
    "    def _check_class_specific_assertions(self):\n",
    "        assert model in [\"xgboost\", \"svr\", \"lasso\"]\n",
    "        assert features in [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "    \n",
    "    def _prepare_labels(self):\n",
    "        return self.labels.drop(columns=\"c\")\n",
    "\n",
    "    def _drop_column(self, column):\n",
    "        for string in self.drop_columns_including:\n",
    "            if string in column:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def _custom_pca(self, train_X):\n",
    "        for i in range(5, train_X.shape[1], int((train_X.shape[1] - 5) / 10)):\n",
    "            pca = PCA(n_components=i)\n",
    "            new_train_X = pca.fit_transform(train_X)\n",
    "            if pca.explained_variance_ratio_.sum() >= 0.95:\n",
    "                break\n",
    "        return new_train_X, pca\n",
    "\n",
    "    def _select_features(self, train_X, train_y, validation_X):\n",
    "        if self.dimensionality_reduction == \"ss_pca_0_95\":\n",
    "            ss = StandardScaler()\n",
    "            train_X = ss.fit_transform(train_X)\n",
    "            validation_X = ss.transform(validation_X)\n",
    "            train_X, pca = self._custom_pca(train_X)\n",
    "            validation_X = pca.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_f_reg_0_10\":\n",
    "            k_best = SelectKBest(f_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_mutual_info_0_10\":\n",
    "            k_best = SelectKBest(mutual_info_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction is None:\n",
    "            pass\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _impute(self, train_X, validation_X):\n",
    "        imputer = KNNImputer()\n",
    "        train_X = imputer.fit_transform(train_X)\n",
    "        validation_X = imputer.transform(validation_X)\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _get_model(self, model_param, train_X=None, train_y=None, train_book_names=None, task_type=None):\n",
    "        if self.model == \"xgboost\":\n",
    "            if task_type == \"binary_classification\":\n",
    "                is_classification = True\n",
    "                class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1], y=train_y.astype(int).tolist())))\n",
    "            elif task_type == \"multiclass_classification\":\n",
    "                is_classification = True\n",
    "                class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1, 2, 3], y=train_y.astype(int).tolist())))\n",
    "            elif task_type == \"regression\":\n",
    "                is_classification = False\n",
    "            else:\n",
    "                raise Exception(\"Not a valid task_type\")\n",
    "            \n",
    "            def feval(preds, train_data):\n",
    "                labels = train_data.get_label()\n",
    "                if is_classification:\n",
    "                    labels = labels.astype(int)\n",
    "                    preds = preds.argmax(axis=1).astype(int)\n",
    "                    if task_type == \"binary_classification\":\n",
    "                        return 'acc', accuracy_score(labels, preds)\n",
    "                    elif task_type == \"multiclass_classification\":\n",
    "                        return 'f1', f1_score(labels, preds, average='macro')\n",
    "                else:\n",
    "                    return 'rmse', np.sqrt(mean_squared_error(labels, preds))\n",
    "            \n",
    "            if is_classification:\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y.astype(int), weight=[class_weights[int(i)] for i in train_y])\n",
    "            else:\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "            results = []\n",
    "            df = np.hstack((train_book_names, train_X))\n",
    "            df = pd.DataFrame(df, columns=[\"book_name\"] + [f\"col_{i}\" for i in range(train_X.shape[1])])\n",
    "            for max_depth in [2, 4, 6, 8]:\n",
    "                for learning_rate in [None, 0.01, 0.033, 0.1]:\n",
    "                    for colsample_bytree in [0.33, 0.60, 0.75]:\n",
    "                        if task_type == \"multiclass_classification\":\n",
    "                            params = {\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"n_jobs\": -1, \"objective\": \"multi:softmax\", \"num_class\": 4, \"eval_metric\": \"mlogloss\"}\n",
    "                        elif task_type == \"binary_classification\":\n",
    "                            params = {\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"n_jobs\": -1, \"objective\": \"multi:softmax\", \"num_class\": 2, \"eval_metric\": \"mlogloss\"}\n",
    "                        elif task_type == \"regression\":\n",
    "                            params = {\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"n_jobs\": -1}\n",
    "                        else:\n",
    "                            raise Exception(\"Not a valid task_type\")\n",
    "                        cv_results = xgboost.cv(\n",
    "                                        params,\n",
    "                                        dtrain,\n",
    "                                        num_boost_round=99999,\n",
    "                                        seed=42,\n",
    "                                        nfold=5,\n",
    "                                        folds=self._split_booknames(df, 5, return_indices=True),\n",
    "                                        feval=feval,\n",
    "                                        maximize=is_classification, # if classification, maximize f1/acc score.\n",
    "                                        early_stopping_rounds=10,\n",
    "                                        verbose_eval=False)\n",
    "\n",
    "                        if task_type == \"binary_classification\":\n",
    "                            nested_cv_score = cv_results.iloc[len(cv_results)-1][\"test-acc-mean\"]\n",
    "                        elif task_type == \"multiclass_classification\":\n",
    "                            nested_cv_score = cv_results.iloc[len(cv_results)-1][\"test-f1-mean\"]\n",
    "                        elif task_type == \"regression\":\n",
    "                            nested_cv_score = cv_results.iloc[len(cv_results)-1][\"test-rmse-mean\"]\n",
    "                        else:\n",
    "                            raise Exception(\"Not a valid task_type\")\n",
    "                        num_boost_round = len(cv_results)\n",
    "                        if task_type == \"multiclass_classification\":\n",
    "                            results.append({\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"num_boost_round\": num_boost_round, \"nested_cv_score\": nested_cv_score, \"objective\": \"multi:softmax\", \"num_class\": 4, \"eval_metric\": \"mlogloss\"})\n",
    "                        elif task_type == \"binary_classification\":\n",
    "                            results.append({\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"num_boost_round\": num_boost_round, \"nested_cv_score\": nested_cv_score, \"objective\": \"multi:softmax\", \"num_class\": 2, \"eval_metric\": \"mlogloss\"})\n",
    "                        elif task_type == \"regression\":\n",
    "                            results.append({\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"num_boost_round\": num_boost_round, \"nested_cv_score\": nested_cv_score})\n",
    "                        else:\n",
    "                            raise Exception(\"Not a valid task_type\")\n",
    "            best_parameters = sorted(results, key=lambda x: x[\"nested_cv_score\"], reverse=is_classification)[0]\n",
    "            return best_parameters\n",
    "        elif self.model == \"svr\":\n",
    "            return SVR(C=model_param)\n",
    "        elif self.model == \"lasso\":\n",
    "            return Lasso(alpha=model_param)\n",
    "        elif self.model == \"svc\":\n",
    "            return SVC(C=model_param, class_weight=\"balanced\")\n",
    "        \n",
    "    def _split_booknames(self, df, nr_splits, return_indices=False):\n",
    "        \"\"\"\n",
    "        Distribute book names over splits.\n",
    "        All works of an author are in the same split.\n",
    "        \"\"\"\n",
    "        book_names = df[\"book_name\"].unique()\n",
    "        authors = []\n",
    "        booknames_authors_mapping = {}\n",
    "\n",
    "        #Get authors\n",
    "        for book_name in book_names:\n",
    "            author = \"_\".join(book_name.split(\"_\")[:2])\n",
    "            authors.append(author)\n",
    "            if author in booknames_authors_mapping:\n",
    "                booknames_authors_mapping[author].append(book_name)\n",
    "            else:\n",
    "                booknames_authors_mapping[author] = []\n",
    "                booknames_authors_mapping[author].append(book_name)\n",
    "        #Distribute authors over splits so that each split has approximately the same number of books\n",
    "        works_per_author = Counter(authors)\n",
    "        goal_sum = round(len(book_names)/nr_splits)\n",
    "        tolerance = 0.03\n",
    "        lower_threshold = goal_sum - round(tolerance*goal_sum)\n",
    "        upper_threshold = goal_sum + round(tolerance*goal_sum)\n",
    "        author_splits = []\n",
    "        popped_dict = {}\n",
    "\n",
    "        for i in range (0, nr_splits-1):\n",
    "            works_in_split = 0\n",
    "            split = []\n",
    "            curr_author_workcount = 0\n",
    "\n",
    "            # take values from popped dict first\n",
    "            if bool(popped_dict):  \n",
    "                popped = []\n",
    "                for curr_author, curr_author_workcount in popped_dict.items():\n",
    "                    # leave item in popped dict if value is too big\n",
    "                    if works_in_split + curr_author_workcount > upper_threshold:\n",
    "                        continue\n",
    "                    else:\n",
    "                        popped.append(curr_author)\n",
    "                        split.append(curr_author)\n",
    "                        works_in_split += curr_author_workcount\n",
    "                        if works_in_split >= lower_threshold:\n",
    "                            break\n",
    "                for current_author in popped:\n",
    "                    del popped_dict[current_author]\n",
    "            while works_in_split < upper_threshold:\n",
    "                if bool(works_per_author):\n",
    "                    curr_author = random.choice(list(works_per_author.keys()))\n",
    "                    curr_author_workcount = works_per_author.pop(curr_author)\n",
    "                    # Put values into separate dict if too big\n",
    "                    if works_in_split + curr_author_workcount > upper_threshold:\n",
    "                        popped_dict[curr_author] = curr_author_workcount\n",
    "                    else:\n",
    "                        split.append(curr_author)\n",
    "                        works_in_split += curr_author_workcount\n",
    "                        if works_in_split >= lower_threshold:\n",
    "                            break\n",
    "                else:\n",
    "                    #ignore upper threshold\n",
    "                    popped = []\n",
    "                    for curr_author, curr_author_workcount in popped_dict.items():\n",
    "                        popped.append(curr_author)\n",
    "                        split.append(curr_author)\n",
    "                        works_in_split += curr_author_workcount\n",
    "                        if works_in_split >= lower_threshold:\n",
    "                            break\n",
    "                    for current_author in popped:\n",
    "                        del popped_dict[current_author]\n",
    "\n",
    "            author_splits.append(split)\n",
    "        #Create last split directly from remaining dict\n",
    "        works_in_last_split = sum(works_per_author.values()) + sum(popped_dict.values())\n",
    "        split = list(works_per_author.keys()) + list(popped_dict.keys())\n",
    "        author_splits.append(split)\n",
    "\n",
    "        if not return_indices:\n",
    "            #Map author splits to book names\n",
    "            book_splits = []\n",
    "            for author_split in author_splits:\n",
    "                book_split = []\n",
    "                for author in author_split:\n",
    "                    book_split.extend(booknames_authors_mapping[author])\n",
    "                book_splits.append(book_split)\n",
    "        else:\n",
    "            book_name_idx_mapping = dict((book_name, index) for index, book_name in enumerate(book_names))\n",
    "            book_splits = []\n",
    "            for author_split in author_splits:\n",
    "                test_split = []\n",
    "                for author in author_split:\n",
    "                    test_split.extend([book_name_idx_mapping[book_name] for book_name in booknames_authors_mapping[author]])\n",
    "                train_split = list(set(book_name_idx_mapping.values()) - set(test_split))\n",
    "                book_splits.append((train_split, test_split))\n",
    "        return book_splits\n",
    "    \n",
    "    def _get_pvalue(self, validation_corr_pvalues):\n",
    "        # Harmonic mean p-value\n",
    "        denominator = sum([1/x for x in validation_corr_pvalues])\n",
    "        mean_p_value = len(validation_corr_pvalues)/denominator\n",
    "        return mean_p_value\n",
    "    \n",
    "    def _combine_df_labels(self, df):\n",
    "        #Average of sentiscores per book\n",
    "        df = df.merge(right=self.labels, on=\"book_name\", how=\"inner\", validate=\"many_to_one\")\n",
    "        return df\n",
    "    \n",
    "    def run(self):\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        train_mses = []\n",
    "        train_maes = []\n",
    "        train_r2s = []\n",
    "        train_corrs = []\n",
    "        \n",
    "        validation_mses = []\n",
    "        validation_maes = []\n",
    "        validation_r2s = []\n",
    "        validation_corrs = []\n",
    "        validation_corr_pvalues = []\n",
    "\n",
    "        df = self.df\n",
    "        df = self._combine_df_labels(df)\n",
    "        book_names_split = self._split_booknames(df, 5)\n",
    "        all_validation_books = []\n",
    "\n",
    "        for index, split in enumerate(book_names_split):\n",
    "            train_df = df[~df[\"book_name\"].isin(split)]\n",
    "            validation_df = df[df[\"book_name\"].isin(split)]\n",
    "            \n",
    "            train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            train_y = train_df[\"y\"].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            validation_y = validation_df[\"y\"].values.ravel()\n",
    "            train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            if self.model == \"xgboost\":\n",
    "                train_book_names = train_df[\"book_name\"].values.reshape(-1, 1)\n",
    "                best_parameters = self._get_model(self.model_param, train_X, train_y, train_book_names, task_type=\"regression\")\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "                num_boost_round = best_parameters[\"num_boost_round\"]\n",
    "                best_parameters.pop(\"nested_cv_score\")\n",
    "                best_parameters.pop(\"num_boost_round\")\n",
    "                model = xgboost.train(best_parameters,\n",
    "                                      dtrain,\n",
    "                                      num_boost_round=num_boost_round,\n",
    "                                      verbose_eval=False)\n",
    "            else:\n",
    "                model = self._get_model(self.model_param)\n",
    "                model.fit(train_X, train_y)\n",
    "            \n",
    "            train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "            validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            \n",
    "            if self.model == \"xgboost\":\n",
    "                train_books[\"yhat\"] = model.predict(xgboost.DMatrix(train_X))\n",
    "                validation_books[\"yhat\"] = model.predict(xgboost.DMatrix(validation_X))\n",
    "                \n",
    "                print(\"train preds:\", model.predict(xgboost.DMatrix(train_X)))\n",
    "                print(\"validation preds:\", model.predict(xgboost.DMatrix(validation_X)))\n",
    "            else:\n",
    "                train_books[\"yhat\"] = model.predict(train_X)\n",
    "                validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "            \n",
    "            train_books = train_books.groupby(\"book_name\").mean()\n",
    "            validation_books = validation_books.groupby(\"book_name\").mean()\n",
    "            all_validation_books.append(validation_books.reset_index())\n",
    "            \n",
    "            train_y = train_books[\"y\"].tolist()\n",
    "            train_yhat = train_books[\"yhat\"].tolist()\n",
    "            validation_y = validation_books[\"y\"].tolist()\n",
    "            validation_yhat = validation_books[\"yhat\"].tolist()\n",
    "            \n",
    "            all_labels.extend(validation_y)\n",
    "            all_predictions.extend(validation_yhat)\n",
    "            \n",
    "            train_mse = mean_squared_error(train_y, train_yhat)\n",
    "            train_mae = mean_absolute_error(train_y, train_yhat)\n",
    "            train_r2 = r2_score(train_y, train_yhat)\n",
    "            train_corr = pearsonr(train_y, train_yhat)[0]\n",
    "            \n",
    "            validation_mse = mean_squared_error(validation_y, validation_yhat)\n",
    "            validation_mae = mean_absolute_error(validation_y, validation_yhat)\n",
    "            validation_r2 = r2_score(validation_y, validation_yhat)\n",
    "            validation_corr = pearsonr(validation_y, validation_yhat)[0]\n",
    "            p_value = pearsonr(validation_y, validation_yhat)[1]\n",
    "            \n",
    "            train_mses.append(train_mse)\n",
    "            train_maes.append(train_mae)\n",
    "            train_r2s.append(train_r2)\n",
    "            train_corrs.append(train_corr)\n",
    "            \n",
    "            validation_mses.append(validation_mse)\n",
    "            validation_maes.append(validation_mae)\n",
    "            validation_r2s.append(validation_r2)\n",
    "            validation_corrs.append(validation_corr)\n",
    "            validation_corr_pvalues.append(p_value)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Fold: {index+1}, TrainMSE: {np.round(train_mse, 3)}, TrainMAE: {np.round(train_mae, 3)}, ValMSE: {np.round(validation_mse, 3)}, ValMAE: {np.round(validation_mae, 3)}, ValR2: {np.round(validation_r2, 3)}, ValCorr: {np.round(validation_corr, 3)}\")\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        \n",
    "        # Save y and y_pred for examples\n",
    "        #pd.concat(all_validation_books).to_csv(results_dir + \"/y-yhat-\" + param_dir + '_' + self.language + \"-\" + \".csv\", index=False)\n",
    "        \n",
    "        mean_train_mse = np.mean(train_mses)\n",
    "        mean_train_rmse = np.mean([sqrt(x) for x in train_mses])\n",
    "        mean_train_mae = np.mean(train_maes)\n",
    "        mean_train_r2 = np.mean(train_r2s)\n",
    "        mean_train_corr = np.mean(train_corrs)\n",
    "        \n",
    "        mean_validation_mse = np.mean(validation_mses)\n",
    "        mean_validation_rmse = np.mean([sqrt(x) for x in validation_mses])\n",
    "        mean_validation_mae = np.mean(validation_maes)\n",
    "        mean_validation_r2 = np.mean(validation_r2s)\n",
    "        mean_validation_corr = np.mean(validation_corrs)\n",
    "        mean_p_value = self._get_pvalue(validation_corr_pvalues)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\"\"TrainMSE: {np.round(mean_train_mse, 3)}, TrainRMSE: {np.round(mean_train_rmse, 3)}, TrainMAE: {np.round(mean_train_mae, 3)}, TrainR2: {np.round(mean_train_r2, 3)}, TrainCorr: {np.round(mean_train_corr, 3)}, ValMSE: {np.round(mean_validation_mse, 3)}, ValRMSE: {np.round(mean_validation_rmse, 3)}, ValMAE: {np.round(mean_validation_mae, 3)}, ValR2: {np.round(mean_validation_r2, 3)}, ValCorr: {np.round(mean_validation_corr, 3)}, ValCorrPValue: {np.round(mean_p_value, 3)}\"\"\")\n",
    "            print(\"\\n---------------------------------------------------\\n\")\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.xticks(fontsize=15)\n",
    "            plt.yticks(fontsize=15)\n",
    "            plt.xlim([0,1])\n",
    "            plt.ylim([0,1])\n",
    "\n",
    "            plt.scatter(all_labels, all_predictions, s=6)\n",
    "            plt.xlabel(\"Canonization Scores\", fontsize=20)\n",
    "            plt.ylabel(\"Predicted Scores\", fontsize=20)\n",
    "            plt.savefig(results_dir + lang + \"-\" + self.model + \"-\" + str(self.dimensionality_reduction) \n",
    "            + \"-\" + self.features + \"-\" + \"-\" + \"param\" + str(self.model_param) + \"-\" + \".png\", \n",
    "            dpi=400, bbox_inches=\"tight\")\n",
    "    \n",
    "            plt.show();\n",
    "        return mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09330efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Classification into reviewed/not reviewed\n",
    "'''\n",
    "\n",
    "class TwoclassClassification(Regression):\n",
    "    def __init__(self, language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose):\n",
    "        super().__init__(language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose)\n",
    "\n",
    "    def _check_class_specific_assertions(self):\n",
    "        assert model in [\"svc\", \"xgboost\"]\n",
    "        assert features in [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "        \n",
    "    def _prepare_labels(self):\n",
    "        labels = self.labels.drop(columns=\"y\").rename(columns={\"c\":\"y\"})\n",
    "        return labels\n",
    "        \n",
    "    def _combine_df_labels(self, df):\n",
    "        #Reviews zum englischen Korpus beginnnen mit 1759 und decken alles bis 1914 ab\n",
    "        agg_labels = self.labels[[\"book_name\"]].drop_duplicates()\n",
    "        agg_labels[\"y\"] = 1\n",
    "        df = df.merge(right=agg_labels, on=\"book_name\", how=\"left\", validate=\"many_to_one\")\n",
    "        df[\"y\"] = df[\"y\"].fillna(value=0)\n",
    "        #Select books written after year of first review)\n",
    "        year = df[\"book_name\"].str.replace('-', '_').str.split('_').str[-1].astype('int64')\n",
    "        df = df.loc[year>=min(year)]\n",
    "        return df\n",
    "    \n",
    "    def _get_sample_weights(self, df):\n",
    "        # Weights for calculating accuracy \n",
    "        chunks_per_book = df[\"book_name\"].value_counts(sort=False).rename('chunks_per_book')\n",
    "        chunks_per_book = chunks_per_book.reset_index().rename(columns={\"index\":'book_name'})\n",
    "        chunks_per_book[\"chunks_per_book\"] = 1/chunks_per_book[\"chunks_per_book\"]\n",
    "        df = df.merge(right=chunks_per_book, how=\"left\", on=\"book_name\")\n",
    "        sample_weights = df[\"chunks_per_book\"].tolist()\n",
    "        return sample_weights\n",
    "    \n",
    "    def _aggregate_chunk_predictions(self, df):\n",
    "        g = df.groupby(\"book_name\")\n",
    "        \n",
    "        # Majority vote\n",
    "        # If one value is more common, assign it to every chunk\n",
    "        # Therefore, accuracy is either 0 or 1\n",
    "        # If both values are equally likely, leave them unchanged, and accuracy is 0.5\n",
    "        def _get_mode_accuracy(group):\n",
    "            counts = group[\"yhat\"].value_counts()\n",
    "            if len(counts) == 1:\n",
    "                mode_acc = counts.index[0]\n",
    "            else:\n",
    "                mode_acc = 0.5\n",
    "            return mode_acc\n",
    "        mode_accs = g.apply(_get_mode_accuracy).rename(\"mode_acc\").reset_index()\n",
    "        mode_acc = mode_accs[\"mode_acc\"].mean()\n",
    "        \n",
    "        # Average accuracy within book\n",
    "        book_acc = g.apply(lambda group: accuracy_score(group[\"y\"], group[\"yhat\"])).mean()\n",
    "        #Accuracy when each chunk is treated as single document\n",
    "        chunk_acc = accuracy_score(df[\"y\"], df[\"yhat\"])#, sample_weight = self._get_sample_weights(df))\n",
    "        return {\"mode_acc\": mode_acc, \"book_acc\": book_acc, \"chunk_acc\": chunk_acc}\n",
    "    \n",
    "    def _split_booknames_stratified(self, df, nr_splits, return_indices=False):\n",
    "        label_splits = []\n",
    "        combined_splits = []\n",
    "        # Split df into folds for each label individualls\n",
    "        df_by_labels = df.groupby(\"y\")\n",
    "        for name, group in df_by_labels:\n",
    "            split = self._split_booknames(group, 5)\n",
    "            label_splits.append(split)\n",
    "        # Combine splits so that one split combines splits of all labels\n",
    "        for fold in range(0, nr_splits):\n",
    "            combined_split = []\n",
    "            for label in range(0, len(pd.unique(df[\"y\"]))):\n",
    "                label_split = label_splits[label]\n",
    "                fold_split = label_split[fold]\n",
    "                combined_split.extend(fold_split)\n",
    "            combined_splits.append(combined_split)\n",
    "        return combined_splits                            \n",
    "                             \n",
    "    def run(self):\n",
    "        train_accs = []\n",
    "        validation_accs = []\n",
    "        df = self.df\n",
    "        df = self._combine_df_labels(df)\n",
    "        book_names_split_stratified = self._split_booknames_stratified(df, nr_splits=5, return_indices=False)\n",
    "        all_validation_books = []\n",
    "\n",
    "        for index, split in enumerate(book_names_split_stratified):\n",
    "            train_df = df[~df[\"book_name\"].isin(split)]\n",
    "            validation_df = df[df[\"book_name\"].isin(split)]\n",
    "            train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            train_y = train_df[\"y\"].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            validation_y = validation_df[\"y\"].values.ravel()\n",
    "            train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            if self.model == \"xgboost\":\n",
    "                train_book_names = train_df[\"book_name\"].values.reshape(-1, 1)\n",
    "                best_parameters = self._get_model(self.model_param, train_X, train_y, train_book_names, task_type=\"binary_classification\")\n",
    "                class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1], y=train_y.astype(int).tolist())))\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y.astype(int), weight=[class_weights[int(i)] for i in train_y])\n",
    "                num_boost_round = best_parameters[\"num_boost_round\"]\n",
    "                best_parameters.pop(\"nested_cv_score\")\n",
    "                best_parameters.pop(\"num_boost_round\")\n",
    "                model = xgboost.train(best_parameters,\n",
    "                                      dtrain,\n",
    "                                      num_boost_round=num_boost_round,\n",
    "                                      verbose_eval=False)\n",
    "            else:\n",
    "                model = self._get_model(self.model_param)\n",
    "                model.fit(train_X, train_y)\n",
    "            \n",
    "            train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "            validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            \n",
    "            if self.model == \"xgboost\":\n",
    "                train_books[\"yhat\"] = model.predict(xgboost.DMatrix(train_X))\n",
    "                validation_books[\"yhat\"] = model.predict(xgboost.DMatrix(validation_X))\n",
    "            else:\n",
    "                train_books[\"yhat\"] = model.predict(train_X)\n",
    "                validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "\n",
    "            train_acc = self._aggregate_chunk_predictions(train_books)\n",
    "            validation_acc = self._aggregate_chunk_predictions(validation_books)\n",
    "            \n",
    "            all_validation_books.append(validation_books)\n",
    "            \n",
    "            train_accs.append(train_acc)\n",
    "            validation_accs.append(validation_acc)\n",
    "        \n",
    "        # Save y and y_pred for examples\n",
    "        all_validation_books = pd.concat(all_validation_books)\n",
    "        #all_validation_books.to_csv(results_dir + \"valiationbooks-class-\" + self.language + \"-\" + \".csv\", index=False)\n",
    "        \n",
    "        print(confusion_matrix(all_validation_books[\"y\"], all_validation_books[\"yhat\"]))\n",
    "        print(pd.crosstab(all_validation_books[\"y\"], all_validation_books[\"yhat\"], rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "        train_accs = pd.DataFrame(train_accs)\n",
    "        validation_accs = pd.DataFrame(validation_accs)\n",
    "\n",
    "        mean_train_mode_acc = train_accs[\"mode_acc\"].mean()\n",
    "        mean_train_book_acc = train_accs[\"book_acc\"].mean()\n",
    "        mean_train_chunk_acc = train_accs[\"chunk_acc\"].mean()\n",
    "        mean_validation_mode_acc = validation_accs[\"mode_acc\"].mean()\n",
    "        mean_validation_book_acc = validation_accs[\"book_acc\"].mean()\n",
    "        mean_validation_chunk_acc = validation_accs[\"chunk_acc\"].mean()\n",
    "        print('validation mode, book, and chunk acc', mean_validation_mode_acc, mean_validation_book_acc, mean_validation_chunk_acc)\n",
    "\n",
    "        return mean_train_book_acc, mean_validation_book_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f655dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Classification into in library/not in library\n",
    "'''\n",
    "\n",
    "class LibraryClassification(TwoclassClassification):\n",
    "    def _combine_df_labels(self, df):\n",
    "        df = df.merge(right=self.labels, on=\"book_name\", how=\"left\", validate=\"one_to_one\")\n",
    "        df[\"y\"] = df[\"y\"].fillna(0)\n",
    "        #Select books written after year first one appeared in a library catalogues\n",
    "        df[\"year\"] = df[\"book_name\"].str.replace('-', '_').str.split('_').str[-1].astype('int64')\n",
    "        helper_df = df.loc[df[\"y\"]!=0]\n",
    "        first_library_year = min(helper_df[\"y\"])\n",
    "        df = df.loc[df[\"year\"]>=first_library_year]\n",
    "        df = df.drop(columns=\"year\")\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a62768d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Classification into not reviewed/negative/not classified/positive\n",
    "'''\n",
    "\n",
    "class MulticlassClassification(TwoclassClassification):\n",
    "    def __init__(self, language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose):\n",
    "        super().__init__(language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose)\n",
    "\n",
    "    def _check_class_specific_assertions(self):\n",
    "        assert model in [\"svc\", \"xgboost\"]\n",
    "        assert features in [\"book\", \"book_and_averaged_chunk\"]#, \"chunk_and_copied_book\", \"chunk\"]\n",
    "                \n",
    "    def _combine_df_labels(self, df):\n",
    "        #Reviews zum englischen Korpus beginnnen mit 1759 und decken alles bis 1914 ab\n",
    "        df = df.merge(right=self.labels, on=\"book_name\", how=\"left\", validate=\"many_to_one\")\n",
    "        df[\"y\"] = df[\"y\"].fillna(value=0)\n",
    "        #Select books written after year of first review\n",
    "        year = df[\"book_name\"].str.replace('-', '_').str.split('_').str[-1].astype('int64')\n",
    "        df = df.loc[year>=min(year)]\n",
    "        return df\n",
    "    \n",
    "    def _evaluate_predictions(self, df):\n",
    "        score = f1_score(df[\"y\"], df[\"yhat\"], average='macro')\n",
    "        return score\n",
    "            \n",
    "        \n",
    "    def run(self):\n",
    "        train_f1s = []\n",
    "        validation_f1s = []\n",
    "\n",
    "        df = self.df\n",
    "        df = self._combine_df_labels(df)\n",
    "        book_names_split_stratified = self._split_booknames_stratified(df, nr_splits=5, return_indices=False)\n",
    "        all_validation_books = []\n",
    "\n",
    "        for index, split in enumerate(book_names_split_stratified):\n",
    "            train_df = df[~df[\"book_name\"].isin(split)]\n",
    "            validation_df = df[df[\"book_name\"].isin(split)]\n",
    "            print(\"class distribution over dfs\")\n",
    "            print(train_df[\"y\"].value_counts())\n",
    "            print(validation_df[\"y\"].value_counts())\n",
    "            #print(train_df.loc[train_df[\"y\"]==1])\n",
    "            print(validation_df.loc[validation_df[\"y\"]==1])\n",
    "            \n",
    "            train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            train_y = train_df[\"y\"].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            validation_y = validation_df[\"y\"].values.ravel()\n",
    "            train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            if self.model == \"xgboost\":\n",
    "                train_book_names = train_df[\"book_name\"].values.reshape(-1, 1)\n",
    "                best_parameters = self._get_model(self.model_param, train_X, train_y, train_book_names, task_type=\"multiclass_classification\")\n",
    "                class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1, 2, 3], y=train_y.astype(int).tolist())))\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y.astype(int), weight=[class_weights[int(i)] for i in train_y])\n",
    "                num_boost_round = best_parameters[\"num_boost_round\"]\n",
    "                best_parameters.pop(\"nested_cv_score\")\n",
    "                best_parameters.pop(\"num_boost_round\")\n",
    "                model = xgboost.train(best_parameters,\n",
    "                                      dtrain,\n",
    "                                      num_boost_round=num_boost_round,\n",
    "                                      verbose_eval=False)\n",
    "            else:\n",
    "                model = self._get_model(self.model_param)\n",
    "                model.fit(train_X, train_y)\n",
    "            \n",
    "            train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "            validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            \n",
    "            if self.model == \"xgboost\":\n",
    "                train_books[\"yhat\"] = model.predict(xgboost.DMatrix(train_X))\n",
    "                validation_books[\"yhat\"] = model.predict(xgboost.DMatrix(validation_X))\n",
    "            else:\n",
    "                train_books[\"yhat\"] = model.predict(train_X)\n",
    "                validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "            \n",
    "            train_f1 = self._evaluate_predictions(train_books)\n",
    "            validation_f1 = self._evaluate_predictions(validation_books)\n",
    "            all_validation_books.append(validation_books)\n",
    "            \n",
    "            train_f1s.append(train_f1)\n",
    "            validation_f1s.append(validation_f1)\n",
    "            if self.verbose:\n",
    "                print(f\"Fold: {index+1}, TrainF1: {np.round(train_f1, 3)}, ValF1: {np.round(validation_f1, 3)}\")\n",
    "        \n",
    "        # Save y and y_pred for examples\n",
    "        all_validation_books = pd.concat(all_validation_books)\n",
    "        #all_validation_books.to_csv(results_dir + \"valiationbooks-class-\" + self.language + \"-\" + \".csv\", index=False)\n",
    "        \n",
    "        print(confusion_matrix(all_validation_books[\"y\"], all_validation_books[\"yhat\"]))\n",
    "        print(pd.crosstab(all_validation_books[\"y\"], all_validation_books[\"yhat\"], rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "        mean_train_f1 = statistics.mean(train_f1s)\n",
    "        mean_validation_f1 = statistics.mean(validation_f1s)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\"\"TrainF1: {np.round(mean_train_f1, 3)}, ValidationF1: {np.round(mean_validation_f1, 3)}\"\"\")\n",
    "            print(\"\\n---------------------------------------------------\\n\")\n",
    "        return mean_train_f1, mean_validation_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f06b21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cross-validation\n",
    "\n",
    "# Feature split\n",
    "complexity_features = []\n",
    "\n",
    "# All parameters\n",
    "models = [\"svr\", \"lasso\", \"xgboost\", \"svc\"]\n",
    "model_params = {\"svr\": [1], \"lasso\": [1, 4], \"xgboost\": [None], \"svc\": [0.1, 1, 10, 100, 1000, 10000]} #\n",
    "dimensionality_reduction = [\"ss_pca_0_95\", 'k_best_f_reg_0_10', 'k_best_mutual_info_0_10', [None]]\n",
    "features = [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "\n",
    "# Which parameters to use\n",
    "regression_params = {\"model\": [\"xgboost\"], \"dimensionality_reduction\": dimensionality_reduction[-1], \"features\": features}\n",
    "testing_params = {\"model\": models[3], \"dimensionality_reduction\": dimensionality_reduction[-1], \n",
    "                  \"features\": [\"book\"]}\n",
    "twoclass_params = {\"model\": [\"svc\", \"xgboost\"], \"dimensionality_reduction\": dimensionality_reduction[-1], \n",
    "                  \"features\": [\"book\", \"book_and_averaged_chunk\"]}\n",
    "multiclass_params = {\"model\": [\"svc\", \"xgboost\"], \"dimensionality_reduction\": dimensionality_reduction[-1],\n",
    "                     \"features\": [\"book\", \"book_and_averaged_chunk\"]}\n",
    "library_params = {\"model\": [\"svc\", \"xgboost\"], \"dimensionality_reduction\": dimensionality_reduction[-1], \n",
    "                  \"features\": [\"book\", \"book_and_averaged_chunk\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd3d2f",
   "metadata": {},
   "source": [
    "book_df = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\")\n",
    "book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "book_df = drop_default_columns(book_df, drop_default_columns_including)\n",
    "book_and_averaged_chunk_df = drop_default_columns(book_and_averaged_chunk_df, drop_default_columns_including)\n",
    "chunk_df = drop_default_columns(chunk_df, drop_default_columns_including)\n",
    "chunk_and_copied_book_df = drop_default_columns(chunk_and_copied_book_df, drop_default_columns_including)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330757f4",
   "metadata": {},
   "source": [
    "print(len(book_df.columns), len(book_and_averaged_chunk_df.columns),len(chunk_df.columns),len(chunk_and_copied_book_df.columns),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7435d1",
   "metadata": {},
   "source": [
    "len(list(book_and_averaged_chunk_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e238a",
   "metadata": {},
   "source": [
    "for i in list(book_and_averaged_chunk_df.columns):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0889634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc\n",
      "Dropped 300 columns.\n",
      "[[ 58  91]\n",
      " [ 94 363]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         58   91  149\n",
      "1.0         94  363  457\n",
      "All        152  454  606\n",
      "validation mode, book, and chunk acc 0.7506124181932468 0.6939003689609058 0.6939003689609058\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 0.1 0.7098500069152862 0.6939003689609058\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 330 columns.\n",
      "[[ 53  96]\n",
      " [ 94 363]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         53   96  149\n",
      "1.0         94  363  457\n",
      "All        147  459  606\n",
      "validation mode, book, and chunk acc 0.7621188907908525 0.6897438423081241 0.6897438423081241\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 0.1 0.7106927156945592 0.6897438423081241\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 378 columns.\n",
      "[[ 65  84]\n",
      " [123 334]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         65   84  149\n",
      "1.0        123  334  457\n",
      "All        188  418  606\n",
      "validation mode, book, and chunk acc 0.6881774583111481 0.6585282928598437 0.6585282928598437\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 0.1 0.6709954872678352 0.6585282928598437\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 408 columns.\n",
      "[[ 63  86]\n",
      " [ 91 366]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         63   86  149\n",
      "1.0         91  366  457\n",
      "All        154  452  606\n",
      "validation mode, book, and chunk acc 0.7492011134511453 0.7101776346270069 0.7101776346270069\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 0.1 0.7010965080844169 0.7101776346270069\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "[[ 35 114]\n",
      " [101 356]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         35  114  149\n",
      "1.0        101  356  457\n",
      "All        136  470  606\n",
      "validation mode, book, and chunk acc 0.7710520641183303 0.6432216724536032 0.6432216724536032\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 0.1 0.6511427239020391 0.6432216724536032\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1198 columns.\n",
      "[[ 54  95]\n",
      " [143 314]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         54   95  149\n",
      "1.0        143  314  457\n",
      "All        197  409  606\n",
      "validation mode, book, and chunk acc 0.6756202699594899 0.6068486226060555 0.6068486226060555\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 0.1 0.6295642480042818 0.6068486226060555\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1246 columns.\n",
      "[[ 54  95]\n",
      " [195 262]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         54   95  149\n",
      "1.0        195  262  457\n",
      "All        249  357  606\n",
      "validation mode, book, and chunk acc 0.5887081216393855 0.5205190954401597 0.5205190954401597\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 0.1 0.6101691490485326 0.5205190954401597\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1276 columns.\n",
      "[[ 67  82]\n",
      " [177 280]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         67   82  149\n",
      "1.0        177  280  457\n",
      "All        244  362  606\n",
      "validation mode, book, and chunk acc 0.6056603409145782 0.5782249263605197 0.5782249263605197\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 0.1 0.6158356453440663 0.5782249263605197\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "[[ 90  59]\n",
      " [174 283]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         90   59  149\n",
      "1.0        174  283  457\n",
      "All        264  342  606\n",
      "validation mode, book, and chunk acc 0.5691258418476763 0.6242200464134662 0.6242200464134662\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 1 0.6385597202589487 0.6242200464134662\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 330 columns.\n",
      "[[ 60  89]\n",
      " [157 300]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         60   89  149\n",
      "1.0        157  300  457\n",
      "All        217  389  606\n",
      "validation mode, book, and chunk acc 0.6354871283585143 0.5907052347231808 0.5907052347231808\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 1 0.6669420555321359 0.5907052347231808\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 378 columns.\n",
      "[[ 69  80]\n",
      " [164 293]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         69   80  149\n",
      "1.0        164  293  457\n",
      "All        233  373  606\n",
      "validation mode, book, and chunk acc 0.6227383921052915 0.6034523845490944 0.6034523845490944\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 1 0.6635092260843094 0.6034523845490944\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 408 columns.\n",
      "[[ 84  65]\n",
      " [177 280]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         84   65  149\n",
      "1.0        177  280  457\n",
      "All        261  345  606\n",
      "validation mode, book, and chunk acc 0.5732282538040255 0.6027757350282721 0.6027757350282721\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 1 0.6368418449498626 0.6027757350282721\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "[[ 76  73]\n",
      " [174 283]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         76   73  149\n",
      "1.0        174  283  457\n",
      "All        250  356  606\n",
      "validation mode, book, and chunk acc 0.5828364524746371 0.5897293560974655 0.5897293560974655\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 1 0.6110968154290152 0.5897293560974655\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1198 columns.\n",
      "[[ 53  96]\n",
      " [166 291]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         53   96  149\n",
      "1.0        166  291  457\n",
      "All        219  387  606\n",
      "validation mode, book, and chunk acc 0.6417904743179804 0.57021535216134 0.57021535216134\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 1 0.6621963368654715 0.57021535216134\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1246 columns.\n",
      "[[ 78  71]\n",
      " [184 273]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         78   71  149\n",
      "1.0        184  273  457\n",
      "All        262  344  606\n",
      "validation mode, book, and chunk acc 0.5656137659455575 0.578160783700944 0.578160783700944\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 1 0.6087321751014614 0.578160783700944\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1276 columns.\n",
      "[[ 76  73]\n",
      " [173 284]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         76   73  149\n",
      "1.0        173  284  457\n",
      "All        249  357  606\n",
      "validation mode, book, and chunk acc 0.5895283741650367 0.5962076330079191 0.5962076330079191\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 1 0.6056224504085345 0.5962076330079191\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "[[ 76  73]\n",
      " [182 275]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         76   73  149\n",
      "1.0        182  275  457\n",
      "All        258  348  606\n",
      "validation mode, book, and chunk acc 0.5725916024819314 0.5766944039675844 0.5766944039675844\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 10 0.6528395204679112 0.5766944039675844\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 330 columns.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 77  72]\n",
      " [204 253]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         77   72  149\n",
      "1.0        204  253  457\n",
      "All        281  325  606\n",
      "validation mode, book, and chunk acc 0.5365002082766446 0.5451705414498442 0.5451705414498442\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 10 0.6532227910740455 0.5451705414498442\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 378 columns.\n",
      "[[ 87  62]\n",
      " [198 259]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         87   62  149\n",
      "1.0        198  259  457\n",
      "All        285  321  606\n",
      "validation mode, book, and chunk acc 0.53204559395642 0.5723756740518244 0.5723756740518244\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 10 0.6446176951805599 0.5723756740518244\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 408 columns.\n",
      "[[ 78  71]\n",
      " [189 268]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         78   71  149\n",
      "1.0        189  268  457\n",
      "All        267  339  606\n",
      "validation mode, book, and chunk acc 0.5587200668363197 0.570145866721286 0.570145866721286\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 10 0.6653193615437625 0.570145866721286\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "[[ 89  60]\n",
      " [220 237]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         89   60  149\n",
      "1.0        220  237  457\n",
      "All        309  297  606\n",
      "validation mode, book, and chunk acc 0.4912239824020874 0.5389165664381002 0.5389165664381002\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 10 0.5726988685035688 0.5389165664381002\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1198 columns.\n",
      "[[ 91  58]\n",
      " [216 241]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         91   58  149\n",
      "1.0        216  241  457\n",
      "All        307  299  606\n",
      "validation mode, book, and chunk acc 0.49326407290260005 0.5457818711747764 0.5457818711747764\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 10 0.5638755845522807 0.5457818711747764\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1246 columns.\n",
      "[[ 59  90]\n",
      " [185 272]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         59   90  149\n",
      "1.0        185  272  457\n",
      "All        244  362  606\n",
      "validation mode, book, and chunk acc 0.5947947997709917 0.542501435043049 0.542501435043049\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 10 0.6395277088513744 0.542501435043049\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1276 columns.\n",
      "[[ 90  59]\n",
      " [227 230]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         90   59  149\n",
      "1.0        227  230  457\n",
      "All        317  289  606\n",
      "validation mode, book, and chunk acc 0.47671110775887354 0.5282633733327108 0.5282633733327108\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 10 0.5668567839600158 0.5282633733327108\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "[[ 74  75]\n",
      " [173 284]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         74   75  149\n",
      "1.0        173  284  457\n",
      "All        247  359  606\n",
      "validation mode, book, and chunk acc 0.58946565520381 0.5869215313769436 0.5869215313769436\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 100 0.7380546231233599 0.5869215313769436\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 330 columns.\n",
      "[[ 69  80]\n",
      " [182 275]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         69   80  149\n",
      "1.0        182  275  457\n",
      "All        251  355  606\n",
      "validation mode, book, and chunk acc 0.5915351674066509 0.5722261761648113 0.5722261761648113\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 100 0.7211522414449227 0.5722261761648113\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 378 columns.\n",
      "[[ 82  67]\n",
      " [188 269]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         82   67  149\n",
      "1.0        188  269  457\n",
      "All        270  336  606\n",
      "validation mode, book, and chunk acc 0.5561302144372446 0.5833833821208284 0.5833833821208284\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 100 0.7259410869596439 0.5833833821208284\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 408 columns.\n",
      "[[ 70  79]\n",
      " [189 268]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         70   79  149\n",
      "1.0        189  268  457\n",
      "All        259  347  606\n",
      "validation mode, book, and chunk acc 0.577384087055074 0.5634253223236275 0.5634253223236275\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 100 0.7245585047536918 0.5634253223236275\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "[[ 84  65]\n",
      " [204 253]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         84   65  149\n",
      "1.0        204  253  457\n",
      "All        288  318  606\n",
      "validation mode, book, and chunk acc 0.5225788694102311 0.5569549751946125 0.5569549751946125\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 100 0.604725089252968 0.5569549751946125\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1198 columns.\n",
      "[[ 81  68]\n",
      " [236 221]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         81   68  149\n",
      "1.0        236  221  457\n",
      "All        317  289  606\n",
      "validation mode, book, and chunk acc 0.47939541134582875 0.5019797150018427 0.5019797150018427\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 100 0.596938051044132 0.5019797150018427\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1246 columns.\n",
      "[[ 65  84]\n",
      " [190 267]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         65   84  149\n",
      "1.0        190  267  457\n",
      "All        255  351  606\n",
      "validation mode, book, and chunk acc 0.5783707597128753 0.5470519869748656 0.5470519869748656\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 100 0.6223175438076575 0.5470519869748656\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1276 columns.\n",
      "[[ 88  61]\n",
      " [200 257]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         88   61  149\n",
      "1.0        200  257  457\n",
      "All        288  318  606\n",
      "validation mode, book, and chunk acc 0.523049728049728 0.5676340326340327 0.5676340326340327\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 100 0.5967400964682941 0.5676340326340327\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "[[ 68  81]\n",
      " [173 284]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         68   81  149\n",
      "1.0        173  284  457\n",
      "All        241  365  606\n",
      "validation mode, book, and chunk acc 0.5984796248173774 0.5780355153106602 0.5780355153106602\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 1000 0.8049364969442727 0.5780355153106602\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 330 columns.\n",
      "[[ 74  75]\n",
      " [178 279]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         74   75  149\n",
      "1.0        178  279  457\n",
      "All        252  354  606\n",
      "validation mode, book, and chunk acc 0.5874842445977133 0.5846519380333587 0.5846519380333587\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 1000 0.7954275129555338 0.5846519380333587\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 378 columns.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 83  66]\n",
      " [189 268]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         83   66  149\n",
      "1.0        189  268  457\n",
      "All        272  334  606\n",
      "validation mode, book, and chunk acc 0.5570447631464581 0.5859993239654255 0.5859993239654255\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 1000 0.782723674626902 0.5859993239654255\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 408 columns.\n",
      "[[ 86  63]\n",
      " [197 260]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         86   63  149\n",
      "1.0        197  260  457\n",
      "All        283  323  606\n",
      "validation mode, book, and chunk acc 0.5328424935787659 0.5707162647360899 0.5707162647360899\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 1000 0.7706136979826441 0.5707162647360899\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "[[ 77  72]\n",
      " [206 251]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         77   72  149\n",
      "1.0        206  251  457\n",
      "All        283  323  606\n",
      "validation mode, book, and chunk acc 0.5304598420289042 0.5400348329264463 0.5400348329264463\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 1000 0.6336429089765202 0.5400348329264463\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1198 columns.\n",
      "[[ 70  79]\n",
      " [176 281]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         70   79  149\n",
      "1.0        176  281  457\n",
      "All        246  360  606\n",
      "validation mode, book, and chunk acc 0.5955679051068627 0.5788764559427213 0.5788764559427213\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 1000 0.6432742176994143 0.5788764559427213\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1246 columns.\n",
      "[[ 76  73]\n",
      " [172 285]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         76   73  149\n",
      "1.0        172  285  457\n",
      "All        248  358  606\n",
      "validation mode, book, and chunk acc 0.588942463038568 0.5954652420651885 0.5954652420651885\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 1000 0.6435947003318045 0.5954652420651885\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1276 columns.\n",
      "[[ 74  75]\n",
      " [179 278]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         74   75  149\n",
      "1.0        179  278  457\n",
      "All        253  353  606\n",
      "validation mode, book, and chunk acc 0.582768142129171 0.5818649612970667 0.5818649612970667\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 1000 0.6490645265859192 0.5818649612970667\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "[[ 70  79]\n",
      " [138 319]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         70   79  149\n",
      "1.0        138  319  457\n",
      "All        208  398  606\n",
      "validation mode, book, and chunk acc 0.6561436483896377 0.6408004855364481 0.6408004855364481\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 10000 0.8514146715654582 0.6408004855364481\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 330 columns.\n",
      "[[ 86  63]\n",
      " [171 286]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         86   63  149\n",
      "1.0        171  286  457\n",
      "All        257  349  606\n",
      "validation mode, book, and chunk acc 0.5738187268784284 0.6120767198341193 0.6120767198341193\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 10000 0.8486594325957462 0.6120767198341193\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 378 columns.\n",
      "[[ 80  69]\n",
      " [148 309]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         80   69  149\n",
      "1.0        148  309  457\n",
      "All        228  378  606\n",
      "validation mode, book, and chunk acc 0.6242162401683837 0.6424495075960036 0.6424495075960036\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 10000 0.8234988784654329 0.6424495075960036\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 408 columns.\n",
      "[[ 74  75]\n",
      " [158 299]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         74   75  149\n",
      "1.0        158  299  457\n",
      "All        232  374  606\n",
      "validation mode, book, and chunk acc 0.6131283001921087 0.6135505986203892 0.6135505986203892\n",
      "eng svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 10000 0.8142100911468997 0.6135505986203892\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "[[ 69  80]\n",
      " [138 319]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         69   80  149\n",
      "1.0        138  319  457\n",
      "All        207  399  606\n",
      "validation mode, book, and chunk acc 0.6592730711628607 0.6407338964534268 0.6407338964534268\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 10000 0.6881235674160547 0.6407338964534268\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1198 columns.\n",
      "[[ 65  84]\n",
      " [139 318]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         65   84  149\n",
      "1.0        139  318  457\n",
      "All        204  402  606\n",
      "validation mode, book, and chunk acc 0.6604220055210595 0.6304363898489918 0.6304363898489918\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 10000 0.682356354075683 0.6304363898489918\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1246 columns.\n",
      "[[ 64  85]\n",
      " [165 292]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         64   85  149\n",
      "1.0        165  292  457\n",
      "All        229  377  606\n",
      "validation mode, book, and chunk acc 0.6180034271754085 0.5842699078781746 0.5842699078781746\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 10000 0.6788219581701785 0.5842699078781746\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1276 columns.\n",
      "[[ 66  83]\n",
      " [133 324]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         66   83  149\n",
      "1.0        133  324  457\n",
      "All        199  407  606\n",
      "validation mode, book, and chunk acc 0.6722784115590128 0.643241603302448 0.643241603302448\n",
      "eng svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 10000 0.6873188316923555 0.643241603302448\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "xgboost\n",
      "Dropped 300 columns.\n",
      "[[ 58  91]\n",
      " [122 335]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         58   91  149\n",
      "1.0        122  335  457\n",
      "All        180  426  606\n",
      "validation mode, book, and chunk acc 0.7012515262515262 0.6436005171299289 0.6436005171299289\n",
      "eng xgboost book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= None 0.9966551618681463 0.6436005171299289\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 330 columns.\n",
      "[[ 70  79]\n",
      " [ 97 360]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         70   79  149\n",
      "1.0         97  360  457\n",
      "All        167  439  606\n",
      "validation mode, book, and chunk acc 0.7195761595960998 0.7068549175428537 0.7068549175428537\n",
      "eng xgboost book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= None 0.988527387775439 0.7068549175428537\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 378 columns.\n",
      "[[ 62  87]\n",
      " [110 347]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         62   87  149\n",
      "1.0        110  347  457\n",
      "All        172  434  606\n",
      "validation mode, book, and chunk acc 0.7118581710686974 0.671032804484817 0.671032804484817\n",
      "eng xgboost book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= None 0.989770001767275 0.671032804484817\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 408 columns.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 69  80]\n",
      " [ 98 359]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         69   80  149\n",
      "1.0         98  359  457\n",
      "All        167  439  606\n",
      "validation mode, book, and chunk acc 0.7252020569836519 0.7074844056676131 0.7074844056676131\n",
      "eng xgboost book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= None 0.9843042948110012 0.7074844056676131\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "[[ 53  96]\n",
      " [105 352]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         53   96  149\n",
      "1.0        105  352  457\n",
      "All        158  448  606\n",
      "validation mode, book, and chunk acc 0.7394680343750692 0.6686305120894315 0.6686305120894315\n",
      "eng xgboost book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= None 0.9937836334859049 0.6686305120894315\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1198 columns.\n",
      "[[ 55  94]\n",
      " [113 344]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         55   94  149\n",
      "1.0        113  344  457\n",
      "All        168  438  606\n",
      "validation mode, book, and chunk acc 0.7258785433080471 0.6611142116228106 0.6611142116228106\n",
      "eng xgboost book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= None 0.997103760029902 0.6611142116228106\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1246 columns.\n",
      "[[ 63  86]\n",
      " [109 348]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         63   86  149\n",
      "1.0        109  348  457\n",
      "All        172  434  606\n",
      "validation mode, book, and chunk acc 0.7168381108594339 0.6788218039174846 0.6788218039174846\n",
      "eng xgboost book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= None 0.9737381527333742 0.6788218039174846\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1276 columns.\n",
      "[[ 60  89]\n",
      " [120 337]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0.0         60   89  149\n",
      "1.0        120  337  457\n",
      "All        180  426  606\n",
      "validation mode, book, and chunk acc 0.7085038118726528 0.659861768079221 0.659861768079221\n",
      "eng xgboost book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= None 0.9639344262295083 0.659861768079221\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run two-class classification\n",
    "'''\n",
    "results = []\n",
    "param_dict = \"library\" #\"twoclass\"\n",
    "for lang in [lang]:\n",
    "    if param_dict == \"testing\":\n",
    "        param_dir = testing_params\n",
    "    elif param_dict == \"twoclass\":\n",
    "        param_dir = twoclass_params\n",
    "    elif param_dict == \"multiclass\":\n",
    "        param_dir = multiclass_params\n",
    "    elif param_dict == \"full_cv\":\n",
    "        param_dir = full_cv_params\n",
    "    elif param_dict == \"library\":\n",
    "        param_dir = library_params\n",
    "    #Eng: 606 books, 14146 chunks, 13170 chunks of books published after 1759\n",
    "    book_df = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\")\n",
    "    book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "    chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "    chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "\n",
    "    for model in [] + param_dir['model']:\n",
    "        print(model)\n",
    "        model_param = model_params[model]\n",
    "        for model_param in model_param:\n",
    "            for dimensionality_reduction in param_dir[\"dimensionality_reduction\"]:\n",
    "                for features in param_dir[\"features\"]:\n",
    "                    for drop_columns_including in [[\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\"],\n",
    "                                                   [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\"],\n",
    "                                                   [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"pos\"],\n",
    "                                                   [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\", \"pos\"]]:\n",
    "                        if param_dict ==\"library\":\n",
    "                            labels = library_scores\n",
    "                            experiment = LibraryClassification(\n",
    "                                language=lang,\n",
    "                                features=features,\n",
    "                                drop_columns_including=drop_columns_including,\n",
    "                                dimensionality_reduction=dimensionality_reduction,\n",
    "                                model_param=model_param,\n",
    "                                model=model,\n",
    "                                verbose=True\n",
    "                            )\n",
    "                        else:\n",
    "                            experiment = TwoclassClassification(\n",
    "                                language=lang,\n",
    "                                features=features,\n",
    "                                drop_columns_including=drop_columns_including,\n",
    "                                dimensionality_reduction=dimensionality_reduction,\n",
    "                                model_param=model_param,\n",
    "                                model=model,\n",
    "                                verbose=True\n",
    "                            )\n",
    "                        mean_train_book_acc, mean_validation_book_acc = experiment.run()\n",
    "                        print(lang, model, features, drop_columns_including, dimensionality_reduction, 'param=', model_param, mean_train_book_acc, mean_validation_book_acc)\n",
    "                        print('\\n-----------------------------------------------------------\\n')\n",
    "                        results.append((lang, model, features, drop_columns_including, dimensionality_reduction, model_param, mean_train_book_acc, mean_validation_book_acc))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"lang\", \"model\", \"features\", \"drop_columns_including\", \n",
    "\"dimensionality_reduction\", \"model_param\", \"mean_train_book_acc\", \"mean_validation_book_acc\"])\n",
    "results_df.to_csv(results_dir + lang + '_' + param_dict + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1b62524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/results/classification_library/eng_library'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dir + lang + '_' + param_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d86da650",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# Run Multiclass Classification\n",
    "# '''\n",
    "# results = []\n",
    "# param_dict = \"multiclass\" \n",
    "# for lang in [lang]:    \n",
    "#     if param_dict == \"testing\":\n",
    "#         param_dir = testing_params\n",
    "#         model_params = {\"svr\": [1], \"lasso\": [1], \"xgboost\": [None], \"svc\": [1]} \n",
    "#     elif param_dict == \"multiclass\":\n",
    "#         param_dir = multiclass_params\n",
    "#     elif param_dict == \"full_cv\":\n",
    "#         param_dir = full_cv_params\n",
    "#     elif param_dict == \"language_specific\":\n",
    "#         if lang == \"eng\":\n",
    "#             param_dir = eng_params\n",
    "#         else: \n",
    "#             param_dir = ger_params\n",
    "    \n",
    "#     #Eng: 606 books, 14146 chunks, 13170 chunks of books published after 1759\n",
    "#     book_df = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\")\n",
    "#     book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "#     #chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "#     #chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "    \n",
    "#     for model in [] + param_dir['model']:\n",
    "#         model_param = model_params[model]\n",
    "#         for model_param in model_param:\n",
    "#             for dimensionality_reduction in param_dir[\"dimensionality_reduction\"]:\n",
    "#                 for features in param_dir[\"features\"]:\n",
    "#                     for drop_columns_including in [[\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"pos\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\", \"pos\"]]:\n",
    "#                                                 #try:\n",
    "#                         print(param_dict, lang, model, features, drop_columns_including, dimensionality_reduction, 'param=', model_param)\n",
    "#                         experiment = MulticlassClassification(\n",
    "#                             language=lang,\n",
    "#                             features=features,\n",
    "#                             drop_columns_including=drop_columns_including,\n",
    "#                             dimensionality_reduction=dimensionality_reduction,\n",
    "#                             model_param=model_param,\n",
    "#                             model=model,\n",
    "#                             verbose=True\n",
    "#                         )\n",
    "#                         mean_train_f1, mean_validation_f1 = experiment.run()\n",
    "#                         results.append((lang, model, features, drop_columns_including, dimensionality_reduction, model_param, mean_train_f1, mean_validation_f1))\n",
    "#                         print(param_dict, lang, model, features, drop_columns_including, dimensionality_reduction, 'param=', model_param, mean_train_f1, mean_validation_f1)\n",
    "# results_df = pd.DataFrame(results, columns=[\"lang\", \"model\", \"features\", \"drop_columns_including\", \n",
    "# \"dimensionality_reduction\", \"model_param\", \"mean_train_f1\", \"mean_validation_f1\"])\n",
    "# results_df.to_csv(results_dir + lang + '_' + param_dict + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb06cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Run Regression\n",
    "# '''\n",
    "# results = []\n",
    "# param_dict = \"regression\" \n",
    "# for lang in [lang]:\n",
    "#     if param_dict == \"regression\":\n",
    "#         param_dir = regression_params\n",
    "#     if param_dict == \"testing\":\n",
    "#         param_dir = testing_params\n",
    "#     elif param_dict == \"multiclass\":\n",
    "#         param_dir = multiclass_params\n",
    "#     elif param_dict == \"full_cv\":\n",
    "#         param_dir = full_cv_params\n",
    "#     elif param_dict == \"language_specific\":\n",
    "#         if lang == \"eng\":\n",
    "#             param_dir = eng_params\n",
    "#         else: \n",
    "#             param_dir = ger_params\n",
    "    \n",
    "#     #Eng: 606 books, 14146 chunks, 13170 chunks of books published after 1759\n",
    "#     book_df = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\")\n",
    "#     book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "#     chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "#     chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "\n",
    "#     for model in param_dir['model']:\n",
    "#         model_param = model_params[model]\n",
    "#         for model_param in model_param:\n",
    "#             for dimensionality_reduction in param_dir[\"dimensionality_reduction\"]:\n",
    "#                 for features in param_dir[\"features\"]:\n",
    "#                     for drop_columns_including in [[\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"pos\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\", \"pos\"]]:\n",
    "#                         print(param_dict, lang, model, features, drop_columns_including, dimensionality_reduction, 'param=', model_param)\n",
    "#                         experiment = Regression(\n",
    "#                             language=lang,\n",
    "#                             features=features,\n",
    "#                             drop_columns_including=drop_columns_including,\n",
    "#                             dimensionality_reduction=dimensionality_reduction,\n",
    "#                             model_param=model_param,\n",
    "#                             model=model,\n",
    "#                             verbose=True\n",
    "#                         )\n",
    "#                         mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value = experiment.run()\n",
    "#                         results.append((lang, model, features, drop_columns_including, dimensionality_reduction, model_param, mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value))\n",
    "#                         #except Exception as e:\n",
    "# #                             print(f\"Error in {lang}, {model}, {features}, {drop_columns_including}, {dimensionality_reduction}\")\n",
    "# #                             print(e)\n",
    "# results_df = pd.DataFrame(results, columns=[\"lang\", \"model\", \"features\", \"drop_columns_including\", \n",
    "# \"dimensionality_reduction\", \"model_param\", \"mean_train_mse\", \"mean_train_rmse\", \n",
    "# \"mean_train_mae\", \"mean_train_r2\", \"mean_train_corr\", \"mean_validation_mse\", \"mean_validation_rmse\",\n",
    "# \"mean_validation_mae\", \"mean_validation_r2\", \"mean_validation_corr\", \"mean_p_value\"])\n",
    "# results_df.to_csv(results_dir + lang + '_' + 'regression_' + param_dict + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ef4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
