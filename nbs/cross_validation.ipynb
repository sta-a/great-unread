{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15df4db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsentiment file names for eng/ger\\nMerge labels and features depending on how labels are aggregated if there are multiple scores for a work.\\ndrop_column reset index???\\nchunk based features?\\ncomplexity features\\ntake out doc2vec_chunk_embedding from default drop columns\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "sentiment file names for eng/ger\n",
    "Merge labels and features depending on how labels are aggregated if there are multiple scores for a work.\n",
    "drop_column reset index???\n",
    "chunk based features?\n",
    "complexity features\n",
    "take out doc2vec_chunk_embedding from default drop columns\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c12bfc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../src/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "extracted_features_dir = \"../data/extracted_features/\"\n",
    "results_dir = \"../data/results/\"\n",
    "sentiment_dir = \"../data/evaluationscore/\"\n",
    "canonization_labels_dir = \"../data/labels/\"\n",
    "lang = \"eng\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2696499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR, SVC\n",
    "import xgboost\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBRegressor\n",
    "from copy import deepcopy\n",
    "from scipy.stats import pearsonr\n",
    "from utils import read_sentiment_scores, read_library_scores\n",
    "from math import sqrt\n",
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\") # %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import statistics\n",
    "import datetime\n",
    "import random\n",
    "random.seed(9)\n",
    "\n",
    "labels = read_sentiment_scores(sentiment_dir, canonization_labels_dir, lang)\n",
    "library_scores = read_library_scores(sentiment_dir, canonization_labels_dir, lang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4594ed42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_name</th>\n",
       "      <th>y</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austen_Jane_Pride-and-Prejudice_1813</td>\n",
       "      <td>0.029731</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austen_Jane_Sense-and-Sensibility_1811</td>\n",
       "      <td>0.049103</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barrie_J-M_Auld-Licht-Idylls_1888</td>\n",
       "      <td>0.016107</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barrie_J-M_Sentimental-Tommy_1896</td>\n",
       "      <td>0.038327</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Beerbohm_Max_Zuleika-Dobson_1911</td>\n",
       "      <td>0.053656</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Wollstonecraft_Mary_Mary_1788</td>\n",
       "      <td>0.040233</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Scott_Walter_The-Black-Dwarf_1816</td>\n",
       "      <td>-0.021840</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Beckford_William_Vathek_1786</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Radcliffe_Ann_Udolpho_1794</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>Smollett_Tobias_Humphry-Clinker_1771</td>\n",
       "      <td>0.010482</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  book_name         y  c\n",
       "1      Austen_Jane_Pride-and-Prejudice_1813  0.029731  3\n",
       "2    Austen_Jane_Sense-and-Sensibility_1811  0.049103  3\n",
       "3         Barrie_J-M_Auld-Licht-Idylls_1888  0.016107  3\n",
       "4         Barrie_J-M_Sentimental-Tommy_1896  0.038327  3\n",
       "5          Beerbohm_Max_Zuleika-Dobson_1911  0.053656  3\n",
       "..                                      ...       ... ..\n",
       "148           Wollstonecraft_Mary_Mary_1788  0.040233  3\n",
       "170       Scott_Walter_The-Black-Dwarf_1816 -0.021840  2\n",
       "177            Beckford_William_Vathek_1786  0.006286  2\n",
       "234              Radcliffe_Ann_Udolpho_1794  0.004301  2\n",
       "241    Smollett_Tobias_Humphry-Clinker_1771  0.010482  2\n",
       "\n",
       "[191 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ab0ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels[\"c\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93415ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 in labels[\"c\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764429a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "## Labels statistics\n",
    "print(len(pd.unique(labels[\"book_name\"]))) #197\n",
    "\n",
    "# 254 labels, 197 different book_names -> 57 second/third... reviews\n",
    "# 36 book_names with more than 1 label, these 36 book_names have 93 labels\n",
    "# 93 = 36 first reviews + 57 second/third... reviews\n",
    "# 6 texts with opposing reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "933bfe04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD4CAYAAAD2FnFTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWzklEQVR4nO3dfZBlZX3g8e/PQRRp5CVIiwPasMuSRUYpaGG3okm3LzgCSl7YZChiGKMZ40vVbi2pddQkWiapIi9ozJINTJRVTKRJYogEiOzApkGrdHGGjMygEEaY7E7DMuHFgcYp2ZHf/nFPp6/Nc7tvv5x7+nZ/P1W37jnPec55fve59/avz8t9TmQmkiTN9IKmA5AkLU8mCElSkQlCklRkgpAkFZkgJElFhzQdwFI69thjc2hoqNY2nnnmGQ4//PBa2+gX9sU0+6LFfpjWL32xffv2xzLzZaVlKypBDA0NsW3btlrbGB8fZ2RkpNY2+oV9Mc2+aLEfpvVLX0TEP3Va5iEmSVKRCUKSVGSCkCQVmSAkSUUmCElSkQlCklRkgpAkFZkgJElFJghJUtGK+iW1tFwNbb65WL7n8vN7HInUPfcgJElFte1BRMQ1wAXAvsw8vSq7Hji1qnIU8L3MPKOw7h7gaeCHwMHMHK4rTklSWZ2HmD4HXAlcO1WQmb8wNR0RVwD7Z1l/NDMfqy06SdKsaksQmXlnRAyVlkVEAD8PvLGu9iVJixOZWd/GWwnipqlDTG3lPwl8stOho4h4CHgSSODqzNwySxubgE0Ag4ODZ42NjS1R9GWTk5MMDAzU2ka/sC+mzdUXOyfKO8vr1h5ZV0iN8DMxrV/6YnR0dHunv8VNXcV0MXDdLMtfn5kTEXEcsDUi7svMO0sVq+SxBWB4eDjrHn+9X8Z47wX7YtpcfbGx01VMl3Repx/5mZi2Evqi51cxRcQhwM8C13eqk5kT1fM+4Abg7N5EJ0ma0sRlrm8G7svMvaWFEXF4RBwxNQ2cC+zqYXySJGpMEBFxHfB14NSI2BsR764WbWDG4aWIeEVE3FLNDgJfi4hvAXcBN2fmV+qKU5JUVudVTBd3KN9YKHsYOK+afhB4bV1xSZK641Ab0jLk0BxaDhxqQ5JUZIKQJBWZICRJRSYISVKRCUKSVGSCkCQVmSAkSUUmCElSkQlCklRkgpAkFZkgJElFjsUkqSuOD7X6uAchSSoyQUiSikwQkqQiE4QkqcgEIUkqMkFIkopMEJKkotoSRERcExH7ImJXW9nHI2IiInZUj/M6rLs+Iu6PiN0RsbmuGCVJndW5B/E5YH2h/FOZeUb1uGXmwohYA/wx8DbgNODiiDitxjglSQW1JYjMvBN4YgGrng3szswHM/NZYAy4cEmDkyTNKTKzvo1HDAE3Zebp1fzHgY3AU8A24LLMfHLGOhcB6zPzPdX8O4FzMvODHdrYBGwCGBwcPGtsbKyW1zJlcnKSgYGBWtvoF/bFtLn6YufE/mL5urVHNlJ/Ibppw8/EtH7pi9HR0e2ZOVxa1uuxmP4E+C0gq+crgF9ezAYzcwuwBWB4eDhHRkYWGeLsxsfHqbuNfmFfTJurLzZ2GsfokvI6dddfiG7a8DMxbSX0RU+vYsrMRzPzh5n5HPCntA4nzTQBnNg2f0JVJknqoZ4miIg4vm32Z4BdhWrfBE6JiJMi4lBgA3BjL+KTJE2r7RBTRFwHjADHRsRe4GPASEScQesQ0x7gvVXdVwCfyczzMvNgRHwQuBVYA1yTmffWFackqay2BJGZFxeKP9uh7sPAeW3ztwDPuwRWktQ7/pJaklRkgpAkFZkgJElFJghJUpEJQpJUZIKQJBWZICRJRSYISVKRCUKSVGSCkCQVmSAkSUUmCElSkQlCklRkgpAkFZkgJElFJghJUpEJQpJUZIKQJBWZICRJRSYISVJRbQkiIq6JiH0Rsaut7Pcj4r6IuCciboiIozqsuycidkbEjojYVleMkqTO6tyD+BywfkbZVuD0zHwN8I/Ah2dZfzQzz8jM4ZrikyTNorYEkZl3Ak/MKPsfmXmwmv0GcEJd7UuSFicys76NRwwBN2Xm6YVlfwtcn5l/Vlj2EPAkkMDVmbllljY2AZsABgcHzxobG1ui6MsmJycZGBiotY1+YV9Mm6svdk7sL5avW3tkI/UXops2/ExM65e+GB0d3d7pSM0hvQ4GICI+ChwE/rxDlddn5kREHAdsjYj7qj2S56mSxxaA4eHhHBkZqSPkfzE+Pk7dbfQL+2LaXH2xcfPNxfI9l5TXqbv+QnTThp+JaSuhL3p+FVNEbAQuAC7JDrsvmTlRPe8DbgDO7lmAkiSgxwkiItYD/wV4R2Z+v0OdwyPiiKlp4FxgV6muJKk+dV7meh3wdeDUiNgbEe8GrgSOoHXYaEdEXFXVfUVE3FKtOgh8LSK+BdwF3JyZX6krTklSWW3nIDLz4kLxZzvUfRg4r5p+EHhtXXFJkrrTyElqSc0a6nDCGWDP5ef3MBItZw61IUkqMkFIkopMEJKkIhOEJKnIBCFJKuoqQUTEuroDkSQtL93uQfy3iLgrIt4fEUs3+pckadnqKkFk5huAS4ATge0R8cWIeEutkUmSGtX1OYjMfAD4deBDwE8Bf1TdHe5n6wpOktScbs9BvCYiPgV8B3gj8PbM/LfV9KdqjE+S1JBuh9r4r8BngI9k5oGpwsx8OCJ+vZbIpFVgtiEvpKZ1myDOBw5k5g8BIuIFwIsz8/uZ+YXaopMkNabbcxC3AYe1zb+kKpMkrVDdJogXZ+bk1Ew1/ZJ6QpIkLQfdJohnIuLMqZmIOAs4MEt9SVKf6/YcxH8C/jIiHgYCeDnwC3UFJUlqXlcJIjO/GRE/DpxaFd2fmf+vvrAkSU2bzx3lXgcMVeucGRFk5rW1RCVJalxXCSIivgD8K2AH8MOqOAEThCStUN3uQQwDp2VmzmfjEXENcAGwLzNPr8qOAa6ntTeyB/j5zHyysO6ltIb2APjtzPz8fNqWJC1Ot1cx7aJ1Ynq+Pgesn1G2Gbg9M08Bbq/mf0SVRD4GnAOcDXwsIo5eQPuSpAXqdg/iWODbEXEX8IOpwsx8x2wrZeadETE0o/hCYKSa/jwwTmsAwHZvBbZm5hMAEbGVVqK5rst4JUmLFN0cNYqInyqVZ+YdXaw7BNzUdojpe5l5VDUdwJNT823r/BqtH+f9djX/G7SG+viDwvY3AZsABgcHzxobG5vz9SzG5OQkAwMDtbbRL+yLaXP1xc6J/T2MZtq6teXbt8wWz3zXaa/vZ2Jav/TF6Ojo9swcLi3r9jLXOyLiVcApmXlbRLwEWLPYwDIzI2Je5zUK29gCbAEYHh7OkZGRxYY1q/Hxcepuo1/YF9Pm6ouNDQ3Kt+eSkWL5bPHMd532+n4mpq2Evuh2uO9fAf4KuLoqWgv8zQLbfDQijq+2ezywr1BngtbNiaacUJVJknqk25PUHwB+AngK/uXmQcctsM0bgUur6UuBLxfq3AqcGxFHVyenz63KJEk90m2C+EFmPjs1ExGH0PodxKwi4jrg68CpEbE3It4NXA68JSIeAN5czRMRwxHxGYDq5PRvAd+sHp+YOmEtSeqNbq9iuiMiPgIcVt2L+v3A3861UmZe3GHRmwp1twHvaZu/Brimy/gkSUus2z2IzcA/AzuB9wK3MP0jNknSCtTtVUzPAX9aPSRJq0C3YzE9ROGcQ2aevOQRSZKWhfmMxTTlxcB/AI5Z+nAkSctFV+cgMvPxtsdEZv4hcH69oUmSmtTtIaYz22ZfQGuPYj73kpBWlKEZvyq+bN1BNm6+mT2X9///TTNfm1avbv/IX9E2fZBqmO4lj0aStGx0exXTaN2BSJKWl24PMf3n2ZZn5ieXJhxJ0nIxn6uYXkdrHCWAtwN3AQ/UEZQkqXndJogTgDMz82mAiPg4cHNm/mJdgUmSmtXtUBuDwLNt889WZZKkFarbPYhrgbsi4oZq/qdp3S5UkrRCdXsV0+9ExN8Bb6iK3pWZ/1BfWJKkpnV7iAngJcBTmflpYG9EnFRTTJKkZaDbW45+DPgQ8OGq6IXAn9UVlCSped3uQfwM8A7gGYDMfBg4oq6gJEnN6/Yk9bOZmRGRABFxeI0xSeoj7WM3TY1JBayIcalWu273IP4iIq4GjoqIXwFuw5sHSdKKNuceREQEcD3w48BTwKnAb2bm1oU0GBGnVtubcnK1vT9sqzMCfBl4qCr668z8xELakyQtzJwJojq0dEtmrgMWlBRmbO9+4AyAiFgDTAA3FKp+NTMvWGx7kqSF6fYQ090R8boa2n8T8N3M/Kcati1JWoRuE8Q5wDci4rsRcU9E7IyIe5ag/Q3AdR2W/fuI+FZE/F1EvHoJ2pIkzUNkZueFEa/MzP8dEa8qLV/Mf/4RcSjwMPDqzHx0xrKXAs9l5mREnAd8OjNP6bCdTcAmgMHBwbPGxsYWGlJXJicnGRgYqLWNfrGa+2LnxP4fmR88DB49AOvWHtlV/V7pdTxT/TBb26tFv3w/RkdHt2fmcGnZXAni7sw8s5r+Umb+3FIFFREXAh/IzHO7qLsHGM7Mx2arNzw8nNu2bVuiCMvGx8cZGRmptY1+sZr7onTL0St2HtLx0s6mbuPZ63im+mG2tleLfvl+RETHBDHXIaZomz556UIC4GI6HF6KiJdXV08REWfTivPxJW5fkjSLua5iyg7Ti1L90O4twHvbyn4VIDOvAi4C3hcRB4EDwIacbVdHkrTk5koQr42Ip2jtSRxWTVPNZ2a+dCGNZuYzwI/NKLuqbfpK4MqFbFuStDRmTRCZuaZXgUjL0XyP1Td1rqGT5RaP+st8hvuWJK0iJghJUpEJQpJUZIKQJBWZICRJRSYISVKRCUKSVGSCkCQVmSAkSUUmCElSkQlCklQ05z2ppdXAMYuk53MPQpJUZIKQJBWZICRJRSYISVKRCUKSVGSCkCQVmSAkSUWNJYiI2BMROyNiR0RsKyyPiPijiNgdEfdExJlNxClJq1XTP5QbzczHOix7G3BK9TgH+JPqWZLUA8v5ENOFwLXZ8g3gqIg4vumgJGm1iMxspuGIh4AngQSuzswtM5bfBFyemV+r5m8HPpSZ22bU2wRsAhgcHDxrbGys1rgnJycZGBiotY1+sZL6YufE/kWtP3gYPHpgiYLpY+39sG7tkc0G07B++X6Mjo5uz8zh0rImDzG9PjMnIuI4YGtE3JeZd853I1Vi2QIwPDycIyMjSxzmjxofH6fuNvrFSuqLjYsci+mydQe5YmfTR2yb194Pey4ZaTaYhq2E70djh5gyc6J63gfcAJw9o8oEcGLb/AlVmSSpBxpJEBFxeEQcMTUNnAvsmlHtRuCXqquZ/h2wPzMf6XGokrRqNbVPPAjcEBFTMXwxM78SEb8KkJlXAbcA5wG7ge8D72ooVklalRpJEJn5IPDaQvlVbdMJfKCXcUmSpi3ny1wlSQ0yQUiSikwQkqQiE4QkqcgEIUkqMkFIkoocG0ArztAih83Q8tLp/dxz+fk9jmT1cQ9CklRkgpAkFZkgJElFJghJUpEJQpJUZIKQJBWZICRJRSYISVKRCUKSVGSCkCQVOdSGlj2HzlhZfD/7h3sQkqQiE4QkqajnCSIiToyIv4+Ib0fEvRHxHwt1RiJif0TsqB6/2es4JWm1a+IcxEHgssy8OyKOALZHxNbM/PaMel/NzAsaiE+SRAN7EJn5SGbeXU0/DXwHWNvrOCRJs4vMbK7xiCHgTuD0zHyqrXwE+BKwF3gY+LXMvLfDNjYBmwAGBwfPGhsbqzXmyclJBgYGam2jX/SqL3ZO7K+9jcUaPAwePdB0FM1r74d1a48s1lmq97PT9peLfvlbMTo6uj0zh0vLGksQETEA3AH8Tmb+9YxlLwWey8zJiDgP+HRmnjLXNoeHh3Pbtm31BFwZHx9nZGSk1jb6Ra/6oh8ui7xs3UGu2OlV4+390OmOb0v1fi73O8r1y9+KiOiYIBq5iikiXkhrD+HPZyYHgMx8KjMnq+lbgBdGxLE9DlOSVrUmrmIK4LPAdzLzkx3qvLyqR0ScTSvOx3sXpSSpiX3inwDeCeyMiB1V2UeAVwJk5lXARcD7IuIgcADYkE2eLJGkVajnCSIzvwbEHHWuBK7sTUSSpBLPqi1QpxNty/3E2XLWDyej1b2630+/g/VzqA1JUpEJQpJUZIKQJBWZICRJRSYISVKRCUKSVGSCkCQVmSAkSUUmCElSkQlCklTkUBuVun+2vxyHBZhvTDPrX7buIBs33zzv1+CQGup3TX2fe92uexCSpCIThCSpyAQhSSoyQUiSikwQkqQiE4QkqcgEIUkqMkFIkooaSRARsT4i7o+I3RGxubD8RRFxfbX8f0XEUANhStKq1vMEERFrgD8G3gacBlwcEafNqPZu4MnM/NfAp4Df7W2UkqQm9iDOBnZn5oOZ+SwwBlw4o86FwOer6b8C3hQR0cMYJWnVi8zsbYMRFwHrM/M91fw7gXMy84NtdXZVdfZW89+t6jxW2N4mYFM1eypwf80v4VjgeXGsUvbFNPuixX6Y1i998arMfFlpQd8P1peZW4AtvWovIrZl5nCv2lvO7Itp9kWL/TBtJfRFE4eYJoAT2+ZPqMqKdSLiEOBI4PGeRCdJAppJEN8ETomIkyLiUGADcOOMOjcCl1bTFwH/M3t9LEySVrmeH2LKzIMR8UHgVmANcE1m3hsRnwC2ZeaNwGeBL0TEbuAJWklkuejZ4aw+YF9Msy9a7Idpfd8XPT9JLUnqD/6SWpJUZIKQJBWZIAoi4piI2BoRD1TPR3eo95WI+F5E3DSj/KRqiJDd1ZAhh/Ym8qU3j764tKrzQERc2lY+Xg2rsqN6HNe76BdvMcPCRMSHq/L7I+KtPQ28Bgvti4gYiogDbZ+Bq3oe/BLroi9+MiLujoiD1W+/2pcVvyvLUmb6mPEAfg/YXE1vBn63Q703AW8HbppR/hfAhmr6KuB9Tb+mOvsCOAZ4sHo+upo+ulo2Dgw3/ToW+NrXAN8FTgYOBb4FnDajzvuBq6rpDcD11fRpVf0XASdV21nT9GtqqC+GgF1Nv4Ye98UQ8BrgWuCitvKO35Xl+HAPoqx9qI/PAz9dqpSZtwNPt5dVQ4K8kdYQIbOu3ye66Yu3Alsz84nMfBLYCqzvTXi1WsywMBcCY5n5g8x8CNhdba9fOUTOtDn7IjP3ZOY9wHMz1u2r74oJomwwMx+ppv8vMDiPdX8M+F5mHqzm9wJrlzK4HuumL9YC/6dtfuZr/u/VoYXf6LM/GHO9rh+pU73n+2l9BrpZt58spi8AToqIf4iIOyLiDXUHW7PFvLd99bno+6E2FioibgNeXlj00faZzMyIWNHXAtfcF5dk5kREHAF8CXgnrd1urR6PAK/MzMcj4izgbyLi1Zn5VNOBaXarNkFk5ps7LYuIRyPi+Mx8JCKOB/bNY9OPA0dFxCHVf1GloUSWlSXoiwlgpG3+BFrnHsjMier56Yj4Iq3d835JEPMZFmbvjGFhulm3nyy4L7J18P0HAJm5vRp8898A22qPuh6LeW87fleWIw8xlbUP9XEp8OVuV6y+DH9Pa4iQea+/DHXTF7cC50bE0dVVTucCt0bEIRFxLEBEvBC4ANjVg5iXymKGhbkR2FBd2XMScApwV4/irsOC+yIiXhat+8AQESfT6osHexR3Hbrpi06K35Wa4ly8ps+SL8cHreOmtwMPALcBx1Tlw8Bn2up9Ffhn4ACtY4lvrcpPpvXHYDfwl8CLmn5NPeiLX65e727gXVXZ4cB24B7gXuDT9NmVPMB5wD/Sumrlo1XZJ4B3VNMvrt7j3dV7fnLbuh+t1rsfeFvTr6WpvgB+rnr/dwB3A29v+rX0oC9eV/1NeIbWHuW9bes+77uyXB8OtSFJKvIQkySpyAQhSSoyQUiSikwQkqQiE4QkqcgEIUkqMkFIkor+PxUfMqL312HBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels[\"y\"].plot.hist(grid=True, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b1785d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression(object):\n",
    "    def __init__(self, language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose):\n",
    "        assert isinstance(drop_columns_including, list)\n",
    "        for i in drop_columns_including:\n",
    "            assert isinstance(i, str)\n",
    "        assert (dimensionality_reduction in [\"k_best_f_reg_0_10\", \"k_best_mutual_info_0_10\", \"ss_pca_0_95\"]) or (dimensionality_reduction is None)\n",
    "        self._check_class_specific_assertions()\n",
    "        \n",
    "        self.language = language\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.labels = self._prepare_labels()\n",
    "        self.drop_columns_including = drop_columns_including\n",
    "        self.dimensionality_reduction = dimensionality_reduction\n",
    "        self.model_param = model_param\n",
    "        self.model = model\n",
    "        self.verbose = verbose\n",
    "        self.datetime = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "        if self.features == \"book\":\n",
    "            self.df = deepcopy(book_df)\n",
    "        elif self.features == \"chunk\":\n",
    "            self.df = deepcopy(chunk_df)\n",
    "        elif self.features == \"chunk_and_copied_book\":\n",
    "            self.df = deepcopy(chunk_and_copied_book_df)\n",
    "        elif self.features == \"book_and_averaged_chunk\":\n",
    "            self.df = deepcopy(book_and_averaged_chunk_df)\n",
    "\n",
    "        columns_before_drop = set(self.df.columns)\n",
    "        if self.drop_columns_including:\n",
    "            self.df = self.df[[column for column in self.df.columns if not self._drop_column(column)]].reset_index(drop=True)\n",
    "        columns_after_drop = set(self.df.columns)\n",
    "        if self.verbose:\n",
    "            print(f\"Dropped {len(columns_before_drop - columns_after_drop)} columns.\")\n",
    "            \n",
    "    def _check_class_specific_assertions(self):\n",
    "        assert model in [\"xgboost\", \"svr\", \"lasso\"]\n",
    "        assert features in [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "    \n",
    "    def _prepare_labels(self):\n",
    "        return self.labels.drop(columns=\"c\")\n",
    "\n",
    "    def _drop_column(self, column):\n",
    "        for string in self.drop_columns_including:\n",
    "            if string in column:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def _custom_pca(self, train_X):\n",
    "        for i in range(5, train_X.shape[1], int((train_X.shape[1] - 5) / 10)):\n",
    "            pca = PCA(n_components=i)\n",
    "            new_train_X = pca.fit_transform(train_X)\n",
    "            if pca.explained_variance_ratio_.sum() >= 0.95:\n",
    "                break\n",
    "        return new_train_X, pca\n",
    "\n",
    "    def _select_features(self, train_X, train_y, validation_X):\n",
    "        if self.dimensionality_reduction == \"ss_pca_0_95\":\n",
    "            ss = StandardScaler()\n",
    "            train_X = ss.fit_transform(train_X)\n",
    "            validation_X = ss.transform(validation_X)\n",
    "            train_X, pca = self._custom_pca(train_X)\n",
    "            validation_X = pca.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_f_reg_0_10\":\n",
    "            k_best = SelectKBest(f_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_mutual_info_0_10\":\n",
    "            k_best = SelectKBest(mutual_info_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction is None:\n",
    "            pass\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _impute(self, train_X, validation_X):\n",
    "        imputer = KNNImputer()\n",
    "        train_X = imputer.fit_transform(train_X)\n",
    "        validation_X = imputer.transform(validation_X)\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _get_model(self, model_param, train_X=None, train_y=None, train_book_names=None, classification=None):\n",
    "        if self.model == \"xgboost\":\n",
    "            def feval(preds, train_data):\n",
    "                labels = train_data.get_label().astype(int)\n",
    "                if classification:\n",
    "                    preds = preds.argmax(axis=1).astype(int)\n",
    "                if classification == True:\n",
    "                    return 'f1', f1_score(labels, preds, average='macro')\n",
    "                else:\n",
    "                    return 'rmse', np.sqrt(mean_squared_error(labels, preds))\n",
    "            class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1, 2, 3], y=train_y.astype(int).tolist())))\n",
    "            dtrain = xgboost.DMatrix(train_X, label=train_y.astype(int), weight=[class_weights[int(i)] for i in train_y])\n",
    "            results = []\n",
    "            df = np.hstack((train_book_names, train_X))\n",
    "            df = pd.DataFrame(df, columns=[\"book_name\"] + [f\"col_{i}\" for i in range(train_X.shape[1])])\n",
    "            for max_depth in [2, 4, 6, 8]:\n",
    "                for learning_rate in [None, 0.01, 0.033, 0.1]:\n",
    "                    for colsample_bytree in [0.33, 0.60, 0.75]:\n",
    "                        if classification:\n",
    "                            params = {\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"n_jobs\": -1, \"objective\": \"multi:softmax\", \"num_class\": 4, \"eval_metric\": \"mlogloss\"}\n",
    "                        else:\n",
    "                            params = {\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"n_jobs\": -1}\n",
    "                        print(\"current_params:\", params)\n",
    "                        cv_results = xgboost.cv(\n",
    "                                        params,\n",
    "                                        dtrain,\n",
    "                                        num_boost_round=99999,\n",
    "                                        seed=42,\n",
    "                                        nfold=5,\n",
    "                                        folds=self._split_booknames(df, 5, return_indices=True),\n",
    "                                        feval=feval,\n",
    "                                        maximize=classification, # if classification, maximize f1 score.\n",
    "                                        early_stopping_rounds=10,\n",
    "                                        verbose_eval=False)\n",
    "                        if classification:\n",
    "                            nested_cv_score = cv_results.iloc[len(cv_results)-1][\"test-f1-mean\"]\n",
    "                        else:\n",
    "                            nested_cv_score = cv_results.iloc[len(cv_results)-1][\"test-rmse-mean\"]\n",
    "                        num_boost_round = len(cv_results)\n",
    "                        if classification:\n",
    "                            results.append({\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"num_boost_round\": num_boost_round, \"nested_cv_score\": nested_cv_score, \"objective\": \"multi:softmax\", \"num_class\": 4, \"eval_metric\": \"mlogloss\"})\n",
    "                        else:\n",
    "                            results.append({\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"num_boost_round\": num_boost_round, \"nested_cv_score\": nested_cv_score})\n",
    "            best_parameters = sorted(results, key=lambda x: x[\"nested_cv_score\"], reverse=classification)[0]\n",
    "            return best_parameters\n",
    "        elif self.model == \"svr\":\n",
    "            return SVR(C=model_param)\n",
    "        elif self.model == \"lasso\":\n",
    "            return Lasso(alpha=model_param)\n",
    "        elif self.model == \"svc\":\n",
    "            return SVC(C=model_param)\n",
    "        \n",
    "    def _split_booknames(self, df, nr_splits, return_indices=False):\n",
    "        \"\"\"\n",
    "        Distribute book names over splits.\n",
    "        All works of an author are in the same split.\n",
    "        \"\"\"\n",
    "        book_names = df[\"book_name\"].unique()\n",
    "        authors = []\n",
    "        booknames_authors_mapping = {}\n",
    "\n",
    "        #Get authors\n",
    "        for book_name in book_names:\n",
    "            author = \"_\".join(book_name.split(\"_\")[:2])\n",
    "            authors.append(author)\n",
    "            if author in booknames_authors_mapping:\n",
    "                booknames_authors_mapping[author].append(book_name)\n",
    "            else:\n",
    "                booknames_authors_mapping[author] = []\n",
    "                booknames_authors_mapping[author].append(book_name)\n",
    "        #Distribute authors over splits so that each split has approximately the same number of books\n",
    "        works_per_author = Counter(authors)\n",
    "        goal_sum = round(len(book_names)/nr_splits)\n",
    "        tolerance = 0.03\n",
    "        lower_threshold = goal_sum - round(tolerance*goal_sum)\n",
    "        upper_threshold = goal_sum + round(tolerance*goal_sum)\n",
    "        author_splits = []\n",
    "        popped_dict = {}\n",
    "\n",
    "        for i in range (0, nr_splits-1):\n",
    "            works_in_split = 0\n",
    "            split = []\n",
    "            curr_author_workcount = 0\n",
    "\n",
    "            # take values from popped dict first\n",
    "            if bool(popped_dict):  \n",
    "                popped = []\n",
    "                for curr_author, curr_author_workcount in popped_dict.items():\n",
    "                    # leave item in popped dict if value is too big\n",
    "                    if works_in_split + curr_author_workcount > upper_threshold:\n",
    "                        continue\n",
    "                    else:\n",
    "                        popped.append(curr_author)\n",
    "                        split.append(curr_author)\n",
    "                        works_in_split += curr_author_workcount\n",
    "                        if works_in_split >= lower_threshold:\n",
    "                            break\n",
    "                for current_author in popped:\n",
    "                    del popped_dict[current_author]\n",
    "            while works_in_split < upper_threshold:\n",
    "                if bool(works_per_author):\n",
    "                    curr_author = random.choice(list(works_per_author.keys()))\n",
    "                    curr_author_workcount = works_per_author.pop(curr_author)\n",
    "                    # Put values into separate dict if too big\n",
    "                    if works_in_split + curr_author_workcount > upper_threshold:\n",
    "                        popped_dict[curr_author] = curr_author_workcount\n",
    "                    else:\n",
    "                        split.append(curr_author)\n",
    "                        works_in_split += curr_author_workcount\n",
    "                        if works_in_split >= lower_threshold:\n",
    "                            break\n",
    "                else:\n",
    "                    #ignore upper threshold\n",
    "                    popped = []\n",
    "                    for curr_author, curr_author_workcount in popped_dict.items():\n",
    "                        popped.append(curr_author)\n",
    "                        split.append(curr_author)\n",
    "                        works_in_split += curr_author_workcount\n",
    "                        if works_in_split >= lower_threshold:\n",
    "                            break\n",
    "                    for current_author in popped:\n",
    "                        del popped_dict[current_author]\n",
    "\n",
    "            author_splits.append(split)\n",
    "        #Create last split directly from remaining dict\n",
    "        works_in_last_split = sum(works_per_author.values()) + sum(popped_dict.values())\n",
    "        split = list(works_per_author.keys()) + list(popped_dict.keys())\n",
    "        author_splits.append(split)\n",
    "\n",
    "        if not return_indices:\n",
    "            #Map author splits to book names\n",
    "            book_splits = []\n",
    "            for author_split in author_splits:\n",
    "                book_split = []\n",
    "                for author in author_split:\n",
    "                    book_split.extend(booknames_authors_mapping[author])\n",
    "                book_splits.append(book_split)\n",
    "        else:\n",
    "            book_name_idx_mapping = dict((book_name, index) for index, book_name in enumerate(book_names))\n",
    "            book_splits = []\n",
    "            for author_split in author_splits:\n",
    "                test_split = []\n",
    "                for author in author_split:\n",
    "                    test_split.extend([book_name_idx_mapping[book_name] for book_name in booknames_authors_mapping[author]])\n",
    "                train_split = list(set(book_name_idx_mapping.values()) - set(test_split))\n",
    "                book_splits.append((train_split, test_split))\n",
    "        return book_splits\n",
    "    \n",
    "    def _combine_df_labels(self, df):\n",
    "        #Average of sentiscores per book\n",
    "        agg_labels = self.labels.groupby(by=\"book_name\", as_index=False).mean()\n",
    "        df = df.merge(right=agg_labels, on=\"book_name\", how=\"inner\", validate=\"many_to_one\")\n",
    "        return df\n",
    "    \n",
    "    def run(self):\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        train_mses = []\n",
    "        train_maes = []\n",
    "        train_r2s = []\n",
    "        train_corrs = []\n",
    "        \n",
    "        validation_mses = []\n",
    "        validation_maes = []\n",
    "        validation_r2s = []\n",
    "        validation_corrs = []\n",
    "        validation_corr_pvalues = []\n",
    "\n",
    "        df = self.df\n",
    "        df = self._combine_df_labels(df)\n",
    "        book_names_split = self._split_booknames(df, 5)\n",
    "        all_validation_books = []\n",
    "\n",
    "        for index, split in enumerate(book_names_split):\n",
    "            train_df = df[~df[\"book_name\"].isin(split)]\n",
    "            validation_df = df[df[\"book_name\"].isin(split)]\n",
    "            \n",
    "            train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            train_y = train_df[\"y\"].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            validation_y = validation_df[\"y\"].values.ravel()\n",
    "            train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            if self.model == \"xgboost\":\n",
    "                train_book_names = train_df[\"book_name\"].values.reshape(-1, 1)\n",
    "                best_parameters = self._get_model(self.model_param, train_X, train_y, train_book_names, classification=False)\n",
    "                class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1, 2, 3], y=train_y.astype(int).tolist())))\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y.astype(int), weight=[class_weights[int(i)] for i in train_y])\n",
    "                num_boost_round = best_parameters[\"num_boost_round\"]\n",
    "                best_parameters.pop(\"nested_cv_score\")\n",
    "                best_parameters.pop(\"num_boost_round\")\n",
    "                model = xgboost.train(best_parameters,\n",
    "                                      dtrain,\n",
    "                                      num_boost_round=num_boost_round,\n",
    "                                      verbose_eval=False)\n",
    "            else:\n",
    "                model = self._get_model(self.model_param)\n",
    "                model.fit(train_X, train_y)\n",
    "            \n",
    "            train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "            validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            \n",
    "            if self.model == \"xgboost\":\n",
    "                train_books[\"yhat\"] = model.predict(xgboost.DMatrix(train_X))\n",
    "                validation_books[\"yhat\"] = model.predict(xgboost.DMatrix(validation_X))\n",
    "            else:\n",
    "                train_books[\"yhat\"] = model.predict(train_X)\n",
    "                validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "            \n",
    "            train_books = train_books.groupby(\"book_name\").mean()\n",
    "            validation_books = validation_books.groupby(\"book_name\").mean()\n",
    "            all_validation_books.append(validation_books.reset_index())\n",
    "            \n",
    "            train_y = train_books[\"y\"].tolist()\n",
    "            train_yhat = train_books[\"yhat\"].tolist()\n",
    "            validation_y = validation_books[\"y\"].tolist()\n",
    "            validation_yhat = validation_books[\"yhat\"].tolist()\n",
    "            \n",
    "            all_labels.extend(validation_y)\n",
    "            all_predictions.extend(validation_yhat)\n",
    "            \n",
    "            train_mse = mean_squared_error(train_y, train_yhat)\n",
    "            train_mae = mean_absolute_error(train_y, train_yhat)\n",
    "            train_r2 = r2_score(train_y, train_yhat)\n",
    "            train_corr = pearsonr(train_y, train_yhat)[0]\n",
    "            \n",
    "            validation_mse = mean_squared_error(validation_y, validation_yhat)\n",
    "            validation_mae = mean_absolute_error(validation_y, validation_yhat)\n",
    "            validation_r2 = r2_score(validation_y, validation_yhat)\n",
    "            validation_corr = pearsonr(validation_y, validation_yhat)[0]\n",
    "            p_value = pearsonr(validation_y, validation_yhat)[1]\n",
    "            \n",
    "            train_mses.append(train_mse)\n",
    "            train_maes.append(train_mae)\n",
    "            train_r2s.append(train_r2)\n",
    "            train_corrs.append(train_corr)\n",
    "            \n",
    "            validation_mses.append(validation_mse)\n",
    "            validation_maes.append(validation_mae)\n",
    "            validation_r2s.append(validation_r2)\n",
    "            validation_corrs.append(validation_corr)\n",
    "            validation_corr_pvalues.append(p_value)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Fold: {index+1}, TrainMSE: {np.round(train_mse, 3)}, TrainMAE: {np.round(train_mae, 3)}, ValMSE: {np.round(validation_mse, 3)}, ValMAE: {np.round(validation_mae, 3)}, ValR2: {np.round(validation_r2, 3)}, ValCorr: {np.round(validation_corr, 3)}\")\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        \n",
    "        # Save y and y_pred for examples\n",
    "        pd.concat(all_validation_books).to_csv(results_dir + \"/y-yhat-\" + self.language + \"-\" + self.datetime + \".csv\", index=False)\n",
    "        \n",
    "        mean_train_mse = np.mean(train_mses)\n",
    "        mean_train_rmse = np.mean([sqrt(x) for x in train_mses])\n",
    "        mean_train_mae = np.mean(train_maes)\n",
    "        mean_train_r2 = np.mean(train_r2s)\n",
    "        mean_train_corr = np.mean(train_corrs)\n",
    "        \n",
    "        mean_validation_mse = np.mean(validation_mses)\n",
    "        mean_validation_rmse = np.mean([sqrt(x) for x in validation_mses])\n",
    "        mean_validation_mae = np.mean(validation_maes)\n",
    "        mean_validation_r2 = np.mean(validation_r2s)\n",
    "        mean_validation_corr = np.mean(validation_corrs)\n",
    "        mean_p_value = self._get_pvalue(validation_corr_pvalues)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\"\"TrainMSE: {np.round(mean_train_mse, 3)}, TrainRMSE: {np.round(mean_train_rmse, 3)}, TrainMAE: {np.round(mean_train_mae, 3)}, TrainR2: {np.round(mean_train_r2, 3)}, TrainCorr: {np.round(mean_train_corr, 3)}, ValMSE: {np.round(mean_validation_mse, 3)}, ValRMSE: {np.round(mean_validation_rmse, 3)}, ValMAE: {np.round(mean_validation_mae, 3)}, ValR2: {np.round(mean_validation_r2, 3)}, ValCorr: {np.round(mean_validation_corr, 3)}, ValCorrPValue: {np.round(mean_p_value, 3)}\"\"\")\n",
    "            print(\"\\n---------------------------------------------------\\n\")\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.xticks(fontsize=15)\n",
    "            plt.yticks(fontsize=15)\n",
    "            plt.xlim([0,1])\n",
    "            plt.ylim([0,1])\n",
    "\n",
    "            plt.scatter(all_labels, all_predictions, s=6)\n",
    "            plt.xlabel(\"Canonization Scores\", fontsize=20)\n",
    "            plt.ylabel(\"Predicted Scores\", fontsize=20)\n",
    "            plt.savefig(results_dir + lang + \"-\" + self.model + \"-\" + str(self.dimensionality_reduction) \n",
    "            + \"-\" + self.features + \"-\" + \"-\" + \"param\" + str(self.model_param) + \"-\" + self.datetime + \".png\", \n",
    "            dpi=400, bbox_inches=\"tight\")    \n",
    "    \n",
    "            plt.show();\n",
    "        return mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value, self.datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "340d71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Classification into reviewed/not reviewed\n",
    "'''\n",
    "\n",
    "class Classification(Regression):\n",
    "    def __init__(self, language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose):\n",
    "        super().__init__(language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose)\n",
    "\n",
    "    def _check_class_specific_assertions(self):\n",
    "        assert model in [\"svc\", \"xgboost\"]\n",
    "        assert features in [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "        \n",
    "    def _prepare_labels(self):\n",
    "        labels = self.labels.drop(columns=\"y\").rename(columns={\"c\":\"y\"})\n",
    "        labels = labels.replace(to_replace={\"positive\": 3, \"not_classified\": 2, \"negative\": 1})\n",
    "        labels = labels.drop_duplicates(subset=\"book_name\")\n",
    "        return labels\n",
    "        \n",
    "    def _combine_df_labels(self, df):\n",
    "        #Reviews zum englischen Korpus beginnnen mit 1759 und decken alles bis 1914 ab\n",
    "        agg_labels = self.labels[[\"book_name\"]].drop_duplicates()\n",
    "        agg_labels[\"y\"] = 1\n",
    "        df = df.merge(right=agg_labels, on=\"book_name\", how=\"left\", validate=\"many_to_one\")\n",
    "        df[\"y\"] = df[\"y\"].fillna(value=0)\n",
    "        #Select books written after 1759 (year of first review)\n",
    "        year = df[\"book_name\"].str.replace('-', '_').str.split('_').str[-1].astype('int64')\n",
    "        df = df.loc[year>=1759]\n",
    "        return df\n",
    "    \n",
    "    def _get_sample_weights(self, df):\n",
    "        # Weight \n",
    "        chunks_per_book = df[\"book_name\"].value_counts(sort=False).rename('chunks_per_book')\n",
    "        chunks_per_book = chunks_per_book.reset_index().rename(columns={\"index\":'book_name'})\n",
    "        chunks_per_book[\"chunks_per_book\"] = 1/chunks_per_book[\"chunks_per_book\"]\n",
    "        df = df.merge(right=chunks_per_book, how=\"left\", on=\"book_name\")\n",
    "        sample_weights = df[\"chunks_per_book\"].tolist()\n",
    "        return sample_weights\n",
    "    \n",
    "    def _aggregate_chunk_predictions(self, df):\n",
    "        g = df.groupby(\"book_name\") \n",
    "        \n",
    "        # Majority vote\n",
    "        # If one value is more common, assign it to every chunk\n",
    "        # Therefore, accuracy is either 0 or 1\n",
    "        # If both values are equally likely, leave them unchanged, and accuracy is 0.5\n",
    "        def _get_mode_accuracy(group):\n",
    "            counts = group[\"yhat\"].value_counts()\n",
    "            if len(counts) == 1:\n",
    "                mode_acc = counts.index[0]\n",
    "            else:\n",
    "                mode_acc = 0.5\n",
    "            return mode_acc\n",
    "        mode_accs = g.apply(_get_mode_accuracy).rename(\"mode_acc\").reset_index() \n",
    "        mode_acc = mode_accs[\"mode_acc\"].mean()\n",
    "        \n",
    "        # Average accuracy within book\n",
    "        book_acc = g.apply(lambda group: accuracy_score(group[\"y\"], group[\"yhat\"])).mean()\n",
    "        #Accuracy when each chunk is treated as single document\n",
    "        chunk_acc = accuracy_score(df[\"y\"], df[\"yhat\"])#, sample_weight = self._get_sample_weights(df))\n",
    "        return {\"mode_acc\": mode_acc, \"book_acc\": book_acc, \"chunk_acc\": chunk_acc}\n",
    "        \n",
    "    def run(self):\n",
    "        train_accs = []\n",
    "        validation_accs = []\n",
    "\n",
    "        df = self.df\n",
    "        df = self._combine_df_labels(df)\n",
    "        book_names_split = self._split_booknames(df, 5)\n",
    "        all_validation_books = []\n",
    "\n",
    "        for index, split in enumerate(book_names_split):\n",
    "            train_df = df[~df[\"book_name\"].isin(split)]\n",
    "            validation_df = df[df[\"book_name\"].isin(split)]\n",
    "            \n",
    "            train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            train_y = train_df[\"y\"].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            validation_y = validation_df[\"y\"].values.ravel()\n",
    "            train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            if self.model == \"xgboost\":\n",
    "                train_book_names = train_df[\"book_name\"].values.reshape(-1, 1)\n",
    "                best_parameters = self._get_model(self.model_param, train_X, train_y, train_book_names, classification=True)\n",
    "                class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1, 2, 3], y=train_y.astype(int).tolist())))\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y.astype(int), weight=[class_weights[int(i)] for i in train_y])\n",
    "                num_boost_round = best_parameters[\"num_boost_round\"]\n",
    "                best_parameters.pop(\"nested_cv_score\")\n",
    "                best_parameters.pop(\"num_boost_round\")\n",
    "                model = xgboost.train(best_parameters,\n",
    "                                      dtrain,\n",
    "                                      num_boost_round=num_boost_round,\n",
    "                                      verbose_eval=False)\n",
    "            else:\n",
    "                model = self._get_model(self.model_param)\n",
    "                model.fit(train_X, train_y)\n",
    "            \n",
    "            if self.model == \"xgboost\":\n",
    "                train_books[\"yhat\"] = model.predict(xgboost.DMatrix(train_X))\n",
    "                validation_books[\"yhat\"] = model.predict(xgboost.DMatrix(validation_X))\n",
    "            else:\n",
    "                train_books[\"yhat\"] = model.predict(train_X)\n",
    "                validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "\n",
    "            train_acc = self._aggregate_chunk_predictions(train_books)\n",
    "            validation_acc = self._aggregate_chunk_predictions(validation_books)\n",
    "            \n",
    "            all_validation_books.append(validation_books)\n",
    "            \n",
    "            train_accs.append(train_acc)\n",
    "            validation_accs.append(validation_acc)\n",
    "            if self.verbose:\n",
    "                print(f\"Fold: {index+1}, TrainAcc: {np.round(train_acc, 3)}, ValAcc: {np.round(validation_acc, 3)}\")\n",
    "        \n",
    "        # Save y and y_pred for examples\n",
    "        all_validation_books = pd.concat(all_validation_books)\n",
    "        all_validation_books.to_csv(results_dir + \"/valiationbooks-class-\" + self.language + \"-\" + self.datetime + \".csv\", index=False)\n",
    "        \n",
    "        print(confusion_matrix(all_validation_books[\"y\"], all_validation_books[\"yhat\"]))\n",
    "        print(pd.crosstab(all_validation_books[\"y\"], all_validation_books[\"yhat\"], rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "                \n",
    "        train_accs = pd.DataFrame(train_accs)\n",
    "        validation_accs = pd.DataFrame(validation_accs)\n",
    "        \n",
    "        mean_train_mode_acc = train_accs[\"mode_acc\"].mean()\n",
    "        mean_train_book_acc = train_accs[\"book_acc\"].mean()\n",
    "        mean_train_chunk_acc = train_accs[\"chunk_acc\"].mean()\n",
    "        mean_validation_mode_acc = validation_accs[\"mode_acc\"].mean()\n",
    "        mean_validation_book_acc = validation_accs[\"book_acc\"].mean()\n",
    "        mean_validation_chunk_acc = validation_accs[\"chunk_acc\"].mean()\n",
    "        print(mean_train_mode_acc, mean_train_book_acc, mean_train_chunk_acc)\n",
    "        print(mean_validation_mode_acc, mean_validation_book_acc, mean_validation_chunk_acc)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\"\"TrainAcc: {np.round(mean_train_acc, 3)}, ValidationAcc: {np.round(mean_validation_acc, 3)}\"\"\")\n",
    "            print(\"\\n---------------------------------------------------\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de317b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Classification into in library/not in library\n",
    "'''\n",
    "\n",
    "class LibraryClassification(Classification):\n",
    "    def _prepare_labels(self):\n",
    "        return self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0170865",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Classification into not reviewed/negative/not classified/positive\n",
    "'''\n",
    "\n",
    "class MulticlassClassification(Regression):\n",
    "    def __init__(self, language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose):\n",
    "        super().__init__(language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose)\n",
    "\n",
    "    def _check_class_specific_assertions(self):\n",
    "        assert model in [\"svc\", \"xgboost\"]\n",
    "        assert features in [\"book\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\", \"chunk\"]\n",
    "        \n",
    "    def _prepare_labels(self):\n",
    "        labels = self.labels.drop(columns=\"y\").rename(columns={\"c\":\"y\"})\n",
    "        return labels\n",
    "        \n",
    "    def _combine_df_labels(self, df):\n",
    "        #Reviews zum englischen Korpus beginnnen mit 1759 und decken alles bis 1914 ab\n",
    "        df = df.merge(right=self.labels, on=\"book_name\", how=\"left\", validate=\"many_to_one\")\n",
    "        df[\"y\"] = df[\"y\"].fillna(value=0)#(value=\"not_reviewed\")\n",
    "        #Select books written after 1759 (year of first review)\n",
    "        year = df[\"book_name\"].str.replace('-', '_').str.split('_').str[-1].astype('int64')\n",
    "        df = df.loc[year>=1759]\n",
    "        return df\n",
    "    \n",
    "    def _get_sample_weights(self, df):\n",
    "        # Weight \n",
    "        chunks_per_book = df[\"book_name\"].value_counts(sort=False).rename('chunks_per_book')\n",
    "        chunks_per_book = chunks_per_book.reset_index().rename(columns={\"index\":'book_name'})\n",
    "        chunks_per_book[\"chunks_per_book\"] = 1/chunks_per_book[\"chunks_per_book\"]\n",
    "        df = df.merge(right=chunks_per_book, how=\"left\", on=\"book_name\")\n",
    "        sample_weights = df[\"chunks_per_book\"].tolist()\n",
    "        return sample_weights\n",
    "    \n",
    "    def _evaluate_predictions(self, df):\n",
    "        score = f1_score(df[\"y\"], df[\"yhat\"], average='macro')\n",
    "        return score\n",
    "        \n",
    "    def run(self):\n",
    "        train_f1s = []\n",
    "        validation_f1s = []\n",
    "\n",
    "        df = self.df\n",
    "        df = self._combine_df_labels(df)\n",
    "        book_names_split = self._split_booknames(df, 10)\n",
    "        all_validation_books = []\n",
    "\n",
    "        for index, split in enumerate(book_names_split):\n",
    "            train_df = df[~df[\"book_name\"].isin(split)]\n",
    "            validation_df = df[df[\"book_name\"].isin(split)]\n",
    "            \n",
    "            train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            train_y = train_df[\"y\"].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            validation_y = validation_df[\"y\"].values.ravel()\n",
    "            train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            if self.model == \"xgboost\":\n",
    "                train_book_names = train_df[\"book_name\"].values.reshape(-1, 1)\n",
    "                best_parameters = self._get_model(self.model_param, train_X, train_y, train_book_names, classification=True)\n",
    "                class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1, 2, 3], y=train_y.astype(int).tolist())))\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y.astype(int), weight=[class_weights[int(i)] for i in train_y])\n",
    "                num_boost_round = best_parameters[\"num_boost_round\"]\n",
    "                best_parameters.pop(\"nested_cv_score\")\n",
    "                best_parameters.pop(\"num_boost_round\")\n",
    "                model = xgboost.train(best_parameters,\n",
    "                                      dtrain,\n",
    "                                      num_boost_round=num_boost_round,\n",
    "                                      verbose_eval=False)\n",
    "            else:\n",
    "                model = self._get_model(self.model_param)\n",
    "                model.fit(train_X, train_y)\n",
    "            \n",
    "            train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "            validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            \n",
    "            if self.model == \"xgboost\":\n",
    "                train_books[\"yhat\"] = model.predict(xgboost.DMatrix(train_X))\n",
    "                validation_books[\"yhat\"] = model.predict(xgboost.DMatrix(validation_X))\n",
    "            else:\n",
    "                train_books[\"yhat\"] = model.predict(train_X)\n",
    "                validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "            \n",
    "            train_f1 = self._evaluate_predictions(train_books)\n",
    "            validation_f1 = self._evaluate_predictions(validation_books)\n",
    "            all_validation_books.append(validation_books)\n",
    "            \n",
    "            train_f1s.append(train_f1)\n",
    "            validation_f1s.append(validation_f1)\n",
    "            if self.verbose:\n",
    "                print(f\"Fold: {index+1}, TrainF1: {np.round(train_f1, 3)}, ValF1: {np.round(validation_f1, 3)}\")\n",
    "        \n",
    "        # Save y and y_pred for examples\n",
    "        all_validation_books = pd.concat(all_validation_books)\n",
    "        all_validation_books.to_csv(results_dir + \"/valiationbooks-class-\" + self.language + \"-\" + self.datetime + \".csv\", index=False)\n",
    "        \n",
    "        print(confusion_matrix(all_validation_books[\"y\"], all_validation_books[\"yhat\"]))\n",
    "        print(pd.crosstab(all_validation_books[\"y\"], all_validation_books[\"yhat\"], rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "        mean_train_f1 = statistics.mean(train_f1s)\n",
    "        mean_validation_f1 = statistics.mean(validation_f1s)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\"\"TrainF1: {np.round(mean_train_f1, 3)}, ValidationF1: {np.round(mean_validation_f1, 3)}\"\"\")\n",
    "            print(\"\\n---------------------------------------------------\\n\")\n",
    "        return mean_train_f1, mean_validation_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56c1f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some columns by default before running cv\n",
    "def drop_default_columns(df, drop_default_columns_including):\n",
    "    def _drop_column(column):\n",
    "        for string in drop_default_columns_including:\n",
    "            if string in column:\n",
    "                return True\n",
    "    df = df[[column for column in df.columns if not _drop_column(column)]].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "# Feature split\n",
    "complexity_features = []\n",
    "\n",
    "\n",
    "# Superfluous featues\n",
    "drop_default_columns_including = [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\"]\n",
    "\n",
    "# All parameters\n",
    "models = [\"svr\", \"lasso\", \"xgboost\", \"svc\"]\n",
    "model_params = {\"svr\": [1], \"lasso\": [1, 4], \"xgboost\": [None], \"svc\": [0.1, 1, 10, 100, 1000, 10000]} #\n",
    "dimensionality_reduction = [\"ss_pca_0_95\", 'k_best_f_reg_0_10', 'k_best_mutual_info_0_10', None]\n",
    "features = [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "\n",
    "# Which parameters to use\n",
    "full_cv_params = {\"model\": models, \"dimensionality_reduction\": dimensionality_reduction, \"features\": features}\n",
    "testing_params = {\"model\": models[3], \"dimensionality_reduction\": dimensionality_reduction[3], \n",
    "                  \"features\": features[2]}\n",
    "twoclass_params = {\"model\": models[3], \"dimensionality_reduction\": dimensionality_reduction, \n",
    "                  \"features\": features}\n",
    "multiclass_params = {\"model\": models[2], \"dimensionality_reduction\": dimensionality_reduction[-1],\n",
    "                     \"features\": [\"book\", \"book_and_averaged_chunk\", \"chunk\", \"chunk_and_copied_book\"]}\n",
    "# Old results from chr2021 paper\n",
    "eng_params = {\"model\": models[0], \"dimensionality_reduction\": dimensionality_reduction[0], \n",
    "                  \"features\": features[2], }, # svr, pca, book_and_average_chunk\n",
    "ger_params = {\"model\": models[0], \"dimensionality_reduction\": dimensionality_reduction[0], \n",
    "                  \"features\": features[1]}, # svr, pca, chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a11a8",
   "metadata": {},
   "source": [
    "book_df = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\")\n",
    "book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "book_df = drop_default_columns(book_df, drop_default_columns_including)\n",
    "book_and_averaged_chunk_df = drop_default_columns(book_and_averaged_chunk_df, drop_default_columns_including)\n",
    "chunk_df = drop_default_columns(chunk_df, drop_default_columns_including)\n",
    "chunk_and_copied_book_df = drop_default_columns(chunk_and_copied_book_df, drop_default_columns_including)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09a045",
   "metadata": {},
   "source": [
    "print(len(book_df.columns), len(book_and_averaged_chunk_df.columns),len(chunk_df.columns),len(chunk_and_copied_book_df.columns),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c542ff4",
   "metadata": {},
   "source": [
    "len(list(book_and_averaged_chunk_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b3cb2",
   "metadata": {},
   "source": [
    "for i in list(book_and_averaged_chunk_df.columns):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b049efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run two class classification\n",
    "'''\n",
    "results = []\n",
    "param_dict = \"multiclass\" #\"full_cv\", \"language_specific\"\n",
    "for lang in [\"eng\"]: #, \"ger\"]:\n",
    "    if param_dict == \"testing\":\n",
    "        param_dir = testing_params\n",
    "    elif param_dict == \"multiclass\":\n",
    "        param_dir = multiclass_params\n",
    "    elif param_dict == \"full_cv\":\n",
    "        param_dir = full_cv_params\n",
    "    elif param_dict == \"language_specific\":\n",
    "        if lang == \"eng\":\n",
    "            param_dir = eng_params\n",
    "        else: \n",
    "            param_dir = ger_params\n",
    "    \n",
    "    #Eng: 606 books, 14146 chunks, 13170 chunks of books published after 1759\n",
    "    book_df = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\")\n",
    "    book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "    chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "    chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "    \n",
    "    book_df = drop_default_columns(book_df, drop_default_columns_including)\n",
    "    book_and_averaged_chunk_df = drop_default_columns(book_and_averaged_chunk_df, drop_default_columns_including)\n",
    "    chunk_df = drop_default_columns(chunk_df, drop_default_columns_including)\n",
    "    chunk_and_copied_book_df = drop_default_columns(chunk_and_copied_book_df, drop_default_columns_including)\n",
    "    \n",
    "    for model in [] + [param_dir['model']]:\n",
    "        model_param = model_params[model]\n",
    "        for model_param in model_param:\n",
    "            for dimensionality_reduction in [param_dir[\"dimensionality_reduction\"]]:\n",
    "                for features in param_dir[\"features\"]:\n",
    "                    for drop_columns_including in [[\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\"],\n",
    "                                                   [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\"],\n",
    "                                                   [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"pos\"],\n",
    "                                                   [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\", \"pos\"]]:\n",
    "                        experiment = MulticlassClassification(\n",
    "                            language=lang,\n",
    "                            features=features,\n",
    "                            drop_columns_including=drop_columns_including,\n",
    "                            dimensionality_reduction=dimensionality_reduction,\n",
    "                            model_param=model_param,\n",
    "                            model=model,\n",
    "                            verbose=True\n",
    "                        )\n",
    "                        mean_train_f1, mean_validation_f1 = experiment.run()\n",
    "                        print(lang, model, features, drop_columns_including, dimensionality_reduction, 'param=', model_param, mean_train_f1, mean_validation_f1)\n",
    "                        results.append((lang, model, features, drop_columns_including, dimensionality_reduction, model_param, mean_train_f1, mean_validation_f1))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"lang\", \"model\", \"features\", \"drop_columns_including\", \n",
    "\"dimensionality_reduction\", \"model_param\", \"mean_train_f1\", \"mean_validation_f1\"])\n",
    "results_df.to_csv(results_dir + lang + '_' + param_dict + \"_\" +\"xgboost_nested_cv\" + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e4e21fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# Run Multiclass Classification\n",
    "# '''\n",
    "# results = []\n",
    "# param_dict = \"multiclass\" #\"full_cv\", \"language_specific\"\n",
    "# for lang in [\"ger\"]: #, \"ger\"]:    \n",
    "#     if param_dict == \"testing\":\n",
    "#         param_dir = testing_params\n",
    "#     elif param_dict == \"multiclass\":\n",
    "#         param_dir = multiclass_params\n",
    "#     elif param_dict == \"full_cv\":\n",
    "#         param_dir = full_cv_params\n",
    "#     elif param_dict == \"language_specific\":\n",
    "#         if lang == \"eng\":\n",
    "#             param_dir = eng_params\n",
    "#         else: \n",
    "#             param_dir = ger_params\n",
    "    \n",
    "#     #Eng: 606 books, 14146 chunks, 13170 chunks of books published after 1759\n",
    "#     book_df = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\")\n",
    "#     book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "#     #chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "#     #chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "    \n",
    "#     book_df = drop_default_columns(book_df, drop_default_columns_including)\n",
    "#     book_and_averaged_chunk_df = drop_default_columns(book_and_averaged_chunk_df, drop_default_columns_including)\n",
    "#     #chunk_df = drop_default_columns(chunk_df, drop_default_columns_including)\n",
    "#     #chunk_and_copied_book_df = drop_default_columns(chunk_and_copied_book_df, drop_default_columns_including)\n",
    "    \n",
    "#     for model in [] + [param_dir['model']]:\n",
    "#         model_param = model_params[model]\n",
    "#         for model_param in model_param:\n",
    "#             for dimensionality_reduction in param_dir[\"dimensionality_reduction\"]:\n",
    "#                 for features in param_dir[\"features\"]:\n",
    "#                     for drop_columns_including in [[]]:\n",
    "#                         #try:\n",
    "#                         print(lang, model, features, drop_columns_including, dimensionality_reduction, 'param=', model_param)\n",
    "#                         experiment = MulticlassClassification(\n",
    "#                             language=lang,\n",
    "#                             features=features,\n",
    "#                             drop_columns_including=drop_columns_including,\n",
    "#                             dimensionality_reduction=dimensionality_reduction,\n",
    "#                             model_param=model_param,\n",
    "#                             model=model,\n",
    "#                             verbose=True\n",
    "#                         )\n",
    "#                         mean_train_f1, mean_validation_f1 = experiment.run()\n",
    "#                         results.append((lang, model, features, drop_columns_including, dimensionality_reduction, model_param, mean_train_f1, mean_validation_f1))\n",
    "\n",
    "# results_df = pd.DataFrame(results, columns=[\"lang\", \"model\", \"features\", \"drop_columns_including\", \n",
    "# \"dimensionality_reduction\", \"model_param\", \"mean_train_f1\", \"mean_validation_f1\"])\n",
    "# results_df.to_csv(results_dir + lang + '_' + param_dict + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1720f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 378 texts without reviews, 197 different books with reviews, 6 of which have opposing reviews and are left out.\n",
    "# 378 + 197 - 6 = 569 texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "162f01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Run Regression\n",
    "# '''\n",
    "# results = []\n",
    "# param_dict = \"testing\" #\"full_cv\", \"language_specific\"\n",
    "# for lang in [\"eng\"]: #, \"ger\"]:    \n",
    "#     if param_dict == \"testing\":\n",
    "#         param_dir = testing_params\n",
    "#     elif param_dict == \"multiclass\":\n",
    "#         param_dir = multiclass_params\n",
    "#     elif param_dict == \"full_cv\":\n",
    "#         param_dir = full_cv_params\n",
    "#     elif param_dict == \"language_specific\":\n",
    "#         if lang == \"eng\":\n",
    "#             param_dir = eng_params\n",
    "#         else: \n",
    "#             param_dir = ger_params\n",
    "    \n",
    "#     #Eng: 606 books, 14146 chunks, 13170 chunks of books published after 1759\n",
    "#     #book_df = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\")\n",
    "#     book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "#     #chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "#     #chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "    \n",
    "#     #book_df = drop_default_columns(book_df, drop_default_columns_including)\n",
    "#     book_and_averaged_chunk_df = drop_default_columns(book_and_averaged_chunk_df, drop_default_columns_including)\n",
    "#     #chunk_df = drop_default_columns(chunk_df, drop_default_columns_including)\n",
    "#     #chunk_and_copied_book_df = drop_default_columns(chunk_and_copied_book_df, drop_default_columns_including)\n",
    "    \n",
    "#     for model in [] + [param_dir['model']]:\n",
    "#         model_param = model_params[model]\n",
    "#         for model_param in model_param:\n",
    "#             for dimensionality_reduction in [param_dir[\"dimensionality_reduction\"]]:\n",
    "#                 for features in [param_dir[\"features\"]]:\n",
    "#                     for drop_columns_including in [[]]:\n",
    "#                                                         #try:\n",
    "#                         experiment = Regression(\n",
    "#                             language=lang,\n",
    "#                             features=features,\n",
    "#                             drop_columns_including=drop_columns_including,\n",
    "#                             dimensionality_reduction=dimensionality_reduction,\n",
    "#                             model_param=model_param,\n",
    "#                             model=model,\n",
    "#                             verbose=True\n",
    "#                         )\n",
    "#                         print(lang, model, features, drop_columns_including, dimensionality_reduction, 'param=', model_param)\n",
    "#                         mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value, datetime = experiment.run()\n",
    "#                         results.append((lang, model, features, drop_columns_including, dimensionality_reduction, model_param, mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value))\n",
    "#                         #except Exception as e:\n",
    "# #                             print(f\"Error in {lang}, {model}, {features}, {drop_columns_including}, {dimensionality_reduction}\")\n",
    "# #                             print(e)\n",
    "# results_df = pd.DataFrame(results, columns=[\"lang\", \"model\", \"features\", \"drop_columns_including\", \n",
    "# \"dimensionality_reduction\", \"model_param\", \"mean_train_mse\", \"mean_train_rmse\", \n",
    "# \"mean_train_mae\", \"mean_train_r2\", \"mean_train_corr\", \"mean_validation_mse\", \"mean_validation_rmse\",\n",
    "# \"mean_validation_mae\", \"mean_validation_r2\", \"mean_validation_corr\", \"mean_p_value\"])\n",
    "# results_df.to_csv(results_dir + lang + '_' + regression_' + param_dict + datetime + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b62c91d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
