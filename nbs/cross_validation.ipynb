{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea9869b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsentiment file names for eng/ger\\nMerge labels and features depending on how labels are aggregated if there are multiple scores for a work.\\ndrop_column reset index???\\nchunk based features?\\ncomplexity features\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "sentiment file names for eng/ger\n",
    "Merge labels and features depending on how labels are aggregated if there are multiple scores for a work.\n",
    "drop_column reset index???\n",
    "chunk based features?\n",
    "complexity features\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c12bfc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../src/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "extracted_features_dir = \"../data/extracted_features/\"\n",
    "results_dir = \"../data/results/\"\n",
    "sentiment_dir = \"../data/sentiment/\"\n",
    "canonization_labels_dir = \"/home/annina/scripts/great_unread_nlp/data/labels/\"\n",
    "lang = \"eng\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2696499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from copy import deepcopy\n",
    "from scipy.stats import pearsonr\n",
    "from utils import read_sentiment_scores \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\") # %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import random\n",
    "random.seed(9)\n",
    "\n",
    "labels = read_sentiment_scores(sentiment_dir, canonization_labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "211457a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_name</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ainsworth_William-Harrison_Rookwood_1834</td>\n",
       "      <td>0.051163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anonymous_Anonymous_The-Adventures-of-Anthony-...</td>\n",
       "      <td>0.042153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anonymous_Anonymous_The-Adventures-of-Anthony-...</td>\n",
       "      <td>0.041043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Austen_Jane_Pride-and-Prejudice_1813</td>\n",
       "      <td>0.002835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Austen_Jane_Sense-and-Sensibility_1811</td>\n",
       "      <td>0.028875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Wells_H-G_Tono-Bungay_1909</td>\n",
       "      <td>0.017329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Wilde_Oscar_The-Picture-of-Dorian-Gray_1890</td>\n",
       "      <td>0.027857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Wollstonecraft_Mary_Mary_1788</td>\n",
       "      <td>0.054404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Wollstonecraft_Mary_Mary_1788</td>\n",
       "      <td>0.056252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>Yonge_Charlotte_The-Daisy-Chain_1856</td>\n",
       "      <td>0.020710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             book_name         y\n",
       "0             Ainsworth_William-Harrison_Rookwood_1834  0.051163\n",
       "1    Anonymous_Anonymous_The-Adventures-of-Anthony-...  0.042153\n",
       "2    Anonymous_Anonymous_The-Adventures-of-Anthony-...  0.041043\n",
       "3                 Austen_Jane_Pride-and-Prejudice_1813  0.002835\n",
       "4               Austen_Jane_Sense-and-Sensibility_1811  0.028875\n",
       "..                                                 ...       ...\n",
       "249                         Wells_H-G_Tono-Bungay_1909  0.017329\n",
       "250        Wilde_Oscar_The-Picture-of-Dorian-Gray_1890  0.027857\n",
       "251                      Wollstonecraft_Mary_Mary_1788  0.054404\n",
       "252                      Wollstonecraft_Mary_Mary_1788  0.056252\n",
       "253               Yonge_Charlotte_The-Daisy-Chain_1856  0.020710\n",
       "\n",
       "[254 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8dc1cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels[\"y\"].plot.hist(grid=True, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b1785d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(object):\n",
    "    def __init__(self, language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose):\n",
    "        assert features in [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "        assert isinstance(drop_columns_including, list)\n",
    "        for i in drop_columns_including:\n",
    "            assert isinstance(i, str)\n",
    "        assert model in [\"xgboost\", \"svr\", \"lasso\"]\n",
    "        assert (dimensionality_reduction in [\"k_best_f_reg_0_10\", \"k_best_mutual_info_0_10\", \"ss_pca_0_95\"]) or (dimensionality_reduction is None)\n",
    "        self.language = language\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.drop_columns_including = drop_columns_including\n",
    "        self.dimensionality_reduction = dimensionality_reduction\n",
    "        self.model_param = model_param\n",
    "        self.model = model\n",
    "        self.verbose = verbose\n",
    "        self.datetime = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "        if self.features == \"book\":\n",
    "            self.df = deepcopy(book_df)\n",
    "        elif self.features == \"chunk\":\n",
    "            self.df = deepcopy(chunk_df)\n",
    "        elif self.features == \"chunk_and_copied_book\":\n",
    "            self.df = deepcopy(chunk_and_copied_book_df)\n",
    "        elif self.features == \"book_and_averaged_chunk\":\n",
    "            self.df = deepcopy(book_and_averaged_chunk_df)\n",
    "\n",
    "        columns_before_drop = set(self.df.columns)\n",
    "        if self.drop_columns_including:\n",
    "            self.df = self.df[[column for column in self.df.columns if not self._drop_column(column)]].reset_index(drop=True)\n",
    "        columns_after_drop = set(self.df.columns)\n",
    "        if self.verbose:\n",
    "            print(f\"Dropped {len(columns_before_drop - columns_after_drop)} columns.\")\n",
    "\n",
    "    def _drop_column(self, column):\n",
    "        for string in self.drop_columns_including:\n",
    "            if string in column:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def _custom_pca(self, train_X):\n",
    "        for i in range(5, train_X.shape[1], int((train_X.shape[1] - 5) / 10)):\n",
    "            pca = PCA(n_components=i)\n",
    "            new_train_X = pca.fit_transform(train_X)\n",
    "            if pca.explained_variance_ratio_.sum() >= 0.95:\n",
    "                break\n",
    "        return new_train_X, pca\n",
    "\n",
    "    def _select_features(self, train_X, train_y, validation_X):\n",
    "        if self.dimensionality_reduction == \"ss_pca_0_95\":\n",
    "            ss = StandardScaler()\n",
    "            train_X = ss.fit_transform(train_X)\n",
    "            validation_X = ss.transform(validation_X)\n",
    "            train_X, pca = self._custom_pca(train_X)\n",
    "            validation_X = pca.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_f_reg_0_10\":\n",
    "            k_best = SelectKBest(f_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_mutual_info_0_10\":\n",
    "            k_best = SelectKBest(mutual_info_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction is None:\n",
    "            pass\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _impute(self, train_X, validation_X):\n",
    "        imputer = KNNImputer()\n",
    "        train_X = imputer.fit_transform(train_X)\n",
    "        validation_X = imputer.transform(validation_X)\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _get_model(self, model_param):\n",
    "        # if any of these performs better than others, we can try to tune the hyperparameters\n",
    "        # but I think for now it\"s more important to see which approach performs better\n",
    "        # chunk based or doc based\n",
    "        # use dimensionality reduction or not...\n",
    "        if self.model == \"xgboost\": #1,0.25,2\n",
    "            return XGBRegressor(n_estimators=1000, max_depth=model_param, learning_rate=0.01, colsample_bytree=0.33, min_child_weight=6) #max_depth=4\n",
    "        elif self.model == \"svr\":\n",
    "            return SVR(C=model_param)\n",
    "        elif self.model == \"lasso\":\n",
    "            return Lasso(alpha=model_param)\n",
    "        \n",
    "    def _split_booknames(self, df, nr_splits):\n",
    "        \"\"\"\n",
    "        Distribute book names over splits.\n",
    "        All works of an author are in the same split.\n",
    "        \"\"\"\n",
    "        book_names = df[\"book_name\"].unique()\n",
    "        authors = []\n",
    "        booknames_authors_mapping = {}\n",
    "\n",
    "        #Get authors\n",
    "        for book_name in book_names:\n",
    "            author = \"_\".join(book_name.split(\"_\")[:2])\n",
    "            authors.append(author)\n",
    "            if author in booknames_authors_mapping:\n",
    "                booknames_authors_mapping[author].append(book_name)\n",
    "            else:\n",
    "                booknames_authors_mapping[author] = []\n",
    "                booknames_authors_mapping[author].append(book_name)\n",
    "        #Distribute authors over splits so that each split has approximately the same number of books\n",
    "        works_per_author = Counter(authors)\n",
    "        goal_sum = round(len(book_names)/nr_splits)\n",
    "        tolerance = 0.03\n",
    "        lower_threshold = goal_sum - round(tolerance*goal_sum)\n",
    "        upper_threshold = goal_sum + round(tolerance*goal_sum)\n",
    "        author_splits = []\n",
    "        popped_dict = {}\n",
    "\n",
    "        for i in range (0, nr_splits-1):\n",
    "            works_in_split = 0\n",
    "            split = []\n",
    "            curr_author_workcount = 0\n",
    "\n",
    "            # take values from popped dict first\n",
    "            if bool(popped_dict):  \n",
    "                popped = []\n",
    "                for curr_author, curr_author_workcount in popped_dict.items():\n",
    "                    # leave item in popped dict if value is too big\n",
    "                    if works_in_split + curr_author_workcount > upper_threshold:\n",
    "                        continue\n",
    "                    else:\n",
    "                        popped.append(curr_author)\n",
    "                        split.append(curr_author)\n",
    "                        works_in_split += curr_author_workcount\n",
    "                        if works_in_split >= lower_threshold:\n",
    "                            break\n",
    "                for current_author in popped:\n",
    "                    del popped_dict[current_author]\n",
    "            while works_in_split < upper_threshold:\n",
    "                if bool(works_per_author):\n",
    "                    curr_author = random.choice(list(works_per_author.keys()))\n",
    "                    curr_author_workcount = works_per_author.pop(curr_author)\n",
    "                    # Put values into separate dict if too big\n",
    "                    if works_in_split + curr_author_workcount > upper_threshold:\n",
    "                        popped_dict[curr_author] = curr_author_workcount\n",
    "                    else:\n",
    "                        split.append(curr_author)\n",
    "                        works_in_split += curr_author_workcount\n",
    "                        if works_in_split >= lower_threshold:\n",
    "                            break\n",
    "                else:\n",
    "                    #ignore upper threshold\n",
    "                    popped = []\n",
    "                    for curr_author, curr_author_workcount in popped_dict.items():\n",
    "                        popped.append(curr_author)\n",
    "                        split.append(curr_author)\n",
    "                        works_in_split += curr_author_workcount\n",
    "                        if works_in_split >= lower_threshold:\n",
    "                            break\n",
    "                    for current_author in popped:\n",
    "                        del popped_dict[current_author]\n",
    "\n",
    "            author_splits.append(split)\n",
    "        #Create last split directly from remaining dict\n",
    "        works_in_last_split = sum(works_per_author.values()) + sum(popped_dict.values())\n",
    "        split = list(works_per_author.keys()) + list(popped_dict.keys())\n",
    "        author_splits.append(split)\n",
    "\n",
    "        #Map author splits to book names\n",
    "        book_splits = []\n",
    "        for author_split in author_splits:\n",
    "            book_split = []\n",
    "            for author in author_split:\n",
    "                book_split.extend(booknames_authors_mapping[author])\n",
    "            book_splits.append(book_split)\n",
    "        return book_splits\n",
    "    \n",
    "    def _combine_df_labels(self, df):\n",
    "        #Average of sentiscores per book\n",
    "        agg_labels = labels.groupby(by=\"book_name\", as_index=False).mean()\n",
    "        df = df.merge(right=agg_labels, on=\"book_name\", how=\"inner\", validate=\"many_to_one\")       \n",
    "        return df\n",
    "\n",
    "    def _get_pvalue(self, validation_corr_pvalues):\n",
    "        # Harmonic mean p-value\n",
    "        denominator = sum([1/x for x in validation_corr_pvalues])\n",
    "        mean_p_value = len(validation_corr_pvalues)/denominator\n",
    "        return mean_p_value\n",
    "    \n",
    "    def run(self):\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        train_mses = []\n",
    "        train_maes = []\n",
    "        train_r2s = []\n",
    "        train_corrs = []\n",
    "        \n",
    "        validation_mses = []\n",
    "        validation_maes = []\n",
    "        validation_r2s = []\n",
    "        validation_corrs = []\n",
    "        validation_corr_pvalues = []\n",
    "\n",
    "        df = self.df\n",
    "        df = self._combine_df_labels(df)\n",
    "        book_names_split = self._split_booknames(df, 10)\n",
    "        all_validation_books = []\n",
    "\n",
    "        for index, split in enumerate(book_names_split):\n",
    "            #print('traindf', traindf)\n",
    "            validation_df = df[df[\"book_name\"].isin(split)]\n",
    "\n",
    "            train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            train_y = train_df[\"y\"].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            validation_y = validation_df[\"y\"].values.ravel()\n",
    "            train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            print('trainX\\n', train_X, 'validationX\\n', validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            model = self._get_model(self.model_param)\n",
    "            model.fit(train_X, train_y)\n",
    "            \n",
    "            train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "            train_books[\"yhat\"] = model.predict(train_X)\n",
    "            validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "            \n",
    "            train_books = train_books.groupby(\"book_name\").mean()\n",
    "            validation_books = validation_books.groupby(\"book_name\").mean()\n",
    "            all_validation_books.append(validation_books.reset_index())\n",
    "            print(all_validation_books)\n",
    "            \n",
    "            train_y = train_books[\"y\"].tolist()\n",
    "            train_yhat = train_books[\"yhat\"].tolist()\n",
    "            validation_y = validation_books[\"y\"].tolist()\n",
    "            validation_yhat = validation_books[\"yhat\"].tolist()\n",
    "            print(validation_yhat)\n",
    "            \n",
    "            all_labels.extend(validation_y)\n",
    "            all_predictions.extend(validation_yhat)\n",
    "            \n",
    "            train_mse = mean_squared_error(train_y, train_yhat)\n",
    "            train_mae = mean_absolute_error(train_y, train_yhat)\n",
    "            train_r2 = r2_score(train_y, train_yhat)\n",
    "            train_corr = pearsonr(train_y, train_yhat)[0]\n",
    "            \n",
    "            validation_mse = mean_squared_error(validation_y, validation_yhat)\n",
    "            validation_mae = mean_absolute_error(validation_y, validation_yhat)\n",
    "            validation_r2 = r2_score(validation_y, validation_yhat)\n",
    "            validation_corr = pearsonr(validation_y, validation_yhat)[0]\n",
    "            p_value = pearsonr(validation_y, validation_yhat)[1]\n",
    "            print(p_value)\n",
    "            \n",
    "            train_mses.append(train_mse)\n",
    "            train_maes.append(train_mae)\n",
    "            train_r2s.append(train_r2)\n",
    "            train_corrs.append(train_corr)\n",
    "            \n",
    "            validation_mses.append(validation_mse)\n",
    "            validation_maes.append(validation_mae)\n",
    "            validation_r2s.append(validation_r2)\n",
    "            validation_corrs.append(validation_corr)\n",
    "            validation_corr_pvalues.append(p_value)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Fold: {index+1}, TrainMSE: {np.round(train_mse, 3)}, TrainMAE: {np.round(train_mae, 3)}, ValMSE: {np.round(validation_mse, 3)}, ValMAE: {np.round(validation_mae, 3)}, ValR2: {np.round(validation_r2, 3)}\")\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        \n",
    "        # Save y and y_pred for examples\n",
    "        mean_train_mse = np.mean(train_mses)\n",
    "        mean_train_rmse = np.mean([sqrt(x) for x in train_mses])\n",
    "        mean_train_mae = np.mean(train_maes)\n",
    "        mean_train_r2 = np.mean(train_r2s)\n",
    "        mean_train_corr = np.mean(train_corrs)\n",
    "        \n",
    "        mean_validation_mse = np.mean(validation_mses)\n",
    "        mean_validation_rmse = np.mean([sqrt(x) for x in validation_mses])\n",
    "        mean_validation_mae = np.mean(validation_maes)\n",
    "        mean_validation_r2 = np.mean(validation_r2s)\n",
    "        mean_validation_corr = np.mean(validation_corrs)\n",
    "        print(\"mean val corr\", mean_validation_corr)\n",
    "        mean_p_value = self._get_pvalue(validation_corr_pvalues)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\"\"TrainMSE: {np.round(mean_train_mse, 3)}, TrainRMSE: {np.round(mean_train_rmse, 3)}, TrainMAE: {np.round(mean_train_mae, 3)}, TrainR2: {np.round(mean_train_r2, 3)}, TrainCorr: {np.round(mean_train_corr, 3)}, ValMSE: {np.round(mean_validation_mse, 3)}, ValRMSE: {np.round(mean_validation_rmse, 3)}, ValMAE: {np.round(mean_validation_mae, 3)}, ValR2: {np.round(mean_validation_r2, 3)}, ValCorr: {np.round(mean_validation_corr, 3)}, ValCorrPValue: {np.round(mean_p_value, 3)}\"\"\")\n",
    "            print(\"------\")\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.xticks(fontsize=15)\n",
    "            plt.yticks(fontsize=15)\n",
    "            plt.xlim([0,0.2])\n",
    "            plt.ylim([0,0.2])\n",
    "\n",
    "            plt.scatter(all_labels, all_predictions, s=6)\n",
    "            plt.xlabel(\"Canonization Scores\", fontsize=20)\n",
    "            plt.ylabel(\"Predicted Scores\", fontsize=20)\n",
    "            plt.savefig(results_dir + lang + \"-\" + self.model + \"-\" + str(self.dimensionality_reduction) \n",
    "            + \"-\" + self.features + \"-\" + \"-\" + \"param\" + str(self.model_param) + \"-\" + self.datetime + \".png\", \n",
    "            dpi=400, bbox_inches=\"tight\")    \n",
    "    \n",
    "            plt.show();\n",
    "        return mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value, self.datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56c1f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some columns by default before running cv\n",
    "def drop_default_columns(df, drop_default_columns_including):\n",
    "    def _drop_column(column):\n",
    "        for string in drop_default_columns_including:\n",
    "            if string in column:\n",
    "                return True\n",
    "    df = df[[column for column in df.columns if not _drop_column(column)]].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "# Superflous featues\n",
    "drop_default_columns_including = [\"average_sentence_embedding\", \"100_most_common_\"]\n",
    "\n",
    "# All parameters\n",
    "models = [\"svr\", \"lasso\", \"xgboost\"]\n",
    "model_params = {\"svr\": [1], \"lasso\": [1, 4], \"xgboost\": [1, 4]}\n",
    "dimensionality_reduction = [\"ss_pca_0_95\", 'k_best_f_reg_0_10', 'k_best_mutual_info_0_10', None]\n",
    "features = [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "\n",
    "# Which parameters to use\n",
    "full_cv_params = {\"model\": models, \"dimensionality_reduction\": dimensionality_reduction, \"features\": features}\n",
    "testing_params = {\"model\": models[0], \"dimensionality_reduction\": dimensionality_reduction[0], \n",
    "                  \"features\": features[1]} # svr, None, chunk\n",
    "# Old results from chr2021 paper\n",
    "eng_params = {\"model\": models[0], \"dimensionality_reduction\": dimensionality_reduction[0], \n",
    "                  \"features\": features[2], }, # svr, pca, book_and_average_chunk\n",
    "ger_params = {\"model\": models[0], \"dimensionality_reduction\": dimensionality_reduction[0], \n",
    "                  \"features\": features[1]}, # svr, pca, chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e4e21fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svr 1 ss_pca_0_95 []\n",
      "Dropped 0 columns.\n",
      "eng svr chunk [] ss_pca_0_95 param= 1 \n",
      "------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'traindf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a3285b9523f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m                         )\n\u001b[1;32m     42\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_columns_including\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensionality_reduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'param='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n------------------------------------------------\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                         \u001b[0mmean_train_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_train_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_train_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_train_r2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_train_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_validation_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_validation_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_validation_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_validation_r2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_validation_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_p_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_columns_including\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensionality_reduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_train_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_train_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_train_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_train_r2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_train_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_validation_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_validation_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_validation_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_validation_r2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_validation_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_p_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-de75db3fc07e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook_names_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"book_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'traindf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraindf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0mvalidation_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"book_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validationdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'traindf' is not defined"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "param_dict = \"testing\" #\"full_cv\", \"language_specific\"\n",
    "for lang in [\"eng\"]: #, \"ger\"]:    \n",
    "    if param_dict==\"testing\":\n",
    "        param_dir = testing_params\n",
    "    elif param_dict==\"full_cv\":\n",
    "        param_dir = full_cv_params\n",
    "    elif param_dict==\"language_specific\":\n",
    "        if lang==\"eng\":\n",
    "            param_dir = eng_params\n",
    "        else: \n",
    "            param_dir = ger_params\n",
    "        \n",
    "    book_df = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\")\n",
    "    book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "    chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "    chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "    \n",
    "    book_df = drop_default_columns(book_df, drop_default_columns_including)\n",
    "    book_and_averaged_chunk_df = drop_default_columns(book_and_averaged_chunk_df, drop_default_columns_including)\n",
    "    chunk_df = drop_default_columns(chunk_df, drop_default_columns_including)\n",
    "    chunk_and_copied_book_df = drop_default_columns(chunk_and_copied_book_df, drop_default_columns_including)\n",
    "\n",
    "    \n",
    "    for model in [] + [param_dir['model']]:\n",
    "        model_param = model_params[model]\n",
    "        for model_param in model_param:\n",
    "            for dimensionality_reduction in [param_dir[\"dimensionality_reduction\"]]:\n",
    "                for features in [param_dir[\"features\"]]:\n",
    "                    for drop_columns_including in [[]]:\n",
    "                        print(model, model_param, dimensionality_reduction, drop_columns_including)\n",
    "                        #try:\n",
    "                        experiment = Experiment(\n",
    "                            language=lang,\n",
    "                            features=features,\n",
    "                            drop_columns_including=drop_columns_including,\n",
    "                            dimensionality_reduction=dimensionality_reduction,\n",
    "                            model_param=model_param,\n",
    "                            model=model,\n",
    "                            verbose=True\n",
    "                        )\n",
    "                        print(lang, model, features, drop_columns_including, dimensionality_reduction, 'param=', model_param, '\\n------------------------------------------------\\n')\n",
    "                        mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value, datetime = experiment.run()\n",
    "                        results.append((lang, model, features, drop_columns_including, dimensionality_reduction, model_param, mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value))\n",
    "\n",
    "                        #except Exception as e:\n",
    "#                             print(f\"Error in {lang}, {model}, {features}, {drop_columns_including}, {dimensionality_reduction}\")\n",
    "#                             print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2103539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.DataFrame(results, columns=[\"lang\", \"model\", \"features\", \"drop_columns_including\", \n",
    "#             \"dimensionality_reduction\", \"model_param\", \"mean_train_mse\", \"mean_train_rmse\", \n",
    "#             \"mean_train_mae\", \"mean_train_r2\", \"mean_train_corr\", \"mean_validation_mse\", \"mean_validation_rmse\",\n",
    "#             \"mean_validation_mae\", \"mean_validation_r2\", \"mean_validation_corr\", \"mean_p_value\"])\n",
    "# results_df.to_csv(results_dir + param_dict + datetime + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7e4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
