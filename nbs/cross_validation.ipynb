{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15df4db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsentiment file names for eng/ger\\nMerge labels and features depending on how labels are aggregated if there are multiple scores for a work.\\ndrop_column reset index???\\nchunk based features?\\ncomplexity features\\ntake out doc2vec_chunk_embedding from default drop columns\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "sentiment file names for eng/ger\n",
    "Merge labels and features depending on how labels are aggregated if there are multiple scores for a work.\n",
    "drop_column reset index???\n",
    "chunk based features?\n",
    "complexity features\n",
    "take out doc2vec_chunk_embedding from default drop columns\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c12bfc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../src/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "extracted_features_dir = \"../data/extracted_features/\"\n",
    "results_dir = \"../data/results/\"\n",
    "sentiment_dir = \"../data/sentiment/\"\n",
    "canonization_labels_dir = \"/home/annina/scripts/great_unread_nlp/data/labels/\"\n",
    "lang = \"eng\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2696499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, confusion_matrix\n",
    "from xgboost import XGBRegressor\n",
    "from copy import deepcopy\n",
    "from scipy.stats import pearsonr\n",
    "from utils import read_sentiment_scores \n",
    "from math import sqrt\n",
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\") # %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import random\n",
    "random.seed(9)\n",
    "\n",
    "labels = read_sentiment_scores(sentiment_dir, canonization_labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4594ed42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_name</th>\n",
       "      <th>y</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anonymous_Anonymous_The-Adventures-of-Anthony-...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Austen_Jane_Pride-and-Prejudice_1813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Austen_Jane_Sense-and-Sensibility_1811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barrie_J-M_Auld-Licht-Idylls_1888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barrie_J-M_Sentimental-Tommy_1896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Wells_H-G_Ann-Veronica_1909</td>\n",
       "      <td>-0.019978</td>\n",
       "      <td>not_classified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Wells_H-G_The-First-Men-in-the-Moon_1901</td>\n",
       "      <td>-0.040821</td>\n",
       "      <td>not_classified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Wells_H-G_The-Island-of-Dr-Moreau_1896</td>\n",
       "      <td>-0.044745</td>\n",
       "      <td>not_classified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Wilde_Oscar_The-Picture-of-Dorian-Gray_1890</td>\n",
       "      <td>0.007171</td>\n",
       "      <td>not_classified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>Wollstonecraft_Mary_Mary_1788</td>\n",
       "      <td>-0.002009</td>\n",
       "      <td>not_classified</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             book_name         y  \\\n",
       "0    Anonymous_Anonymous_The-Adventures-of-Anthony-...  0.000000   \n",
       "1                 Austen_Jane_Pride-and-Prejudice_1813  0.000000   \n",
       "2               Austen_Jane_Sense-and-Sensibility_1811  0.000000   \n",
       "3                    Barrie_J-M_Auld-Licht-Idylls_1888  0.000000   \n",
       "4                    Barrie_J-M_Sentimental-Tommy_1896  0.000000   \n",
       "..                                                 ...       ...   \n",
       "249                        Wells_H-G_Ann-Veronica_1909 -0.019978   \n",
       "250           Wells_H-G_The-First-Men-in-the-Moon_1901 -0.040821   \n",
       "251             Wells_H-G_The-Island-of-Dr-Moreau_1896 -0.044745   \n",
       "252        Wilde_Oscar_The-Picture-of-Dorian-Gray_1890  0.007171   \n",
       "253                      Wollstonecraft_Mary_Mary_1788 -0.002009   \n",
       "\n",
       "                  c  \n",
       "0          positive  \n",
       "1          positive  \n",
       "2          positive  \n",
       "3          positive  \n",
       "4          positive  \n",
       "..              ...  \n",
       "249  not_classified  \n",
       "250  not_classified  \n",
       "251  not_classified  \n",
       "252  not_classified  \n",
       "253  not_classified  \n",
       "\n",
       "[254 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48d9f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #random labels\n",
    "# labels = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\", usecols=[0])\n",
    "# labels.loc[:,\"y\"] = pd.DataFrame(np.random.randint(0,100,size=(labels.shape[0],1)), columns=[\"y\"])\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "933bfe04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels[\"y\"].plot.hist(grid=True, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b1785d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression(object):\n",
    "    def __init__(self, language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose):\n",
    "        assert features in [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "        assert isinstance(drop_columns_including, list)\n",
    "        for i in drop_columns_including:\n",
    "            assert isinstance(i, str)\n",
    "        assert (dimensionality_reduction in [\"k_best_f_reg_0_10\", \"k_best_mutual_info_0_10\", \"ss_pca_0_95\"]) or (dimensionality_reduction is None)\n",
    "        self._check_class_specific_assertions()\n",
    "        \n",
    "        self.language = language\n",
    "        self.features = features\n",
    "        self.labels = labels.drop(labels=\"c\")\n",
    "        self.drop_columns_including = drop_columns_including\n",
    "        self.dimensionality_reduction = dimensionality_reduction\n",
    "        self.model_param = model_param\n",
    "        self.model = model\n",
    "        self.verbose = verbose\n",
    "        self.datetime = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "        if self.features == \"book\":\n",
    "            self.df = deepcopy(book_df)\n",
    "        elif self.features == \"chunk\":\n",
    "            self.df = deepcopy(chunk_df)\n",
    "        elif self.features == \"chunk_and_copied_book\":\n",
    "            self.df = deepcopy(chunk_and_copied_book_df)\n",
    "        elif self.features == \"book_and_averaged_chunk\":\n",
    "            self.df = deepcopy(book_and_averaged_chunk_df)\n",
    "\n",
    "        columns_before_drop = set(self.df.columns)\n",
    "        if self.drop_columns_including:\n",
    "            self.df = self.df[[column for column in self.df.columns if not self._drop_column(column)]].reset_index(drop=True)\n",
    "        columns_after_drop = set(self.df.columns)\n",
    "        if self.verbose:\n",
    "            print(f\"Dropped {len(columns_before_drop - columns_after_drop)} columns.\")\n",
    "            \n",
    "    def _check_class_specific_assertions(self):\n",
    "        assert model in [\"xgboost\", \"svr\", \"lasso\"]\n",
    "\n",
    "    def _drop_column(self, column):\n",
    "        for string in self.drop_columns_including:\n",
    "            if string in column:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def _custom_pca(self, train_X):\n",
    "        for i in range(5, train_X.shape[1], int((train_X.shape[1] - 5) / 10)):\n",
    "            pca = PCA(n_components=i)\n",
    "            new_train_X = pca.fit_transform(train_X)\n",
    "            if pca.explained_variance_ratio_.sum() >= 0.95:\n",
    "                break\n",
    "        return new_train_X, pca\n",
    "\n",
    "    def _select_features(self, train_X, train_y, validation_X):\n",
    "        if self.dimensionality_reduction == \"ss_pca_0_95\":\n",
    "            ss = StandardScaler()\n",
    "            train_X = ss.fit_transform(train_X)\n",
    "            validation_X = ss.transform(validation_X)\n",
    "            train_X, pca = self._custom_pca(train_X)\n",
    "            validation_X = pca.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_f_reg_0_10\":\n",
    "            k_best = SelectKBest(f_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_mutual_info_0_10\":\n",
    "            k_best = SelectKBest(mutual_info_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction is None:\n",
    "            pass\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _impute(self, train_X, validation_X):\n",
    "        imputer = KNNImputer()\n",
    "        train_X = imputer.fit_transform(train_X)\n",
    "        validation_X = imputer.transform(validation_X)\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _get_model(self, model_param):\n",
    "        # if any of these performs better than others, we can try to tune the hyperparameters\n",
    "        # but I think for now it\"s more important to see which approach performs better\n",
    "        # chunk based or doc based\n",
    "        # use dimensionality reduction or not...\n",
    "        if self.model == \"xgboost\": #1,0.25,2\n",
    "            return XGBRegressor(n_estimators=1000, max_depth=model_param, learning_rate=0.01, colsample_bytree=0.33, min_child_weight=6) #max_depth=4\n",
    "        elif self.model == \"svr\":\n",
    "            return SVR(C=model_param)\n",
    "        elif self.model == \"lasso\":\n",
    "            return Lasso(alpha=model_param)\n",
    "        elif self.model == \"svc\":\n",
    "            return SVC(C=model_param)\n",
    "        \n",
    "    def _split_booknames(self, df, nr_splits):\n",
    "        \"\"\"\n",
    "        Distribute book names over splits.\n",
    "        All works of an author are in the same split.\n",
    "        \"\"\"\n",
    "        book_names = df[\"book_name\"].unique()\n",
    "        authors = []\n",
    "        booknames_authors_mapping = {}\n",
    "\n",
    "        #Get authors\n",
    "        for book_name in book_names:\n",
    "            author = \"_\".join(book_name.split(\"_\")[:2])\n",
    "            authors.append(author)\n",
    "            if author in booknames_authors_mapping:\n",
    "                booknames_authors_mapping[author].append(book_name)\n",
    "            else:\n",
    "                booknames_authors_mapping[author] = []\n",
    "                booknames_authors_mapping[author].append(book_name)\n",
    "        #Distribute authors over splits so that each split has approximately the same number of books\n",
    "        works_per_author = Counter(authors)\n",
    "        goal_sum = round(len(book_names)/nr_splits)\n",
    "        tolerance = 0.03\n",
    "        lower_threshold = goal_sum - round(tolerance*goal_sum)\n",
    "        upper_threshold = goal_sum + round(tolerance*goal_sum)\n",
    "        author_splits = []\n",
    "        popped_dict = {}\n",
    "\n",
    "        for i in range (0, nr_splits-1):\n",
    "            works_in_split = 0\n",
    "            split = []\n",
    "            curr_author_workcount = 0\n",
    "\n",
    "            # take values from popped dict first\n",
    "            if bool(popped_dict):  \n",
    "                popped = []\n",
    "                for curr_author, curr_author_workcount in popped_dict.items():\n",
    "                    # leave item in popped dict if value is too big\n",
    "                    if works_in_split + curr_author_workcount > upper_threshold:\n",
    "                        continue\n",
    "                    else:\n",
    "                        popped.append(curr_author)\n",
    "                        split.append(curr_author)\n",
    "                        works_in_split += curr_author_workcount\n",
    "                        if works_in_split >= lower_threshold:\n",
    "                            break\n",
    "                for current_author in popped:\n",
    "                    del popped_dict[current_author]\n",
    "            while works_in_split < upper_threshold:\n",
    "                if bool(works_per_author):\n",
    "                    curr_author = random.choice(list(works_per_author.keys()))\n",
    "                    curr_author_workcount = works_per_author.pop(curr_author)\n",
    "                    # Put values into separate dict if too big\n",
    "                    if works_in_split + curr_author_workcount > upper_threshold:\n",
    "                        popped_dict[curr_author] = curr_author_workcount\n",
    "                    else:\n",
    "                        split.append(curr_author)\n",
    "                        works_in_split += curr_author_workcount\n",
    "                        if works_in_split >= lower_threshold:\n",
    "                            break\n",
    "                else:\n",
    "                    #ignore upper threshold\n",
    "                    popped = []\n",
    "                    for curr_author, curr_author_workcount in popped_dict.items():\n",
    "                        popped.append(curr_author)\n",
    "                        split.append(curr_author)\n",
    "                        works_in_split += curr_author_workcount\n",
    "                        if works_in_split >= lower_threshold:\n",
    "                            break\n",
    "                    for current_author in popped:\n",
    "                        del popped_dict[current_author]\n",
    "\n",
    "            author_splits.append(split)\n",
    "        #Create last split directly from remaining dict\n",
    "        works_in_last_split = sum(works_per_author.values()) + sum(popped_dict.values())\n",
    "        split = list(works_per_author.keys()) + list(popped_dict.keys())\n",
    "        author_splits.append(split)\n",
    "\n",
    "        #Map author splits to book names\n",
    "        book_splits = []\n",
    "        for author_split in author_splits:\n",
    "            book_split = []\n",
    "            for author in author_split:\n",
    "                book_split.extend(booknames_authors_mapping[author])\n",
    "            book_splits.append(book_split)\n",
    "        return book_splits\n",
    "    \n",
    "    def _combine_df_labels(self, df):\n",
    "        #Average of sentiscores per book\n",
    "        agg_labels = self.labels.groupby(by=\"book_name\", as_index=False).mean()\n",
    "        df = df.merge(right=agg_labels, on=\"book_name\", how=\"inner\", validate=\"many_to_one\")\n",
    "        return df\n",
    "    \n",
    "    def run(self):\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        train_mses = []\n",
    "        train_maes = []\n",
    "        train_r2s = []\n",
    "        train_corrs = []\n",
    "        \n",
    "        validation_mses = []\n",
    "        validation_maes = []\n",
    "        validation_r2s = []\n",
    "        validation_corrs = []\n",
    "        validation_corr_pvalues = []\n",
    "\n",
    "        df = self.df\n",
    "        df = self._combine_df_labels(df)\n",
    "        book_names_split = self._split_booknames(df, 5)\n",
    "        all_validation_books = []\n",
    "\n",
    "        for index, split in enumerate(book_names_split):\n",
    "            train_df = df[~df[\"book_name\"].isin(split)]\n",
    "            validation_df = df[df[\"book_name\"].isin(split)]\n",
    "            \n",
    "            train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            train_y = train_df[\"y\"].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            validation_y = validation_df[\"y\"].values.ravel()\n",
    "            train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            model = self._get_model(self.model_param)\n",
    "            model.fit(train_X, train_y)\n",
    "            \n",
    "            train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "            train_books[\"yhat\"] = model.predict(train_X)\n",
    "            validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "            \n",
    "            train_books = train_books.groupby(\"book_name\").mean()\n",
    "            validation_books = validation_books.groupby(\"book_name\").mean()\n",
    "            all_validation_books.append(validation_books.reset_index())\n",
    "            \n",
    "            train_y = train_books[\"y\"].tolist()\n",
    "            train_yhat = train_books[\"yhat\"].tolist()\n",
    "            validation_y = validation_books[\"y\"].tolist()\n",
    "            validation_yhat = validation_books[\"yhat\"].tolist()\n",
    "            \n",
    "            all_labels.extend(validation_y)\n",
    "            all_predictions.extend(validation_yhat)\n",
    "            \n",
    "            train_mse = mean_squared_error(train_y, train_yhat)\n",
    "            train_mae = mean_absolute_error(train_y, train_yhat)\n",
    "            train_r2 = r2_score(train_y, train_yhat)\n",
    "            train_corr = pearsonr(train_y, train_yhat)[0]\n",
    "            \n",
    "            validation_mse = mean_squared_error(validation_y, validation_yhat)\n",
    "            validation_mae = mean_absolute_error(validation_y, validation_yhat)\n",
    "            validation_r2 = r2_score(validation_y, validation_yhat)\n",
    "            validation_corr = pearsonr(validation_y, validation_yhat)[0]\n",
    "            p_value = pearsonr(validation_y, validation_yhat)[1]\n",
    "            \n",
    "            train_mses.append(train_mse)\n",
    "            train_maes.append(train_mae)\n",
    "            train_r2s.append(train_r2)\n",
    "            train_corrs.append(train_corr)\n",
    "            \n",
    "            validation_mses.append(validation_mse)\n",
    "            validation_maes.append(validation_mae)\n",
    "            validation_r2s.append(validation_r2)\n",
    "            validation_corrs.append(validation_corr)\n",
    "            validation_corr_pvalues.append(p_value)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Fold: {index+1}, TrainMSE: {np.round(train_mse, 3)}, TrainMAE: {np.round(train_mae, 3)}, ValMSE: {np.round(validation_mse, 3)}, ValMAE: {np.round(validation_mae, 3)}, ValR2: {np.round(validation_r2, 3)}, ValCorr: {np.round(validation_corr, 3)}\")\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        \n",
    "        # Save y and y_pred for examples\n",
    "        pd.concat(all_validation_books).to_csv(results_dir + \"/y-yhat-\" + self.language + \"-\" + self.datetime + \".csv\", index=False)\n",
    "        \n",
    "        mean_train_mse = np.mean(train_mses)\n",
    "        mean_train_rmse = np.mean([sqrt(x) for x in train_mses])\n",
    "        mean_train_mae = np.mean(train_maes)\n",
    "        mean_train_r2 = np.mean(train_r2s)\n",
    "        mean_train_corr = np.mean(train_corrs)\n",
    "        \n",
    "        mean_validation_mse = np.mean(validation_mses)\n",
    "        mean_validation_rmse = np.mean([sqrt(x) for x in validation_mses])\n",
    "        mean_validation_mae = np.mean(validation_maes)\n",
    "        mean_validation_r2 = np.mean(validation_r2s)\n",
    "        mean_validation_corr = np.mean(validation_corrs)\n",
    "        mean_p_value = self._get_pvalue(validation_corr_pvalues)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\"\"TrainMSE: {np.round(mean_train_mse, 3)}, TrainRMSE: {np.round(mean_train_rmse, 3)}, TrainMAE: {np.round(mean_train_mae, 3)}, TrainR2: {np.round(mean_train_r2, 3)}, TrainCorr: {np.round(mean_train_corr, 3)}, ValMSE: {np.round(mean_validation_mse, 3)}, ValRMSE: {np.round(mean_validation_rmse, 3)}, ValMAE: {np.round(mean_validation_mae, 3)}, ValR2: {np.round(mean_validation_r2, 3)}, ValCorr: {np.round(mean_validation_corr, 3)}, ValCorrPValue: {np.round(mean_p_value, 3)}\"\"\")\n",
    "            print(\"\\n---------------------------------------------------\\n\")\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.xticks(fontsize=15)\n",
    "            plt.yticks(fontsize=15)\n",
    "            plt.xlim([0,1])\n",
    "            plt.ylim([0,1])\n",
    "\n",
    "            plt.scatter(all_labels, all_predictions, s=6)\n",
    "            plt.xlabel(\"Canonization Scores\", fontsize=20)\n",
    "            plt.ylabel(\"Predicted Scores\", fontsize=20)\n",
    "            plt.savefig(results_dir + lang + \"-\" + self.model + \"-\" + str(self.dimensionality_reduction) \n",
    "            + \"-\" + self.features + \"-\" + \"-\" + \"param\" + str(self.model_param) + \"-\" + self.datetime + \".png\", \n",
    "            dpi=400, bbox_inches=\"tight\")    \n",
    "    \n",
    "            plt.show();\n",
    "        return mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value, self.datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85abcb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(Regression):\n",
    "    def __init__(self, language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose):\n",
    "        super().__init__(language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose)\n",
    "\n",
    "    def _check_class_specific_assertions(self):\n",
    "        assert model in [\"svc\"]\n",
    "        \n",
    "    def _combine_df_labels(self, df):\n",
    "        #Reviews zum englischen Korpus beginnnen mit 1759 und decken alles bis 1914 ab\n",
    "        agg_labels = self.labels[[\"book_name\"]].drop_duplicates()\n",
    "        agg_labels[\"y \"] = 1\n",
    "        df = df.merge(right=agg_labels, on=\"book_name\", how=\"left\", validate=\"many_to_one\")\n",
    "        df[\"y\"] = df[\"y\"].fillna(value=0)\n",
    "        #Select books written after 1759 (year of first review)\n",
    "        year = df[\"book_name\"].str.split('_').str[-1].astype('int64')\n",
    "        df = df.loc[year>=1759]\n",
    "        return df\n",
    "    \n",
    "    def _get_sample_weights(self, df):\n",
    "        # Weight \n",
    "        chunks_per_book = df[\"book_name\"].value_counts(sort=False).rename('chunks_per_book')\n",
    "        chunks_per_book = chunks_per_book.reset_index().rename(columns={\"index\":'book_name'})\n",
    "        chunks_per_book[\"chunks_per_book\"] = 1/chunks_per_book[\"chunks_per_book\"]\n",
    "        df = df.merge(right=chunks_per_book, how=\"left\", on=\"book_name\")\n",
    "        print(df)\n",
    "        sample_weights = df[\"chunks_per_book\"].tolist()\n",
    "        return sample_weights\n",
    "    \n",
    "    def _aggregate_chunk_predictions(self, df):\n",
    "        g = df.groupby(\"book_name\") \n",
    "        \n",
    "        # Majority vote\n",
    "        # If one value is more common, assign it to every chunk\n",
    "        # Therefore, accuracy is either 0 or 1\n",
    "        # If both values are equally likely, leave them unchanged, and accuracy is 0.5\n",
    "        def _get_mode_accuracy(group):\n",
    "            counts = group[\"yhat\"].value_counts()\n",
    "            if len(counts) == 1:\n",
    "                mode_acc = counts.index[0]\n",
    "            else:\n",
    "                mode_acc = 0.5\n",
    "            return mode_acc\n",
    "        mode_accs = g.apply(_get_mode_accuracy).rename(\"mode_acc\").reset_index() \n",
    "        mode_acc = mode_accs[\"mode_acc\"].mean()\n",
    "        \n",
    "        # Average accuracy within book\n",
    "        book_acc = g.apply(lambda group: accuracy_score(group[\"y\"], group[\"yhat\"])).mean()\n",
    "        #Accuracy when each chunk is treated as single document\n",
    "        chunk_acc = accuracy_score(df[\"y\"], df[\"yhat\"], sample_weight = self._get_sample_weights(df))\n",
    "        return {\"mode_acc\": mode_acc, \"book_acc\": book_acc, \"chunk_acc\": chunk_acc}\n",
    "        \n",
    "    def run(self):\n",
    "        train_accs = []\n",
    "        validation_accs = []\n",
    "\n",
    "        df = self.df\n",
    "        df = self._combine_df_labels(df)\n",
    "        book_names_split = self._split_booknames(df, 5)\n",
    "        all_validation_books = []\n",
    "\n",
    "        for index, split in enumerate(book_names_split):\n",
    "            train_df = df[~df[\"book_name\"].isin(split)]\n",
    "            validation_df = df[df[\"book_name\"].isin(split)]\n",
    "            \n",
    "            train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            train_y = train_df[\"y\"].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            validation_y = validation_df[\"y\"].values.ravel()\n",
    "            train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            model = self._get_model(self.model_param)\n",
    "            model.fit(train_X, train_y)\n",
    "            \n",
    "            train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "            train_books[\"yhat\"] = model.predict(train_X)\n",
    "            train_acc = self._aggregate_chunk_predictions(train_books)\n",
    "            validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "            validation_acc = self._aggregate_chunk_predictions(validation_books)\n",
    "            \n",
    "            all_validation_books.append(validation_books.reset_index())\n",
    "            \n",
    "            train_accs.append(train_acc)\n",
    "            validation_accs.append(validation_acc)\n",
    "            if self.verbose:\n",
    "                print(f\"Fold: {index+1}, TrainAcc: {np.round(train_acc, 3)}, ValAcc: {np.round(validation_acc, 3)}\")\n",
    "        \n",
    "        # Save y and y_pred for examples\n",
    "        pd.concat(all_validation_books).to_csv(results_dir + \"/valiationbooks-class-\" + self.language + \"-\" + self.datetime + \".csv\", index=False)\n",
    "        print(all_validation_books)\n",
    "       # confusion_matrix(all_validation_books[\"y\"], all_validation_books[\"yhat\"])\n",
    "        \n",
    "        \n",
    "        train_accs = pd.DataFrame(train_accs)\n",
    "        validation_accs = pd.DataFrame(validation_accs)\n",
    "        \n",
    "        mean_train_mode_acc = train_accs[\"mode_acc\"].mean()\n",
    "        mean_train_book_acc = train_accs[\"book_acc\"].mean()\n",
    "        mean_train_chunk_acc = train_accs[\"chunk_acc\"].mean()\n",
    "        mean_validation_mode_acc = validation_accs[\"mode_acc\"].mean()\n",
    "        mean_validation_book_acc = validation_accs[\"book_acc\"].mean()\n",
    "        mean_validation_chunk_acc = validation_accs[\"chunk_acc\"].mean()\n",
    "        print(mean_train_mode_acc, mean_train_book_acc, mean_train_chunk_acc)\n",
    "        print(mean_validation_mode_acc, mean_validation_book_acc, mean_validation_chunk_acc)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\"\"TrainAcc: {np.round(mean_train_acc, 3)}, ValidationAcc: {np.round(mean_validation_acc, 3)}\"\"\")\n",
    "            print(\"\\n---------------------------------------------------\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "942eca3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\naccuravy, each chunk separate doc\\naccuracy withing book accross chunks\\ny_true = book label, y_pred Label, dass bei den meisten chunks die zum Buch gehÃ¶ren vorkommt. Was ist wenn genau 50/50?\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "accuravy, each chunk separate doc\n",
    "accuracy withing book accross chunks\n",
    "y_true = book label, y_pred Label, dass bei den meisten chunks die zum Buch gehÃ¶ren vorkommt. Was ist wenn genau 50/50?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56c1f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some columns by default before running cv\n",
    "def drop_default_columns(df, drop_default_columns_including):\n",
    "    def _drop_column(column):\n",
    "        for string in drop_default_columns_including:\n",
    "            if string in column:\n",
    "                return True\n",
    "    df = df[[column for column in df.columns if not _drop_column(column)]].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "# Feature split\n",
    "complexity_features = []\n",
    "\n",
    "\n",
    "# Superfluous featues\n",
    "drop_default_columns_including = [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\"]\n",
    "\n",
    "# All parameters\n",
    "models = [\"svr\", \"lasso\", \"xgboost\", \"svc\"]\n",
    "model_params = {\"svr\": [1], \"lasso\": [1, 4], \"xgboost\": [1, 4], \"svc\": [1]}\n",
    "dimensionality_reduction = [\"ss_pca_0_95\", 'k_best_f_reg_0_10', 'k_best_mutual_info_0_10', None]\n",
    "features = [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "\n",
    "# Which parameters to use\n",
    "full_cv_params = {\"model\": models, \"dimensionality_reduction\": dimensionality_reduction, \"features\": features}\n",
    "testing_params = {\"model\": models[3], \"dimensionality_reduction\": dimensionality_reduction[0], \n",
    "                  \"features\": features[3]}\n",
    "# Old results from chr2021 paper\n",
    "eng_params = {\"model\": models[0], \"dimensionality_reduction\": dimensionality_reduction[0], \n",
    "                  \"features\": features[2], }, # svr, pca, book_and_average_chunk\n",
    "ger_params = {\"model\": models[0], \"dimensionality_reduction\": dimensionality_reduction[0], \n",
    "                  \"features\": features[1]}, # svr, pca, chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a11a8",
   "metadata": {},
   "source": [
    "book_df = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\")\n",
    "book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "book_df = drop_default_columns(book_df, drop_default_columns_including)\n",
    "book_and_averaged_chunk_df = drop_default_columns(book_and_averaged_chunk_df, drop_default_columns_including)\n",
    "chunk_df = drop_default_columns(chunk_df, drop_default_columns_including)\n",
    "chunk_and_copied_book_df = drop_default_columns(chunk_and_copied_book_df, drop_default_columns_including)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09a045",
   "metadata": {},
   "source": [
    "print(len(book_df.columns), len(book_and_averaged_chunk_df.columns),len(chunk_df.columns),len(chunk_and_copied_book_df.columns),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c542ff4",
   "metadata": {},
   "source": [
    "len(list(book_and_averaged_chunk_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b3cb2",
   "metadata": {},
   "source": [
    "for i in list(book_and_averaged_chunk_df.columns):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e4e21fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng svc chunk_and_copied_book [] ss_pca_0_95 param= 1\n",
      "                                       book_name    y  yhat  chunks_per_book\n",
      "0                 Radcliffe_Ann_The-Italian_1797  1.0   1.0          0.03125\n",
      "1                 Radcliffe_Ann_The-Italian_1797  1.0   1.0          0.03125\n",
      "2                 Radcliffe_Ann_The-Italian_1797  1.0   1.0          0.03125\n",
      "3                 Radcliffe_Ann_The-Italian_1797  1.0   1.0          0.03125\n",
      "4                 Radcliffe_Ann_The-Italian_1797  1.0   1.0          0.03125\n",
      "...                                          ...  ...   ...              ...\n",
      "11407   Edgeworth_Maria_The-Modern-Griselda_1804  1.0   0.0          0.20000\n",
      "11408   Edgeworth_Maria_The-Modern-Griselda_1804  1.0   0.0          0.20000\n",
      "11409   Edgeworth_Maria_The-Modern-Griselda_1804  1.0   0.0          0.20000\n",
      "11410   Edgeworth_Maria_The-Modern-Griselda_1804  1.0   0.0          0.20000\n",
      "11411  Baldwin_Louisa_My-Next-Door-Neighour_1894  0.0   0.0          1.00000\n",
      "\n",
      "[11412 rows x 4 columns]\n",
      "                                              book_name    y  yhat  \\\n",
      "0            Gissing_George_In-the-Year-of-Jubilee_1894  0.0   1.0   \n",
      "1            Gissing_George_In-the-Year-of-Jubilee_1894  0.0   1.0   \n",
      "2            Gissing_George_In-the-Year-of-Jubilee_1894  0.0   1.0   \n",
      "3            Gissing_George_In-the-Year-of-Jubilee_1894  0.0   1.0   \n",
      "4            Gissing_George_In-the-Year-of-Jubilee_1894  0.0   1.0   \n",
      "...                                                 ...  ...   ...   \n",
      "1753  Russell_William_Recollections-of-a-Detective-P...  0.0   0.0   \n",
      "1754  Russell_William_Recollections-of-a-Detective-P...  0.0   0.0   \n",
      "1755  Russell_William_Recollections-of-a-Detective-P...  0.0   0.0   \n",
      "1756  Russell_William_Recollections-of-a-Detective-P...  0.0   0.0   \n",
      "1757      Kipling_Rudyard_To-Be-Held-For-Reference_1888  0.0   0.0   \n",
      "\n",
      "      chunks_per_book  \n",
      "0            0.020408  \n",
      "1            0.020408  \n",
      "2            0.020408  \n",
      "3            0.020408  \n",
      "4            0.020408  \n",
      "...               ...  \n",
      "1753         0.043478  \n",
      "1754         0.043478  \n",
      "1755         0.043478  \n",
      "1756         0.043478  \n",
      "1757         1.000000  \n",
      "\n",
      "[1758 rows x 4 columns]\n",
      "                                        book_name    y  yhat  chunks_per_book\n",
      "0      Gissing_George_In-the-Year-of-Jubilee_1894  0.0   0.0         0.020408\n",
      "1      Gissing_George_In-the-Year-of-Jubilee_1894  0.0   0.0         0.020408\n",
      "2      Gissing_George_In-the-Year-of-Jubilee_1894  0.0   0.0         0.020408\n",
      "3      Gissing_George_In-the-Year-of-Jubilee_1894  0.0   0.0         0.020408\n",
      "4      Gissing_George_In-the-Year-of-Jubilee_1894  0.0   0.0         0.020408\n",
      "...                                           ...  ...   ...              ...\n",
      "10868    Edgeworth_Maria_The-Modern-Griselda_1804  1.0   0.0         0.200000\n",
      "10869    Edgeworth_Maria_The-Modern-Griselda_1804  1.0   0.0         0.200000\n",
      "10870    Edgeworth_Maria_The-Modern-Griselda_1804  1.0   0.0         0.200000\n",
      "10871    Edgeworth_Maria_The-Modern-Griselda_1804  1.0   0.0         0.200000\n",
      "10872   Baldwin_Louisa_My-Next-Door-Neighour_1894  0.0   0.0         1.000000\n",
      "\n",
      "[10873 rows x 4 columns]\n",
      "                           book_name    y  yhat  chunks_per_book\n",
      "0     Radcliffe_Ann_The-Italian_1797  1.0   0.0         0.031250\n",
      "1     Radcliffe_Ann_The-Italian_1797  1.0   0.0         0.031250\n",
      "2     Radcliffe_Ann_The-Italian_1797  1.0   0.0         0.031250\n",
      "3     Radcliffe_Ann_The-Italian_1797  1.0   0.0         0.031250\n",
      "4     Radcliffe_Ann_The-Italian_1797  1.0   1.0         0.031250\n",
      "...                              ...  ...   ...              ...\n",
      "2292  Wells_H-G_A-Modern-Utopia_1905  1.0   0.0         0.052632\n",
      "2293  Wells_H-G_A-Modern-Utopia_1905  1.0   0.0         0.052632\n",
      "2294  Wells_H-G_A-Modern-Utopia_1905  1.0   0.0         0.052632\n",
      "2295  Wells_H-G_A-Modern-Utopia_1905  1.0   0.0         0.052632\n",
      "2296  Wells_H-G_A-Modern-Utopia_1905  1.0   0.0         0.052632\n",
      "\n",
      "[2297 rows x 4 columns]\n",
      "                                      book_name    y  yhat  chunks_per_book\n",
      "0                Radcliffe_Ann_The-Italian_1797  1.0   1.0          0.03125\n",
      "1                Radcliffe_Ann_The-Italian_1797  1.0   1.0          0.03125\n",
      "2                Radcliffe_Ann_The-Italian_1797  1.0   1.0          0.03125\n",
      "3                Radcliffe_Ann_The-Italian_1797  1.0   1.0          0.03125\n",
      "4                Radcliffe_Ann_The-Italian_1797  1.0   1.0          0.03125\n",
      "...                                         ...  ...   ...              ...\n",
      "9635   Edgeworth_Maria_The-Modern-Griselda_1804  1.0   0.0          0.20000\n",
      "9636   Edgeworth_Maria_The-Modern-Griselda_1804  1.0   0.0          0.20000\n",
      "9637   Edgeworth_Maria_The-Modern-Griselda_1804  1.0   1.0          0.20000\n",
      "9638   Edgeworth_Maria_The-Modern-Griselda_1804  1.0   1.0          0.20000\n",
      "9639  Baldwin_Louisa_My-Next-Door-Neighour_1894  0.0   0.0          1.00000\n",
      "\n",
      "[9640 rows x 4 columns]\n",
      "                                    book_name    y  yhat  chunks_per_book\n",
      "0                Collins_Wilkie_Armadale_1864  1.0   0.0         0.012048\n",
      "1                Collins_Wilkie_Armadale_1864  1.0   0.0         0.012048\n",
      "2                Collins_Wilkie_Armadale_1864  1.0   0.0         0.012048\n",
      "3                Collins_Wilkie_Armadale_1864  1.0   0.0         0.012048\n",
      "4                Collins_Wilkie_Armadale_1864  1.0   0.0         0.012048\n",
      "...                                       ...  ...   ...              ...\n",
      "3525  Eliot_George_The-Mill-on-the-Floss_1860  1.0   0.0         0.023256\n",
      "3526  Eliot_George_The-Mill-on-the-Floss_1860  1.0   0.0         0.023256\n",
      "3527  Eliot_George_The-Mill-on-the-Floss_1860  1.0   0.0         0.023256\n",
      "3528  Eliot_George_The-Mill-on-the-Floss_1860  1.0   0.0         0.023256\n",
      "3529  Eliot_George_The-Mill-on-the-Floss_1860  1.0   0.0         0.023256\n",
      "\n",
      "[3530 rows x 4 columns]\n",
      "                                     book_name    y  yhat  chunks_per_book\n",
      "0               Radcliffe_Ann_The-Italian_1797  1.0   1.0         0.031250\n",
      "1               Radcliffe_Ann_The-Italian_1797  1.0   1.0         0.031250\n",
      "2               Radcliffe_Ann_The-Italian_1797  1.0   1.0         0.031250\n",
      "3               Radcliffe_Ann_The-Italian_1797  1.0   1.0         0.031250\n",
      "4               Radcliffe_Ann_The-Italian_1797  1.0   1.0         0.031250\n",
      "...                                        ...  ...   ...              ...\n",
      "11073  Eliot_George_The-Mill-on-the-Floss_1860  1.0   1.0         0.023256\n",
      "11074  Eliot_George_The-Mill-on-the-Floss_1860  1.0   1.0         0.023256\n",
      "11075  Eliot_George_The-Mill-on-the-Floss_1860  1.0   1.0         0.023256\n",
      "11076  Eliot_George_The-Mill-on-the-Floss_1860  1.0   1.0         0.023256\n",
      "11077  Eliot_George_The-Mill-on-the-Floss_1860  1.0   1.0         0.023256\n",
      "\n",
      "[11078 rows x 4 columns]\n",
      "                                              book_name    y  yhat  \\\n",
      "0     Nesbit_Edith_The-Story-of-the-Treasure-Seekers...  1.0   0.0   \n",
      "1     Nesbit_Edith_The-Story-of-the-Treasure-Seekers...  1.0   0.0   \n",
      "2     Nesbit_Edith_The-Story-of-the-Treasure-Seekers...  1.0   0.0   \n",
      "3     Nesbit_Edith_The-Story-of-the-Treasure-Seekers...  1.0   0.0   \n",
      "4     Nesbit_Edith_The-Story-of-the-Treasure-Seekers...  1.0   0.0   \n",
      "...                                                 ...  ...   ...   \n",
      "2087           Edgeworth_Maria_The-Modern-Griselda_1804  1.0   1.0   \n",
      "2088           Edgeworth_Maria_The-Modern-Griselda_1804  1.0   1.0   \n",
      "2089           Edgeworth_Maria_The-Modern-Griselda_1804  1.0   1.0   \n",
      "2090           Edgeworth_Maria_The-Modern-Griselda_1804  1.0   1.0   \n",
      "2091          Baldwin_Louisa_My-Next-Door-Neighour_1894  0.0   0.0   \n",
      "\n",
      "      chunks_per_book  \n",
      "0            0.066667  \n",
      "1            0.066667  \n",
      "2            0.066667  \n",
      "3            0.066667  \n",
      "4            0.066667  \n",
      "...               ...  \n",
      "2087         0.200000  \n",
      "2088         0.200000  \n",
      "2089         0.200000  \n",
      "2090         0.200000  \n",
      "2091         1.000000  \n",
      "\n",
      "[2092 rows x 4 columns]\n",
      "                                      book_name    y  yhat  chunks_per_book\n",
      "0                Radcliffe_Ann_The-Italian_1797  1.0   1.0          0.03125\n",
      "1                Radcliffe_Ann_The-Italian_1797  1.0   1.0          0.03125\n",
      "2                Radcliffe_Ann_The-Italian_1797  1.0   1.0          0.03125\n",
      "3                Radcliffe_Ann_The-Italian_1797  1.0   1.0          0.03125\n",
      "4                Radcliffe_Ann_The-Italian_1797  1.0   1.0          0.03125\n",
      "...                                         ...  ...   ...              ...\n",
      "9672   Edgeworth_Maria_The-Modern-Griselda_1804  1.0   0.0          0.20000\n",
      "9673   Edgeworth_Maria_The-Modern-Griselda_1804  1.0   0.0          0.20000\n",
      "9674   Edgeworth_Maria_The-Modern-Griselda_1804  1.0   0.0          0.20000\n",
      "9675   Edgeworth_Maria_The-Modern-Griselda_1804  1.0   0.0          0.20000\n",
      "9676  Baldwin_Louisa_My-Next-Door-Neighour_1894  0.0   0.0          1.00000\n",
      "\n",
      "[9677 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        book_name    y  yhat  chunks_per_book\n",
      "0                     Conrad_Joseph_Lord-Jim_1899  0.0   1.0         0.026316\n",
      "1                     Conrad_Joseph_Lord-Jim_1899  0.0   1.0         0.026316\n",
      "2                     Conrad_Joseph_Lord-Jim_1899  0.0   1.0         0.026316\n",
      "3                     Conrad_Joseph_Lord-Jim_1899  0.0   1.0         0.026316\n",
      "4                     Conrad_Joseph_Lord-Jim_1899  0.0   1.0         0.026316\n",
      "...                                           ...  ...   ...              ...\n",
      "3488  Forster_E-M_Where-Angels-Fear-to-Tread_1905  0.0   1.0         0.047619\n",
      "3489  Forster_E-M_Where-Angels-Fear-to-Tread_1905  0.0   1.0         0.047619\n",
      "3490  Forster_E-M_Where-Angels-Fear-to-Tread_1905  0.0   1.0         0.047619\n",
      "3491  Forster_E-M_Where-Angels-Fear-to-Tread_1905  0.0   1.0         0.047619\n",
      "3492  Forster_E-M_Where-Angels-Fear-to-Tread_1905  0.0   1.0         0.047619\n",
      "\n",
      "[3493 rows x 4 columns]\n",
      "[      index                                          book_name    y  yhat\n",
      "0        32         Gissing_George_In-the-Year-of-Jubilee_1894  0.0   1.0\n",
      "1        33         Gissing_George_In-the-Year-of-Jubilee_1894  0.0   1.0\n",
      "2        34         Gissing_George_In-the-Year-of-Jubilee_1894  0.0   1.0\n",
      "3        35         Gissing_George_In-the-Year-of-Jubilee_1894  0.0   1.0\n",
      "4        36         Gissing_George_In-the-Year-of-Jubilee_1894  0.0   1.0\n",
      "...     ...                                                ...  ...   ...\n",
      "1753  13979  Russell_William_Recollections-of-a-Detective-P...  0.0   0.0\n",
      "1754  13980  Russell_William_Recollections-of-a-Detective-P...  0.0   0.0\n",
      "1755  13981  Russell_William_Recollections-of-a-Detective-P...  0.0   0.0\n",
      "1756  13982  Russell_William_Recollections-of-a-Detective-P...  0.0   0.0\n",
      "1757  14075      Kipling_Rudyard_To-Be-Held-For-Reference_1888  0.0   0.0\n",
      "\n",
      "[1758 rows x 4 columns],       index                       book_name    y  yhat\n",
      "0         0  Radcliffe_Ann_The-Italian_1797  1.0   0.0\n",
      "1         1  Radcliffe_Ann_The-Italian_1797  1.0   0.0\n",
      "2         2  Radcliffe_Ann_The-Italian_1797  1.0   0.0\n",
      "3         3  Radcliffe_Ann_The-Italian_1797  1.0   0.0\n",
      "4         4  Radcliffe_Ann_The-Italian_1797  1.0   1.0\n",
      "...     ...                             ...  ...   ...\n",
      "2292  13955  Wells_H-G_A-Modern-Utopia_1905  1.0   0.0\n",
      "2293  13956  Wells_H-G_A-Modern-Utopia_1905  1.0   0.0\n",
      "2294  13957  Wells_H-G_A-Modern-Utopia_1905  1.0   0.0\n",
      "2295  13958  Wells_H-G_A-Modern-Utopia_1905  1.0   0.0\n",
      "2296  13959  Wells_H-G_A-Modern-Utopia_1905  1.0   0.0\n",
      "\n",
      "[2297 rows x 4 columns],       index                                book_name    y  yhat\n",
      "0        81             Collins_Wilkie_Armadale_1864  1.0   0.0\n",
      "1        82             Collins_Wilkie_Armadale_1864  1.0   0.0\n",
      "2        83             Collins_Wilkie_Armadale_1864  1.0   0.0\n",
      "3        84             Collins_Wilkie_Armadale_1864  1.0   0.0\n",
      "4        85             Collins_Wilkie_Armadale_1864  1.0   0.0\n",
      "...     ...                                      ...  ...   ...\n",
      "3525  14116  Eliot_George_The-Mill-on-the-Floss_1860  1.0   0.0\n",
      "3526  14117  Eliot_George_The-Mill-on-the-Floss_1860  1.0   0.0\n",
      "3527  14118  Eliot_George_The-Mill-on-the-Floss_1860  1.0   0.0\n",
      "3528  14119  Eliot_George_The-Mill-on-the-Floss_1860  1.0   0.0\n",
      "3529  14120  Eliot_George_The-Mill-on-the-Floss_1860  1.0   0.0\n",
      "\n",
      "[3530 rows x 4 columns],       index                                          book_name    y  yhat\n",
      "0       316  Nesbit_Edith_The-Story-of-the-Treasure-Seekers...  1.0   0.0\n",
      "1       317  Nesbit_Edith_The-Story-of-the-Treasure-Seekers...  1.0   0.0\n",
      "2       318  Nesbit_Edith_The-Story-of-the-Treasure-Seekers...  1.0   0.0\n",
      "3       319  Nesbit_Edith_The-Story-of-the-Treasure-Seekers...  1.0   0.0\n",
      "4       320  Nesbit_Edith_The-Story-of-the-Treasure-Seekers...  1.0   0.0\n",
      "...     ...                                                ...  ...   ...\n",
      "2087  14122           Edgeworth_Maria_The-Modern-Griselda_1804  1.0   1.0\n",
      "2088  14123           Edgeworth_Maria_The-Modern-Griselda_1804  1.0   1.0\n",
      "2089  14124           Edgeworth_Maria_The-Modern-Griselda_1804  1.0   1.0\n",
      "2090  14125           Edgeworth_Maria_The-Modern-Griselda_1804  1.0   1.0\n",
      "2091  14126          Baldwin_Louisa_My-Next-Door-Neighour_1894  0.0   0.0\n",
      "\n",
      "[2092 rows x 4 columns],       index                                    book_name    y  yhat\n",
      "0       164                  Conrad_Joseph_Lord-Jim_1899  0.0   1.0\n",
      "1       165                  Conrad_Joseph_Lord-Jim_1899  0.0   1.0\n",
      "2       166                  Conrad_Joseph_Lord-Jim_1899  0.0   1.0\n",
      "3       167                  Conrad_Joseph_Lord-Jim_1899  0.0   1.0\n",
      "4       168                  Conrad_Joseph_Lord-Jim_1899  0.0   1.0\n",
      "...     ...                                          ...  ...   ...\n",
      "3488  13896  Forster_E-M_Where-Angels-Fear-to-Tread_1905  0.0   1.0\n",
      "3489  13897  Forster_E-M_Where-Angels-Fear-to-Tread_1905  0.0   1.0\n",
      "3490  13898  Forster_E-M_Where-Angels-Fear-to-Tread_1905  0.0   1.0\n",
      "3491  13899  Forster_E-M_Where-Angels-Fear-to-Tread_1905  0.0   1.0\n",
      "3492  13900  Forster_E-M_Where-Angels-Fear-to-Tread_1905  0.0   1.0\n",
      "\n",
      "[3493 rows x 4 columns]]\n",
      "0.344607086952615 0.9924237499284289 0.992423749928429\n",
      "0.3478450576011552 0.6326426565338324 0.6326426565338324\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "param_dict = \"testing\" #\"full_cv\", \"language_specific\"\n",
    "for lang in [\"eng\"]: #, \"ger\"]:    \n",
    "    if param_dict==\"testing\":\n",
    "        param_dir = testing_params\n",
    "    elif param_dict==\"full_cv\":\n",
    "        param_dir = full_cv_params\n",
    "    elif param_dict==\"language_specific\":\n",
    "        if lang==\"eng\":\n",
    "            param_dir = eng_params\n",
    "        else: \n",
    "            param_dir = ger_params\n",
    "    \n",
    "    #Eng: 606 books, 14146 chunks, 13170 chunks of books published after 1759\n",
    "    #book_df = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\")\n",
    "    #book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "    #chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "    chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "    \n",
    "    #book_df = drop_default_columns(book_df, drop_default_columns_including)\n",
    "    #book_and_averaged_chunk_df = drop_default_columns(book_and_averaged_chunk_df, drop_default_columns_including)\n",
    "    #chunk_df = drop_default_columns(chunk_df, drop_default_columns_including)\n",
    "    chunk_and_copied_book_df = drop_default_columns(chunk_and_copied_book_df, drop_default_columns_including)\n",
    "    \n",
    "    for model in [] + [param_dir['model']]:\n",
    "        model_param = model_params[model]\n",
    "        for model_param in model_param:\n",
    "            for dimensionality_reduction in [param_dir[\"dimensionality_reduction\"]]:\n",
    "                for features in [param_dir[\"features\"]]:\n",
    "                    for drop_columns_including in [[]]:\n",
    "                        if model == \"svc\":\n",
    "                            experiment = Classification(\n",
    "                                language=lang,\n",
    "                                features=features,\n",
    "                                drop_columns_including=drop_columns_including,\n",
    "                                dimensionality_reduction=dimensionality_reduction,\n",
    "                                model_param=model_param,\n",
    "                                model=model,\n",
    "                                verbose=False\n",
    "                            )\n",
    "                            print(lang, model, features, drop_columns_including, dimensionality_reduction, 'param=', model_param)\n",
    "                            classification_results = experiment.run()\n",
    "                            #results.append((lang, model, features, drop_columns_including, dimensionality_reduction, model_param, mean_train_acc, mean_validation_acc))\n",
    "                        else:\n",
    "                            #try:\n",
    "                            experiment = Regression(\n",
    "                                language=lang,\n",
    "                                features=features,\n",
    "                                drop_columns_including=drop_columns_including,\n",
    "                                dimensionality_reduction=dimensionality_reduction,\n",
    "                                model_param=model_param,\n",
    "                                model=model,\n",
    "                                verbose=True\n",
    "                            )\n",
    "                            print(lang, model, features, drop_columns_including, dimensionality_reduction, 'param=', model_param)\n",
    "                            mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value, datetime = experiment.run()\n",
    "                            results.append((lang, model, features, drop_columns_including, dimensionality_reduction, model_param, mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value))\n",
    "\n",
    "                            #except Exception as e:\n",
    "    #                             print(f\"Error in {lang}, {model}, {features}, {drop_columns_including}, {dimensionality_reduction}\")\n",
    "    #                             print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2103539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.DataFrame(results, columns=[\"lang\", \"model\", \"features\", \"drop_columns_including\", \n",
    "#             \"dimensionality_reduction\", \"model_param\", \"mean_train_mse\", \"mean_train_rmse\", \n",
    "#             \"mean_train_mae\", \"mean_train_r2\", \"mean_train_corr\", \"mean_validation_mse\", \"mean_validation_rmse\",\n",
    "#             \"mean_validation_mae\", \"mean_validation_r2\", \"mean_validation_corr\", \"mean_p_value\"])\n",
    "# results_df.to_csv(results_dir + param_dict + datetime + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
