{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b64b32c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../src/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "extracted_features_dir = \"../data/extracted_features/\"\n",
    "results_dir = \"../data/results/classification_library/\"\n",
    "sentiment_dir = \"../data/evaluationscore/\"\n",
    "canonization_labels_dir = \"../data/labels/\"\n",
    "lang = \"ger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b8e470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAE4CAYAAADmYq9ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABIzklEQVR4nO3debwdVZX3/88iDA6ogERkDmpQ44SaBqduRVABbRNt7AZbxeFpmp/i7E9jd/ugPSjaODSK0qDYoCiNKBAhygyKMiSEkIGQkZCEzPN8b26ynj/W2lTlcO6Q3ENybvJ9v173dc6pU8Peu/betWpXnbrm7oiIiIhI+9prVydARERERHqmgE1ERESkzSlgExEREWlzCthERERE2pwCNhEREZE2p4BNREREpM3tvasT0EoHH3ywDxkyZFcnQ0RERKRXDzzwwDJ3H9yXeXergG3IkCGMGzduVydDREREpFdm9lhf59UlUREREZE2p4BNREREpM0pYBMRERFpcwrYRERERNpcSwI2MzvFzKaZ2UwzG9Xk+5eY2T1m1mFmX2j4bo6ZTTKzCWY2rjb9IDO7xcxm5OuBrUiriIiIyEDT74DNzAYBFwGnAsOAM81sWMNsK4BPARd0s5oT3f04dx9emzYKuM3dhwK35WcRERGRPU4rRtiOB2a6+2x37wSuAkbUZ3D3Je4+Fti8HesdAVye7y8HRrYgrSIiIiIDTisCtsOBebXP83NaXzlws5k9YGZn16Yf4u4LAfL1ef1OqYiIiMgA1IoH51qTab4dy7/R3ReY2fOAW8zsEXf/Q583HkHe2QBHHXXUdmxWREREZGBoxQjbfODI2ucjgAV9XdjdF+TrEuBa4hIrwGIzOxQgX5d0s/wl7j7c3YcPHtyn/+4gIiIiMqC0ImAbCww1s2PMbF/gDGB0XxY0s2ea2bPKe+DtwOT8ejRwVr4/C7i+BWkVERER2aWGjLqRIaNu3K5l+n1J1N27zOxc4CZgEHCZu08xs3Py+4vN7PnAOODZwFYz+wzxi9KDgWvNrKTlF+7++1z1+cDVZvYxYC7wvv6mVURERGQgask/f3f3McCYhmkX194vIi6VNloDvKqbdS4HTmpF+kREREQGMv2nAxEREZE2p4BNREREpM0pYBMRERFpcwrYRERERNqcAjYRERGRNqeATURERKTNKWATERERaXMK2ERERETanAI2ERERkTangE1ERESkzSlgExEREWlzCthERERE2pwCNhEREZE2p4BNREREpM0pYBMRERFpcwrYRERERNqcAjYRERGRNqeATURERKTNKWATERERaXMK2ERERETanAI2ERERkTangE1ERESkzSlgExEREWlzLQnYzOwUM5tmZjPNbFST719iZveYWYeZfaE2/Ugzu8PMpprZFDP7dO27r5rZ42Y2If9Oa0VaRURERAaavfu7AjMbBFwEvA2YD4w1s9Hu/nBtthXAp4CRDYt3AZ939/Fm9izgATO7pbbsd939gv6mUURERGQga8UI2/HATHef7e6dwFXAiPoM7r7E3ccCmxumL3T38fl+LTAVOLwFaRIRERHZbbQiYDscmFf7PJ8dCLrMbAjwauC+2uRzzWyimV1mZgf2K5UiIiIiA1QrAjZrMs23awVm+wO/Bj7j7mty8o+AFwLHAQuBb3ez7NlmNs7Mxi1dunR7NisiIiIyILQiYJsPHFn7fASwoK8Lm9k+RLB2pbv/pkx398XuvsXdtwKXEpden8TdL3H34e4+fPDgwTuUAREREZF21oqAbSww1MyOMbN9gTOA0X1Z0MwM+Akw1d2/0/DdobWP7wEmtyCtIiIiIgNOv38l6u5dZnYucBMwCLjM3aeY2Tn5/cVm9nxgHPBsYKuZfQYYBrwS+CAwycwm5Cr/yd3HAN8ys+OIy6tzgH/sb1pFREREBqJ+B2wAGWCNaZh2ce39IuJSaaO7aX4PHO7+wVakTURERGSg0386EBEREWlzCthERERE2pwCNhEREZE2p4BNREREpM0pYBMRERFpcwrYRERERNqcAjYRERGRNqeATURERKTNKWATERERaXMK2ERERER2giGjbtzhZRWwiYiIiLQ5BWwiIiIibU4Bm4iIiEibU8AmIiIi0uYUsImIiIi0OQVsIiIiIm1OAZuIiIhIm1PAJiIiItLmFLCJiIiItDkFbCIiIiJtTgGbiIiISJtTwCYiIiLS5hSwiYiIiLS5lgRsZnaKmU0zs5lmNqrJ9y8xs3vMrMPMvtCXZc3sIDO7xcxm5OuBrUiriIiIyEDT74DNzAYBFwGnAsOAM81sWMNsK4BPARdsx7KjgNvcfShwW34WERER2eO0YoTteGCmu892907gKmBEfQZ3X+LuY4HN27HsCODyfH85MLIFaRUREREZcFoRsB0OzKt9np/T+rvsIe6+ECBfn9fPdIqIiIgMSK0I2KzJNN8Jy8YKzM42s3FmNm7p0qXbs6iIiIjIgNCKgG0+cGTt8xHAghYsu9jMDgXI1yXNVuDul7j7cHcfPnjw4O1KuIiIiMhA0IqAbSww1MyOMbN9gTOA0S1YdjRwVr4/C7i+BWkVERERGXD27u8K3L3LzM4FbgIGAZe5+xQzOye/v9jMng+MA54NbDWzzwDD3H1Ns2Vz1ecDV5vZx4C5wPv6m1YRERGRgajfARuAu48BxjRMu7j2fhFxubNPy+b05cBJrUifiIiIyECm/3QgIiIi0uYUsImIiIi0OQVsIiIiIm1OAZuIiIhIm1PAJiIiItLmFLCJiIiItDkFbCIiIiJtTgGbiIiISJtTwCYiIiLS5hSwiYiIiLQ5BWwiIiIibU4Bm4iIiEibU8AmIiIi8hQZMupGhoy6sd/rUcAmIiIi0uYUsImIiIi0OQVsIiIiIm1OAZuIiIhIm1PAJiIiItJirfihQZ0CNhEREZE2p4BNREREpM0pYBMRERFpcwrYRERERNqcAjYRERGRNteSgM3MTjGzaWY208xGNfnezOzC/H6imb0mp7/YzCbU/taY2Wfyu6+a2eO1705rRVpFREREngqt+jdUzezd3xWY2SDgIuBtwHxgrJmNdveHa7OdCgzNvxOAHwEnuPs04Ljaeh4Hrq0t9113v6C/aRQREREZyFoxwnY8MNPdZ7t7J3AVMKJhnhHAFR7uBQ4ws0Mb5jkJmOXuj7UgTSIiIiK7jVYEbIcD82qf5+e07Z3nDOCXDdPOzUuol5nZgS1Iq4iIiEhLPVWXQetaEbBZk2m+PfOY2b7Au4Ff1b7/EfBC4pLpQuDbTTdudraZjTOzcUuXLt2OZIuIiIgMDK0I2OYDR9Y+HwEs2M55TgXGu/viMsHdF7v7FnffClxKXHp9Ene/xN2Hu/vwwYMH9yMbIiIiIu2pFQHbWGComR2TI2VnAKMb5hkNfCh/Lfo6YLW7L6x9fyYNl0Mb7nF7DzC5BWkVERER6ben8hehzfT7V6Lu3mVm5wI3AYOAy9x9ipmdk99fDIwBTgNmAhuAj5TlzewZxC9M/7Fh1d8ys+OIS6dzmnwvIiIiskfod8AG4O5jiKCsPu3i2nsHPtHNshuA5zaZ/sFWpE1ERERkoNN/OhARERFpcwrYRERERNqcAjYRERGRNqeATURERKTNKWATERERaXMK2ERERETanAI2ERERkT7amQ/LrVPAJiIiItKDnf1fDZpRwCYiIiLS5hSwiYiIiLQ5BWwiIiIibU4Bm4iIiEibU8AmIiIi0uYUsImIiIi0OQVsIiIisscrj+2oP8JjVz/Ko04Bm4iIiEibU8AmIiIie6R2eCBuXylgExEREWlzCthERERE2pwCNhEREdmjDJTLoHUK2ERERETanAI2ERER2W01e1zHQKSATURERKTNtSRgM7NTzGyamc00s1FNvjczuzC/n2hmr6l9N8fMJpnZBDMbV5t+kJndYmYz8vXAVqRVREREZKDpd8BmZoOAi4BTgWHAmWY2rGG2U4Gh+Xc28KOG70909+PcfXht2ijgNncfCtyWn0VERET2OK0YYTsemOnus929E7gKGNEwzwjgCg/3AgeY2aG9rHcEcHm+vxwY2YK0ioiIiAw4rQjYDgfm1T7Pz2l9nceBm83sATM7uzbPIe6+ECBfn9eCtIqIiMhuqh3/B2irtCJgsybTfDvmeaO7v4a4bPoJM/ur7dq42dlmNs7Mxi1dunR7FhUREZEBbncMzpppRcA2Hziy9vkIYEFf53H38roEuJa4xAqwuFw2zdclzTbu7pe4+3B3Hz548OB+ZkVERESk/bQiYBsLDDWzY8xsX+AMYHTDPKOBD+WvRV8HrHb3hWb2TDN7FoCZPRN4OzC5tsxZ+f4s4PoWpFVERER2A3vKyFrR74DN3buAc4GbgKnA1e4+xczOMbNzcrYxwGxgJnAp8PGcfghwt5k9BNwP3Ojuv8/vzgfeZmYzgLflZxEREdnD7M73pvVVS57D5u5j3P1Yd3+hu/9HTrvY3S/O9+7un8jvX+Hu43L6bHd/Vf69rCyb3y1395PcfWi+rmhFWkVERKQ9NAvEFJw1p/90ICIiItLmFLCJiIjITqORsx2jgE1ERESkzSlgExERkaecRtb6RwGbiIiISJtTwCYiIiJPCY2qtY4CNhEREWkpBWqtp4BNRERE+k1B2lNLAZuIiIjsMAVqO4cCNhEREelW/T8P6L8Q7DoK2ERERGQb9eBM2oMCNhEREQE0ctbOFLCJiIiItDkFbCIiInswXf4cGBSwiYiIiLQ5BWwiIiK7Of3Sc+BTwCYiIiLS5hSwiYiIDHDNRs50b9ruRQGbiIjIAKWAbM+hgE1ERKRNaeRMCgVsIiIibUYBmTRSwCYiIrILaeRM+kIBm4iIyE6i4Ex2VEsCNjM7xcymmdlMMxvV5Hszswvz+4lm9pqcfqSZ3WFmU81sipl9urbMV83scTObkH+ntSKtIiIirdbs2WZ63pm0Ur8DNjMbBFwEnAoMA840s2ENs50KDM2/s4Ef5fQu4PPu/lLgdcAnGpb9rrsfl39j+ptWERGR/lIgJrtCK0bYjgdmuvtsd+8ErgJGNMwzArjCw73AAWZ2qLsvdPfxAO6+FpgKHN6CNImIiOwQjZJJO2pFwHY4MK/2eT5PDrp6ncfMhgCvBu6rTT43L6FeZmYHtiCtIiIiT2gWnIm0o1YEbNZkmm/PPGa2P/Br4DPuviYn/wh4IXAcsBD4dtONm51tZuPMbNzSpUu3M+kiIrK76+n+MpGBohUB23zgyNrnI4AFfZ3HzPYhgrUr3f03ZQZ3X+zuW9x9K3Apcen1Sdz9Encf7u7DBw8e3O/MiIjIwKJLmLInaEXANhYYambHmNm+wBnA6IZ5RgMfyl+Lvg5Y7e4LzcyAnwBT3f079QXM7NDax/cAk1uQVhERGSCaPQJDj8WQPVW/AzZ37wLOBW4ifjRwtbtPMbNzzOycnG0MMBuYSYyWfTynvxH4IPDWJo/v+JaZTTKzicCJwGf7m1YREdk1egu6FIiJ9GzvVqwkH7kxpmHaxbX3DnyiyXJ30/z+Ntz9g61Im4iI7DpDRt3InPPfuauTITLg6T8diIhIS2mUTKT1FLCJiIiItDkFbCIist16ug9NRFpPAZuIiDxBPwgQaU8K2EREdgO9PYust2kKxETamwI2EZE21tegS0R2bwrYRETahAIxEemOAjYRkV1MwZmI9EYBm4jITqRfVIrIjlDAJiKyg3p7tIWCMxFpFQVsIrJH6+tjLPSLShHZlRSwicgeQ0GXiAxUCthEZMDTiJiI7O4UsIlI21IgJiISFLCJyE6np+6LiGwfBWwi0i878i+RRERk+yhgE5E+05P4RUR2DQVsItKUgjMRkfahgE1EFJyJiLQ5BWwiexgFZyIiA48CNpHdTG//JklERAYeBWwiA0Rf/3WSiIjsfhSwiewi2/s/LEVEZM/VkoDNzE4xs2lmNtPMRjX53szswvx+opm9prdlzewgM7vFzGbk64GtSKvIrqSHwoqIyI7od8BmZoOAi4BTgWHAmWY2rGG2U4Gh+Xc28KM+LDsKuM3dhwK35WeRAUlBmoiI9EcrRtiOB2a6+2x37wSuAkY0zDMCuMLDvcABZnZoL8uOAC7P95cDI1uQVpGnnP7FkoiItForArbDgXm1z/NzWl/m6WnZQ9x9IUC+Pq8FaRXZLn29v0zBmYiIPKXcvV9/wPuAH9c+fxD4fsM8NwJvqn2+DXhtT8sCqxrWsbKb7Z8NjAPGHXXUUX70l25wd/ejv3TDNu+7m9b4fbtP60ue2n1au5RlX6aJiIg8VYBx3sd4qxUjbPOBI2ufjwAW9HGenpZdnJdNydclzTbu7pe4+3B3Hz548OAdzoTInPPf2fS9iIjIrtaKgG0sMNTMjjGzfYEzgNEN84wGPpS/Fn0dsNrjMmdPy44Gzsr3ZwHXtyCtsocrgZiCMxERGUj27u8K3L3LzM4FbgIGAZe5+xQzOye/vxgYA5wGzAQ2AB/padlc9fnA1Wb2MWAucflUpEfNAjEFZCIiMtD1O2ADcPcxRFBWn3Zx7b0Dn+jrsjl9OXBSK9InA19vgZiCMhER2Z3pPx3ILtFT0KVATEREZFstGWET6Y6CLxERkf5TwCYto+BMRETkqaFLotJvCs5ERESeWhphk+2iX16KiIjsfArYpFsKzkRERNqDLomKiIiItDmNsMk2NJomIiLSfjTCJoACNRERkXamEbY9mII0ERGRgUEB225O/z1ARERk4NMlUREREZE2p4BNREREpM3pkugA1dd/ni4iIiIDnwK2NqVATERERApdEm0zCspERESkkQI2ERERkTangK0NaFRNREREeqKAbRdSoCYiIiJ9oYDtKaQfDoiIiEgrKGBrEf0nAREREXmqKGATERERaXMK2ERERETaXL8CNjM7yMxuMbMZ+XpgN/OdYmbTzGymmY2qTf9PM3vEzCaa2bVmdkBOH2JmG81sQv5d3J90ioiIiAxk/R1hGwXc5u5Dgdvy8zbMbBBwEXAqMAw408yG5de3AC9391cC04Ev1xad5e7H5d85/UxnS+l+NREREdmZ+huwjQAuz/eXAyObzHM8MNPdZ7t7J3BVLoe73+zuXTnfvcAR/UzPU0rBmYiIiOwK/Q3YDnH3hQD5+rwm8xwOzKt9np/TGn0U+F3t8zFm9qCZ3WVmf9nPdIqIiIgMWL3+83czuxV4fpOv/rmP27Am07xhG/8MdAFX5qSFwFHuvtzMXgtcZ2Yvc/c1TdJ3NnA2wFFHHdV0Y/2hUTURERHZ1XoN2Nz95O6+M7PFZnaouy80s0OBJU1mmw8cWft8BLCgto6zgHcBJ7m75zY7gI58/4CZzQKOBcY1Sd8lwCUAw4cP92W9ZaiPFKiJiIhIu+jvJdHRwFn5/izg+ibzjAWGmtkxZrYvcEYuh5mdAnwJeLe7bygLmNng/LECZvYCYCgwu59pFRERERmQ+huwnQ+8zcxmAG/Lz5jZYWY2BiB/VHAucBMwFbja3afk8j8AngXc0vD4jr8CJprZQ8A1wDnuvqKfae2VRtVERESkHfV6SbQn7r4cOKnJ9AXAabXPY4AxTeZ7UTfr/TXw6/6kTURERGR3of90gEbWREREpL0pYBMRERFpcwrYRERERNqcAjYRERGRNqeATURERKTNKWATERERaXMK2ERERETanAI2ERERkTangE1ERESkzSlgExEREWlze2zApv9uICIiIgPFHhuwiYiIiAwU/frn7wONRtVERERkINojRtgUqImIiMhAtkcEbCIiIiIDmQI2ERERkTangE1ERESkzSlgExEREWlzCthERERE2txu+1gP/TJUREREdhe73QibAjURERHZ3ex2AZuIiIjI7kYBm4iIiEib61fAZmYHmdktZjYjXw/sZr5TzGyamc00s1G16V81s8fNbEL+nVb77ss5/zQze0d/0ikiIiIykPV3hG0UcJu7DwVuy8/bMLNBwEXAqcAw4EwzG1ab5bvuflz+jcllhgFnAC8DTgF+mOsRERER2eP0N2AbAVye7y8HRjaZ53hgprvPdvdO4Kpcrrf1XuXuHe7+KDAz1yMiIiKyx+lvwHaIuy8EyNfnNZnncGBe7fP8nFaca2YTzeyy2iXV3pYRERER2WP0GrCZ2a1mNrnJX2+jZE+sosk0z9cfAS8EjgMWAt/uwzKN6TvbzMaZ2bilS5f2MUkiIiIiA0evD85195O7+87MFpvZoe6+0MwOBZY0mW0+cGTt8xHAglz34tq6LgVu6G2ZJum7BLgEYPjw4U2DOhEREZGBrL+XREcDZ+X7s4Drm8wzFhhqZseY2b7EjwlGA2SQV7wHmFxb7xlmtp+ZHQMMBe7vZ1pFREREBqT+/muq84GrzexjwFzgfQBmdhjwY3c/zd27zOxc4CZgEHCZu0/J5b9lZscRlzvnAP8I4O5TzOxq4GGgC/iEu2/pZ1pFREREBiRz332uIg4fPtzHjRu3q5MhIiIi0isze8Ddh/dlXv2nAxEREZE2t1uNsJnZUuAx4GBgWe0VTet1WjukYU+b1g5p2NOmtUMa9rRp7ZCGPW1aO6RhT5u2o+t5prsPpi/cfbf7A8bVXzWt92ntkIY9bVo7pGFPm9YOadjTprVDGva0ae2Qhj1tWn/X05c/XRIVERERaXMK2ERERETa3O4asF3S8KppvU9rhzTsadPaIQ172rR2SMOeNq0d0rCnTWuHNOxp0/q7nl7tVj86EBEREdkd7a4jbCIiIiK7DQVsIiIiIm1u712dABER2bOZ2QuJ/yd9JPHvCGcAjwIvA1YT/7rwPndfV1vmFHf//Q5s6yXACOBw4t8iLgBGu/vU7VzP8YC7+1gzGwacAjzi7mO2N027Su3/ey9w91vN7P3AG4CpwCXuvrmHZU8Aprr7GjN7OjAKeA3xLyW/7u6r+5iGK9z9Q/3NS219nwKudfd5rVpnu9jj7mEzs+e5+5J8b8DxbNtw73d3N7Pnuvvy7VjvPo2V28wOdvdl3S2znek+Cljj7qvMbAgwnOgcJu/g+vYCcPet2WhfDsxx9xXdzP9xd//hjqW+x3TsC2z2rIhmdiLZ6N39d70sO5xaB+/uj3Qz3/7AscDsLL+WlmUP6eu1TvRUB3dge69094n9THbbbOep1s+690Q/0sM8L+muTjaZt9v21dd6vqOa9AVn5Fdr6UN93J58drP8p4C/JoKzOcAE4GjgpHz/KmA28Czgk+5+fS43HnhbYz9tZh9x95/W+/Cyv8zsS8CZwFXAKmAdcETm+Sp3P79hXdv0HbXp3wf+ghj0uAU4AbgTOBm4yd3/o4f8PqnubO/xpj8ayuVKIg/PIMpjf+A3RNmbu5/Vw3qmAK/y+H/hlwAbgGty2Ve5+3ubLDO6cRJwInA7gLu/u3+5AzNbQ+zXWcBvgZ+6+9L+rrdhG722/ybL9H8fb89D29rxD3hlD999ATiIaPy3A9OBLcBGoBPYmq/rgT/lPOvy82yiAs8GrgZeBLwWOBDYj+jYDwHOBh4HlhIN904iALgDWAP8OzCeOEt8PP825ronA98HTicafAmgjTgD/Dzw98BFxNnmI8B38/Ua4izoPmAi8AvghcA3gF8C/wUcUMoIuC7XdyrwN8BiYGFu577M92rgH4DP5d9XgP8/l1uW035YW8+/18r64/X9QTT81wAHNNtHwCsynw/l+k4kGtfDwMU5/delXHI9ZX1vBsYBdwErgRuyTCYDXwP+QBwAXgK8F5iX+2NBLrccmA/8Ry53PTA3y+jrZdnavjgh1/OeWnrOze+H5/R3A6/PaW/L8l0G3AwMqa1vAvB3mY93Z9puAX6cf78HZgIfBf6KqG/HEHXklJz2itynpwNvz9eTibo9E7gQuBR4ecM+adpWiFsj9gI+DuxLBJAHNcxzUubzfKLdLAQuJ+rDFKLuLAXGAjdmua4DVhD19Ls5fU7utzc2KduP1t7/FdkecvsnAl8l6l29zN8K/CvRTruATUR7m0zV5mYRbXEpMAn4Qe7jR7J8v0hV9/4tl1mR+/A3wPuAP+Y6HgTekvk4MNNQ2sO+mafnZDl1EX3JWmBaTns70Va/ldv6H6K+L8/1/BvwQeAnwD25nVup6vnCnP6y3F9DMu8XEm35DuDnRIB3S5bBA8BlwM+A9wMHZ5kelfMszu3/L9CR79cTdbfUx7fnMs8n2tdFwHNzn3QC12Yafga8P+cdQvRH03K7exF19Y5cZynXLbnPNgOHAXdnPt9C9JVvy22szvRdSNSBjlzPY8CVtXwtznKbnftyTM772tyvM4m6+Wgu++Ysr3W53b/NfM3KcriX2Pfzcp9uIE6uvkn0vZuIAAXg6Tnt+UQbvBY4LufdSNSxtURffRDwjlz3qtprV/5tzmVW5DYWEsewXwCH1PqnE4m+cnlu50e1/bMwt3k10WZWEe13S25jS857Z05fAPyOqPebqNr1EuL4eFGW4905f0embyNRvw+o9XOlbY+n6jfHE/VzXL7enGn4MfCNXPaHRD14MNOwmehLpwL/H9E2Juf+KcfC44g2fRxRV14AnJVlvQy4P5e/iqptdGWZ31ibtp7ow15NtOOf5DzXAC+mOm7MJ9r/QbU+qvRLf038+vPg2vTZmd5VRL1bnem8F/hwn+OdXR1wtSBgKwepm7Oyfo44AF9PNKr1uWO2ZsXyrICbgdty2WnEwWdWFuaK3Mn/STS+0bnc5lxPF9FwOrNCLQM+TVT2LuIA9uP8bktWrulER7UuK08n0bFszmU2Ep3MqKwMpfGsy+9mEYHWFmAwEWD8D9GQrgA+m/P/PPPnRMfy7lzXVqIBdOT7O4E3ZfksyvnnZbq25LyL87tv5LoeIQ4cD+d2PdM3LfP6uVz3/KyQ9+U6yz76t9o+6gL+mehQ1me5r8vtzCIa14osq/r+W5DfDyYa/99lmjpyO52ZruW1bc8DvlPbfyuJDqMrl5tO1Ul6LrMg8zsn03Jv/s3N77dmnmdnuku5Tsr83Jfl8ABxeWcR0QmW9JU8PZjpu6FWp6/MeTZRda4zqOrelszLH3O7c3O9m4H/S9RHz/nLScrM/PxYw364jOjU1lC1lRL4fIwqOF6f6ynbXpr7qivLaGzOUw6iC3O9pYNbn/OvJurOlkxjJ9GJbcx1rqvlewMRvFxc279dWV6lzEu93UAcnDfkuv6Ueb8817cql52U6y4H4/L6ONEflHq4Fbgp35c6sSjzXg4iK3Kd9+Q+2AD8d27/0fzuJqqDbtl2aZvrcrv1+rSFql8oB+w/EW1wUpbfmlp5lHJYltubRvQHS4k2sJSo74uAkURftpJoe4/mdj+X2/Kc9jlixH1hpmVDbX+Uk91STiVPm7Mcz85t/DrL/Pws4/szf06cVJX6WerTwtq+7AA2ZFvYRPSb03Jbx1Ed7DYRfevyXHZuzuNZrmsyvSVvpW97gOjbVxMH03GZxs5cZm1uc2F+tyFfl+Yy49m2v1ya215M9MOTib6iHG8811n6eacKjFdmWqYT7WIBcUJzfS7zJSKQWEe03Y5M1wziWDGP2PfT87vydz1Vv3Ya0d43EwHLYuK4Nj3LaVWmeXPut7VEwLWW6E++ntsfR/Q5W6kGJ36e27qJGCi4lwh2O3Idt2ee/5xp/QTw7dwPlxF10qmOp/cTfcZ8oh6dkOmbShxXpxPB4gzimO+5T0q7Kn+ba3+P5+u4zMcSos1uIU72luf3D+V6/zHLZ1Lmp8QRpe8u+7D0uXflsqUersp5/pjlsiHz/yfiZGM6cTxaCwwl+qiv7ykB24NE59KRBbCIquPanIW4kWgY5wFdudwMYih4I3Fg3ofoAO4jDjidOd+kXO/MrDClA+/KbW/JnVGGYTfl38z8vrO2nXFZiafmDt6QaX+EOPA/mvPfk9MuJIKyzfm6Iivm96lGDJZnBbww0zSfaJRbgAuogqCpRGcwN9fx89zeJuJsfQNx9ruJ6Ahm5TpKJ1wCpn/Nsp6c5XAfccD2LPs1mcefZMVdkeu5ljiol0DEiVGXuZnuf8u8v5poLA/mNsbm+h4lzpRX5/wXZr6X5t8W4sxvTS53d+Z9BtH5biFG6DYCz6TqoIZnesqyU4gOeXZ+Xw6IvyU6r8eJBubEiNzynHcMVcPfnOn7ab6/kmpkd2rmbULmY2ZO6wSGZJ1bn/txYr5/OJc/IdNSDpSduY0uIlDoymmX5vxnUY0GbCDq8eNEh72V6uD/37kvnKgX9xIdbRfRbi7N99dkfsYTox5ridG/R3NfdGRZrskyGU+Mbm/I9E4jOvoHct4rM/8zc91fJ9rFrNw/H8/8bMx9t4EIIjtqZb4py2hdlt1Dmbe92PZg/2Du23H5eS3Rbudken9P/F+/rZnmko+pWQZ35vKTc12P535ZT4yifzXL5rws+0eJA+kUYrR5Sk6fRYyOllGDh4mRkK25X1+b8x2XaTiaqAtbiPa0mWgr5aSsnNCMy+9GEiMUpW2eTdXOrybq5bqc9yf52pnl0QE8O/N4U+ZxHfCrXF8JPruIk48NuR9WUrXRGbmdP2eeLiHqxpJMYwleFuW63k70AROpDoJL8/WVVMHEBKJP3JsqSNxCjAb9MPNxa37uyv35cO6r0lbmEqPUm2rzXZLp2Uqc3I6vpWdLlt9moi09RtSXs3LaybmORUQ9W0ScqJeT8H8nrvJsIvuVrKNbidHDH+b7smwJOKflfCVonVZ770T7KVeIVlPV842Z3wuo+tktRN29I6fV55tEBJhlNG8+cdJSTmxupQout1Idn0pg/yhxXConTmVgYEvuLyfqxpZc98LadpyoK2W//zNR195JVbfvyPIfT9S18VT1oJTH40S/+2iW9V3A1lp84ERfVoLwuzI90/O7ibmOEjRuzvnW5rTFVCfMS7NszqMauLiB6gT3Yqpjz0aijpX6PiPnuyPzeUdJJ9FXPdKXeGd3+JWoe9x7NJToUG4mzjCPpLrkshp4GjFc32VmbyI6lXLgupdoBPsQlwkWAWvN7GtEBRlEVJhOosFuIu5peHW+fy9x0JlI7MgpwKHEJbmtwKeIHff0fH0OsfMt025ExzQy0/C83N5HiI6vI9f1DKJCvYu4/HIwUVE3ZPrLmcXvcrv/Q1Tm/YiDxlLifozNRAf6tFzPsVmW1xJnQR8hKv+cTNvHqM4mn01ckpuey5zo7kPyu3cTFXUY8NIst4tzvr8iLgN+jujknbiXZAVRYYdmWf46992xxAF8X6pO4C+JMxMj77Eggq+S92k5/wuIA98ziEsTz8/vJ+c2LiX283qiwxpEnPltybKGasSzjGCeRJx5LScuG+PuIzNfhxHD5QdQjTzOpLrMcDNRB/cj7pfx3NYSojHfmml4wMzmEPXkIqqAuYO41+o+quBrM3Hwuje3UQ6Qm4gzOXf3y7OsH8x8lpGSxcTBer8sv3cS7cTd/QNEAF/OxPenCvrGZJmbu9+SaT4607k28zuptp3nEaPC+2QZzctplmV8FtW9e2cS7cjy85rM06Lcp8PyuyuJA1sp865M515m9pVcFqoz9yGZtgOIA8V+uQ/3Bc4hLkvtTwRN03Ibv860riDq0Bqi7ziMqEv75ftDcj+cSlyW2ivLEqpAs4wKlr72UaJ/WEns26NzWbLcHsx1Tszl/2/t89NzPd/LPAC8Lr/7bH6+jKiTW4ig5fW5zEzihGxvIlBcQ1yOM6LOdBDBxl1Ev/IKqpPDk6lGy6YTB5r3Uo0SrMoyWZvlfjrRb24lRnf2yrx8P6ftR9Sd5cTBbz/iMnUXcdl+MHF5+ALigH0rcbD+fe6jhbn+O4lbVZZmuZWrEh05rfQjkzO/5YTmm1SjMkdRndz9kehnn5bLORGkbSL2+wZgRbarBUQfs4Gow98k6vko4iRwC3C4u19AnBAdBhxuZs/K7/6JOElZkuU3KNP3CPCYmX0R2MfMbsh9tHfef7eeuN/uESL4PzPTd1Lu171y3VuoRrQ7qU6I9868Hky0m7X5eXq+/26m40J3PznLZBCwxd3LyVkXcdn1de7+YSIYmUO0kbuJPmc4Uc+Pz7J6DnGyv4IIzrYQI++3Ev3Of2T6fpLb6yCCodfnPtqL6NePJNrsoLz/dzkRcB2YZXw24Gb2nSzrzUS/PZkYpf5BbvuLub0jqUbMy0nrlNwnczP/64k2vg/hO+RVHXd/FxG4rSHqcSfRZywjbvvYTJxU35jvHyWOs9/JPOLuW6n6xJ7t6hGyVoywNXweQRywTs/K8SfiwFAuLdYvKy3KHfj9LNByRj2J6qygvL+e6BRelzvy9KxYjxKjQpOJM8JHidGYhbnzbqEadSmXwdZmWpYT94hdTtXBlxG8jqw4X810ziGGnL9Gdbl1OXHNfENWmk35/sBM+0NZsdbk+pcRHd2/EJ2SUY1mbCYObh8ggooJmc4TiE5lMRG4lMskszI/pay3Uh1k1hIHrtuJSrsxt/Xm2n5aTjSEsp0y+teZ6yyX+jZn+X+ttr5HqALIu4mDVLkMdXuW/08z37/N9Zey+GOW6wzijH91/i0nDggriVGIR4iD5SbissT7iSCmpKuD6FzeT3WZr4u4x2IucaP0oFq5lDPTxbUyepgIbL+c3y2jGqX7FdFp/DJfx1Ndvnss51md+3lpfrci5/8Z1QjK1WQbIervE/sh3y/I8v6/mcbnZxm+iagXV9XStDLT/gPgw7V9uImoty8m6t7/1vJZgrdFRFCxjBiZLKOYHbmNOVSjmdcRnefGnG8F1Vn+0tx+KfP6CORK4v6qJVkuHVQjSCuI/f5HqjPuK4j7gcpl89W5vhfl/htHHAxeQtSJcqm89CPXEnXu8pz3S0Q9W59lVy7BbaW6b/Ux4gRjVub57kz31tq++yNx0vUr4kSinCScluW5NLezJee/h9jXNxMnrWOpRkZXUt2yUS4TryVO5u7I9f1vlv3fE8FqGXWYTowSDcvvxuV3G2rteBxRZy4B/pDTynY6iQBmBlH39sr1Ts0yqpfrBmB5Q1/+lkxbGQ0aQ1xOu7o2bR1xQN6nNn8JsjYSI5BjifpXX9+0LMd5WYYPEfXxfzOPnyL681cTff+FRJ0qdWkqEaT9ppbeDVSX0jqAI2ojKJOJuvlgpq2kZSFRL28m+p9HiHpS6nu5LL4p0/Bhog+8gug3HiQC169n2dyaebkS+D9Z9icSx7dVmedSNuUKwqXEifUmok5+KMttFVE/v5DpuZQ4Xs3JvD9CFRDOz+8PqqXrZ0Tg/W3ihGE68OUsk98AJ+f7Mjp+Xi6znBh9HE/Vx00l2u3dxDG1M/fhyszDVcS9Y/dmvsr7lUS/9Cqibfwu0/1fVIMw9xD7+/Is8yXEMfY84tabr2d53Jnldi9Rn+4hRohvzzR8O9c5gahXC6mOzaWuPlor13Lf92DgU32Jdwb8r0TN7P3u/ouGac8kAp0TiAPoRUTnUIZBNxMN6DwisHgDVUUeRgQuexOVfTrRuY7JdSzI9T5E3CT9WmLnDCIOSOuInfgG4rJGCerKyMhLc90riLPrEgCtIyriTKJynUs0jH1y/m+4+8OZvxcRgdoLiYa0mLjev5Y4G3k9cda3T25jDdGxHkD8gunuWlmdnPl+M1Hp30OcdQzK1w/V8nRspmUJ8Qupt2X+v0ac1f8tETBfn/nanxjB+Zy7lxGA+n7anzhwHEecsc3PsnprlrURIyG353zPyvX9BTF0fy5x8/Y/EJeWn0M0hgnkjblE4PFsokN6H/DJ3BeTiMDqacTZZll2X+AzOW0JsR//IsuzjOgdkN//kqgvq4kg4hPEmdOz3P2x/AXqCCIAIdexhDjw/gEY6u6/MrODiZGo3+d8g3K7RxGXKP+eCKIPIDrkZxOjjauIgx1Z1jcTHfUJRAB2G1H3R7j7L7ppK39B1Ll/IkZcPkg1EjacOKh8MstuDBGUHkbUgxcR9fL2XM8kd99UW/cJxH58R5bRa7Ocn02MFj1KHEw7iBHpYUT7KgHJDUQdfBfViN39xGXYUuYfJQ5Gw4g2cDCxzw/M/ehEJzs/y/UZuc0vEiNHL8ty3UB0wOVy4YeBu939GjP7fK7/NVnG3yP6l2VEO55PtKtVZjaIaA9Dcl13EQfMo7P8DiJO+u7IvPxNbn8GcS/Tq4iTgLm57wZnHlcRfdbtxEnV3xD7+T3EKMtc4CJ3X59lfzNxcP2Cu38wH0HxLCI4GJVpHkwcWK/JdZ1Z2+6+Od+ILHeIuruaGG15Q27nZCIY/Hfiss5qMzuMOFC/k6hbl+Z+HZbb2R+40t1vMrNP5/69EXiTu5+e6z2B6pERz8i0vJdoN1/2fGSEmX0LuNndb800ll///g/Rzq4k+vARRB/4G3e/18xeRlwenUa0odK3zScO7C8nRocmu/vN+SiQI4iD9RFEW74v9+2Bmb/Dcl++lmqEfRNRz9+f6x9CjMSsJgKH5xLBWxdRt9YQ/dlNxGjle919XnlURe7vaz0fWZGPEzmf6uSpPshwJPCoV4/rOIfom48jgtwLiID0HuBrue9ektu9F1jv1SNL/iPTe2Lm4bdEIHQPEXy9KffXmEzXS7PsZxD95lQimD2abh59Uh7vkVfAjifq+wc8H/lhZlcQwdM7iP5iMnFy8yrgBe7+PTP7e6K9XEQEZyOJuvUGoo1/iDhW/ZnoK6529yty/S8ijifX5z7di2gjLyLqzvOIkcw7ifqxIPO9P1HHnpNlsoroizqJfmwsMTr4D8Qx8vVEPb4kl3mYPj4GZcAHbD2p/Vy8gyjYacRQ/yDioHAw0XCmEwXqROGXM/NnEDv7tcT9NJPcfXmu911Uv0Ysl1IPIBrNxvy7kgg+JhDB1RuJA+AriAqwkmiAH3f3OzPNzX7y/RzirGUk0Qj3ISpFJ3GQX5zruZU4QP6B6KwmNNtGL2V1dC5T8nQgceBZQFTk87328/ZW6O4n0mb2O6LBLu9pvsZl3P3UvnzfuL7elt3RfGxP+rqbr4/rfg4RLJxCdDQQB9kn7bfyE/O+PmKgm/nKiNQsIkC+hnjsxAH5/eO5/euIE6eZxIH3JJr87L88jqGnPDZJ/ygiuDqEaMufJjrWjxPB2Fji4LKMCPAfze9/S5wkfJm45H8A1a0J64m2P5U4cJ5HtLVZRDCLx6WQ7tJ2DxH4vIU4CD+TONieRO+PSvgd8Fx3Pz7351XEQXIr0SZXEiPKT+zPXGYzcZArBlP1cTMzz7N58iMo7iSCs5E5T7nsWkYFFxP156Is0w8QwcDEkmSivm0E7nL3v6494mEYEcQ8l+ib9if206HEqNwyIohZmOk8lDjIQvSXr/LqkRHriQPeVmJffAX4lecjcszsfnc/Pt/fSxw4N+X2tlDdkrCZuER5PBFMn0zs4xFZ5i8kTgRKme9Xy2e5tL2R6GNflnkbQ9SXN+T03xC3fhyR0y/MMj+MuPWjjJyVKynPpLpNwnKbTnWZdG3ul9VZTuuI4OkKYhR2JlU7LJdq9yNODjpz/r2zLOZnOa8hAvh7iePRI0QbPpfqEuS8XP8JxAniPcRJ4N/musltv5g4cRlMtPcSpL0907UX0dbq9W5wzkeu+27ismNpe5uoLp1vyfUfWCuXdcQo1/zMxyuIkd03Em3vGKI/ejUR0M3N5SDa5BuJY10XUf8gTsxHZ1kdSwyYHJGfr8ttjyROBq4hTtwOIPqfbfq0fFzKcGL/vpQYVR9OnPh+hTj+jyROQpr2h03t6KXIdvkjOoF/JTrXTWz7k+guYuhzCXHWsYSomPcQHXO5aXYNcSlgU65vLXH2MZY4Az2d6ifAZRh1OdteKjqWaMCLcyetr803nzizLcHjfKIS3k6MUq0hGuMiqiHwcu/LZqoGvjLT+M3c3jfz8yXEEO5WomJenZXoTmJo+eFM9/RMx9zM/yriEsXCzPO7qILVcl/EsMzTSuJs4W5ihOQbRGO9hejM30eMOqzI7aymuidsJjGaMDvzfg0xQvBHolNYRDXS8ghxpn821a84VxOd9YJc5maicd+b6V6S+ejMMniYGMGYkfO+nTh4nkgcJN5CnB0vJ0YW35KvpT6U0bkjqH419qecdjgxVF/uh1lO9eulcnlhWe7DKcTI1VjiMs1Mqkc9rMm8lHysy7K7mRgNWJvLXEc0+HlUj6pYSBy0v0ecEZabbus3/T5GnHkfm2VxJ9HJvjXzOZ/q3o03Ud1oW9K3jKgPb810dhJnlWW+KUSH9tp8fx3Vpb2ziE50I9ExlzQ/Umu3E4l2OyW3t5TY11/K/beMqCf/SRX0fZ+ok6UNbsn0zyX29RCiI3wst7uBOBhOyn32X1T9QrkMdwdxIldG39YSdXs9cYCdknk6iKizv6D6te3HiUuR03O/vZtoi57rKembSoyUfiCnr6W6xLMpl/8/RN0ptwAsJdrncqJf2T/T1UX1S83Sp5VL0vMzHZ/JdXya6kcxJ2aZdFA9wmc91Y3+y3J75YTyS0Tbfj4RZC8m6tPsTMNdRBv6Yqb5MeIk8SeZvl8Qz7ODOHFcSoxydxJBTunX7iROKN+S096cf1NrdaXcrF8uq87K7SwlTljPAh6qzb+B6pf0m7OcHiXaTDk+/CMRmJR9PDH38UaiL3k9Ecj9S5bZV6iugDxO9FNDMg+fJg7qm4l7xaZQ9d/lBP7Zma6j8/OanG8zUUfLSf4sqsuYTgwYrKP6BeuGLPvVRH/YmX+fIEb8u6jqw+ep2spb87WUcwdwSpbXkJxvAVHPNhIDGONzXc/O16dT3fYyiOiT1lD9SOyXmf7puX+6Mh9vz/L4NyIQf3qu4+eZnhm1dK0m+swZVD+AmJ9lvJloq+/ONI/PaYOIOraFKuh/ceZpUeax3F7xjExz2R/PIOr0cqI+bc3PL6S6x3QTVfBV2v6bqX4EUY6HK6geadOZZbmcOIaOy/SNy/yMp3Y7FzChT/HOrg64WhCwXU8VnNxABDF/IDqT9cSBooO4nDGX6CxLRSwd5yTi4LGVGKHaQDSmB4iD5BLiYPQg0WC3EJe9oArAbiEa+KJc535Zuabl92szjbNyvXOofm3aldtck9NWZKW4gzh720h0mrfkuv8p399K1SEtyXTdmpVodW5nfG57Q+Z5ZW7jbqqD74Jcttx8XC4F30MEskuJDnsm1a9Fb810Tcq/jUQnvzbzNjPL7rNZJguoHh/SQXUPoVMdNLbW0u5Ugeqa2nzl/aZc/1KqX5qWMijzLc5yXFnbbnndSnW5bBPVo15mEsH6HUTH3JHrmJx5eIzqMS8lKO+gujG/rLcccLuIQLfc91SC0Hm1tEymCuw3NZRJudRR8tyZ6VhDdX/d2tzexqyTxxL17kIiGJyR+2Rrrew7qH6N1ZVltYSow6VcynyLavMtz+mP5GsJtMcSndFWom6syjR+NZcr90luqO2neVS/kF1E9UzEWcQ9iEup7vX6JnEQGksVEF/PtvcIvpQ4CJeAZyvRjkteSl/wKNWjM+ZlOR6WZedUwW8JIp2oC7OJvmUu1a+fSz0odarsvxlUwdedRMDZSXWytozqF+WlfpZAbBFxCeqxnO/rVP3cJmL0ZhlV4L+E6tdsS6h+4faa3N4j+f5OqlswJlM9ZmYsEQTMJh8xQPV8sen5tzVfZ1A9WmUzUbc7iJGYu4hLozcTB+N1xMnuVKpfdm+harelr7mDuDWhs9av/wr4SL7/KXHAHE/U7bE5fR+ibqyiulm8BNiTqep6CSielmmZRQQXZV93ULXz0iZnU/1as7SracTJxP5Z9t/JMrqJOGl1InA6gurZnFOJuvJdql82biT6uYmZvkOIPnRTrndqpnVrLZ+zqH79/8tc9wP5t5XqIcDTieCm3Ju4jKqtrSVOfDtz/uG1/uKBXP47tXTuT9WWyjo2Uv26vSPLYywRSH+W6pfoF1D9ivmzuc0LiLZzf85zHnEcm0aMns3J728hRsY+m3kr33cC4zJtryFGybYQdW0a1bP2FtZOChdRPVqj1KONRJsofUcJlqdkmpfXyu0gon0+QvXL2O9T3f+8ieoe6FLfSr8wsbaNsu5bqZ5kMK1W/mP3lIDtodKY8rWMiu1FdEydVMHKnVQ/p99KBD4rievZG7PQS6fbQZyp/iGX/0NtG2uIhnV57pyrqR5P8AfirGwh1Y3PH6N6ntHCXP9Hau/H1dZd/0lymbaeOJOdSTTGiVlJNmdFmJDbmpnfjc3KcS7RyZUh/GnE8HMX0QjvAObmNkpZrcvtXU0VjP0h1/ul3MZG4gzKcx3XZrk9N/MzIf/KmXHpoKYRN7BOr82/kepRK48QZ6oPEp3nRqqz9EmZ3x9SnSGeTbX/SyfTkfk4lm1/0j2ztn/vzXV35DJbcp4SxJdAdkvOV/KxMKc9N8v791Q/kS+XkeYSjXEo1a+hnpv7oZPo9B6s7YctDfvhQaqb++sjFKXzfrCWlnJf1yaqh3t+kTgAlLyXM+wVWQZl9LSsx2vbXp+vj2felhH14blUAeMdOe3eWtpLQHQ7VeBR/hbntFInS1DRRbS7UVleDxGB6AaqYLDsj/VE2y2XtjbW0rqOaEtlRKMExjfn9uZRPS/pL3IbE/LvZqI+dVE9muG3RD29n7iloQRim4mD+FriEstiqgeZvpnqTHxIpnF2bn8+MVKwKtdzXub3YWJf19vIX1LVu3Lpr4yQPUR1u8aXqW72fnXu16Pz9Uaqx3CUE4A1RDC1rra/HiLq5BqiXW/M8vhSLZ9lH8+henzRPZmGP1OdgHQR9+KsIAKLcuJSRns7Mh31IHcVcaP33VTPJFtV6/ueQ1x6mkWcfJc2fxf5kNpav3Vc5nVu/q0k6nBXvpZR+MlZ/nvlsqOIA+jDVA/a3ZppeXmWx78Q9e97Oe8f8/1txCXJMiKzLvN1GzFSWkasyno7swxeQXWvbicRNJf2U04IS3l01fJZ2uuEfH16fVrm++lEGzyO6gRjLnEcK+1wE1U7KnX6LqJOl/rvwDNq5fvbXOY6qqtHZaT6if2R+2xSlv8Pct6TqfrsO4g++xTi+LOI6ikIq3N9s4k6/Svi0nU5iVtN9YzP52d67mLbE/+VRD+zMdMyngi4ZlL1QSXd43L6y4l6MY/qGZubczuLcxsrcpkVVCelpY5vpnoAbsnX/+R3f0vUi1OJfmFhLS3lZPmJ8t9TArY/E5d0biYi6DuoDuIzsyAnEpd3Tic62MOIM6KTiWvQpxMPYD2duFl7ITF8ezMxlLyUaKT3E9H/r4hgr1zOu7NWiculxx8TZ8Tfyc8P5DZvIM4qf5Y7aj5xuW0O1WXNctC9kxhhu7+2jVVUo0LLs0JOJf/rAHH55HSqiryCaBCW+flipvulWcGW5XYnEh3Gb4j7O35B1ZGsyPffzO0tIALFzlzHT/P7ckl3eaZpPtVDcpcDt2Yaj6DqNErHMZK4uX0sccD8apb1WuIg8cvM89uzTJcRneOficB0A3HJ+UGqp1J3EIHTpNp2H8n0zs90fY7Y33MzDSXN5XLDvIZpm6mekP7h/LyAuIzpOf2buf03U/3yqowofCPz8/ksr8eJ+85WUD2k9t+JIH8Dsd9Lh7QXVd2eSPXLo44sqxVEkFFOSDYSdeYWoiN/K9UDhJcRZ+ydOd9XMo2/yjL6DtXo5FyqUaaRRP36QK1cy+XaocDIJm20k7h/5a8zfW/K/fb53BfLcpvjMy/Tcn0XZT4/SdTdZcQBfANRn+7N8rmAaE+DiPo4PbfxRuKSxzFUdf83RB05hgjq78wyWkn1iIofEgenHxKXdBYRZ+6H1Mr/J8QJ2+nAi2snVm8i2s5ZuX/uIS75lZHJy3K5xbmvxzcsO5LqBvkribrUQXVLRhntHp9/V+f++Rrws1zXO4l69w/EaNl+DX1lGdl8IMvoGqpRmvpIcBnFvYQYHSu/6vtFbb+vzv3xeap/HzUz8/Ew0dceRgSIB+Q+/CXViOYBRD37LE0eHprre1WW/+ubfP8T4scKjdMPI04kX5l/ZwDHN5nvFcQIzHdyW3OofgFZyqMEWyuJvvsH5BPus/xKHicR9e904gcfN9W2M4246nJwbvMFwAn53euI/u74LI9PZnlcXFv+2Pprvi918Viq/v8IIqCZVNJVK+d/zvVeQNyK83byPyY0LPuWxvLNfB5MtOPhuT9+01CWJW/jiTr4zdp346mOFWW+QbVp72zc/8T9XRfV6vT3gefn58m5jhfX9kNJ/7yyjdq6TqOqR6UPOrbJfL+o7c9PEv3D94gg/CLi5PASom5dS/TT06lOAkp5PUB1da2kubSbt2VaPlov/z0lYHslEdCszsKZRfWg3EVE4zsw530RcE1t2ZcQ98Ts37DOU/P1LVSPKJjNtj+9XkF08J25M0dmhflmw/dTsxKUA8UkqkdL/D7T8F9Uv04rozplaLWcWZbRhkdKnojLuCOJ+5hOrqX/Rbn+nxEHiknEPR0HUj0SYRXRIT9KdTPyS4gD1SqiY/oUMZR/MvF/9iAa2LeIs9PlpeyofpJdzkLKZdiNVJdtX5TzPp84ayuXl1dk+azK9NyaZfUoEQz9gDg4L6P6afapuUy5r6Oc9X8st/H6nO+vG/b5KcSvpeZlmZ+X+/hCImg/j7hvagJx9v/Nhmk/rKX7PqLRfpLqDPAjuT/XEJ3/P1D9p4Abs3zX5LbvqeVjE1FX3lBL53nE5dcySvY3xD5/AXBFzvdFqn+/soUqAHucqJcH1ub7X6rRyym1cvsQ1ahnZ5bj2cRo5yepnvx/SuZtFVE/PkmM+owkAxfyvpiG9vTHHtrtqkzPhZnfP+Xr6eTjC3L+v899NoPqUvT83MdnA3v30k8cSNU2O6gezVDabmMfcArVowEW1ab/a5N5X0L8uvY2tn0cwseIjvl2Inj5PtXo7nJipOwaoj2MzmXX5bLH1urxD4gDZVn3pHz/YK53SmMZUAskm5T5euIAOpHqV9U/z31Ztrc/VfDx4ny/TX+Z5Vba/XlUj0HYZt/Vy42GgJ6GPnkXHkee2NeZz5MzvfX3p9Tmb1bP63W6vg8H08fHNmxnmp9UF9uhTMljRW/T+rmNJ9Xv2ncjW7idt/DkR8s0trVtjr/1OkL8CKt1+d5VO3UnVZyP1F8bpn2SiICvIw6sI2rzjO/LevP9lbme8T2tp75Md9OIIe1/6cs2avlomtZmeW+yvuuIA/026yMCtT7nqZvy7eu0q4jArGlaavOfTfW/MXssjybpv7C2nnlUT75/vIftdZvmZt837Lsey7ev+6GbvPdUln1dX7M09zVvH+mlfJ9UH2lSB/vaLrajHjVdppd2/KR63lhemfeX95DWso4nyry39t+X7XZXz/vaDnvJd3dpWU60ibK9WU3azXU9pXl79u327q+n+i/39TealMd1Tcqjx2PErs5ju5Tprtj/OyPvO9LWWrLdXb0Dn+JCnVt/bZg2iepMcQgxCvLp/PxgX9ab78uvQeb2tJ76Mn2Z1odtrOwprc3W07i+fP944/pK2fQ1T92Ub1+n9ZiW7SjzntLfUUv/xr6UZU9pbmGediTvrS7L7d6HvZTvk+ojTepgX9tFf9La21+TfPTYrrpJa70fKWXebVvZnu2ynX3LduS7u7SUy9IrqX45uE27yfe9llVf9u327q+d8ddNeXy6SXk0zXNfynwn5aNtynRn7/+dkfcdaWut+Bvwz2Ezs4n5dmht8n7la6rnuUzOz8e6+35m9rC7D6utZ3/i8sTDxL0+e9XWV9bduF6IX/NMpvpl3l5UNyDv35CmyQ3re1ptPfV1e+21WT4GEfegLW/YRt3Tast09JL+jvxuWa7PiEtP9Ty9lOo+KmrraZbW7Z3WLC1butlWY3mU+/Mal+loKAOneoZPb8v2luZW5mlH8t7qstyRfVjWPYWoJ+U+n/2pnq9UlnsZVd0v6m2qozZ9v26221O6mrbx+sZq/USzbdfb1TCqdtCYj9Ie6o4lLieXfD5M9+2/o+F9d3WwzNesb6m3w/r6ptTz3ZDf7vLdmJZO4paSlxL37ZxM3Ou3ifjhSX0fNvZB9bKqb6exz6tPg272187QTRkdS9X3zSLuKyvHhZOJWzQeBt7q7sf1YX3wFOVxZ2+vr/pQ9/q9/3dG3vu6jZ26H3Z1xN2CSLf8SmgpcT/QG4j7U87KaW/MeY4mzpIW5HK3A8c1rGtv4pc/W2rrPbq27vp635Dr7iRuaFxQW+YFxOMwyk+iT6ul4ejatC1N0ry1to0yrTNft8kH1b1u9byXdG2pbbcx/WV9jfOVNP+pSZ6WEfeWbaHntG7vtO7SsqlhW92VR70sS/rG5ff19ZX0ey/LNm5veTdpaEWediTvrS7LHdmHpXwXU9XF+q/Jjq79DclpxzVMr7eLxra7PWXebRvvpp+ot+d6Pupp7S4fCxrWczTRVk5l23x21/6bbbdZHeypb1nWZL4n9kM3+e2pzOtpqbf7ej9Yfjl4apPyKH1Qd9vpaVq3+2snHzvq6amXQfnFZf248MT7Pq7vKcvjzt5eP9PV0v2/M/Le123szP2wNwPfDcQZ3mhgrbv/2cyuJc6ObnH3P5nZbe7+GICZ3ZnLfYj4JdQT3L0L+JCZ/TfxC4793X2CmY0mbuyur/fPub4biWHzO6kuG0wA/sbM3pjrWQPU0zCa6qb0xjQ/WrZB9SDFG4mb7d/RkI+NRIc7vKyn5CX/ifga4kbojQ3pv5H4BdOi+nzu/v5M82NZNk/kiXgO1E+JG4s/2kNat3da07Q05K3b8qB6COUc4pdW59XS37i+n1L9gKG7ZUveyvaOIi6PPBV52pG8t7osd2QffjHL9x3uPqdWF39KPIH/MWoyLaVdlGmlDTxRP2vtYHvKvKc2XncDT27PI2v5KMvOIX6c0CwfJZ9P5MXM/o54/MKihnw+qf1TtcMntkuTOthT30KtHdbmu622H56U357KvCEtZb/e6fEvgko/+FPi128PuvuiJuUxrl5W9e006fNu66Zcd4VmZfR3VH3fF4l9+8RxoeEY0ev6auu9s+Wp3/nb66se616L9v/OyHtft7HT9sOAvyQqIiIisrvbq/dZRERERGRXUsAmIiIi0uYUsImIiIi0OQVsIiIiIm1OAZuIiIhIm/t/cRKUmRkVOGoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR, SVC\n",
    "import xgboost\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBRegressor\n",
    "from copy import deepcopy\n",
    "from scipy.stats import pearsonr\n",
    "from utils import read_sentiment_scores, read_library_scores\n",
    "from math import sqrt\n",
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\") # %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import statistics\n",
    "import random\n",
    "random.seed(9)\n",
    "\n",
    "labels = read_sentiment_scores(sentiment_dir, canonization_labels_dir, lang)\n",
    "library_scores = read_library_scores(sentiment_dir, canonization_labels_dir, lang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir + lang + '_' + param_dict + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de034c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(547, 2)\n",
      "(547, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_name</th>\n",
       "      <th>c</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexis_Willibald_Der-falsche-Woldemar_1842</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexis_Willibald_Ruhe-ist-die-erste-Buergerpfl...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexis_Willibald_Der-Roland-von-Berlin_1840</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexis_Willibald_Cabanis_1830</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexis_Willibald_Schloss-Avalon_1826</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>Ziegler_Heinrich_Asiatische-Bansie_1689</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>Zschokke_Johann_Das-Goldmacherdorf_1817</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>Zschokke_Johann_Addrich-im-Moos_1825</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>Zschokke_Johann_Der-Freihof-von-Aarau_1823</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>Zschokke_Johann_Die-Rose-von-Disentis_1844</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>547 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             book_name  c  y\n",
       "0           Alexis_Willibald_Der-falsche-Woldemar_1842  1  0\n",
       "1    Alexis_Willibald_Ruhe-ist-die-erste-Buergerpfl...  1  0\n",
       "2          Alexis_Willibald_Der-Roland-von-Berlin_1840  1  0\n",
       "3                        Alexis_Willibald_Cabanis_1830  1  0\n",
       "4                 Alexis_Willibald_Schloss-Avalon_1826  1  0\n",
       "..                                                 ... .. ..\n",
       "542            Ziegler_Heinrich_Asiatische-Bansie_1689  1  0\n",
       "543            Zschokke_Johann_Das-Goldmacherdorf_1817  1  0\n",
       "544               Zschokke_Johann_Addrich-im-Moos_1825  1  0\n",
       "545         Zschokke_Johann_Der-Freihof-von-Aarau_1823  1  0\n",
       "546         Zschokke_Johann_Die-Rose-von-Disentis_1844  1  0\n",
       "\n",
       "[547 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(library_scores.shape)\n",
    "library_scores = library_scores.rename(columns={\"y\":\"c\"}).drop_duplicates(\"book_name\")\n",
    "print(library_scores.shape)\n",
    "library_scores[\"c\"] = library_scores[\"c\"].apply(lambda x: 1 if x!=0 else 0)\n",
    "library_scores[\"y\"] = 0\n",
    "library_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2eda57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_name</th>\n",
       "      <th>y</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexis_Willibald_Der-falsche-Woldemar_1842</td>\n",
       "      <td>0.025530</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexis_Willibald_Ruhe-ist-die-erste-Buergerpfl...</td>\n",
       "      <td>0.023794</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexis_Willibald_Cabanis_1830</td>\n",
       "      <td>0.023481</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexis_Willibald_Schloss-Avalon_1826</td>\n",
       "      <td>0.027773</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexis_Willibald_Walladmor_1824</td>\n",
       "      <td>0.046649</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Eichendorff_Joseph_Viel-Laermen-um-Nichts_1832</td>\n",
       "      <td>0.022402</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Fontane_Theodor_Effi-Briest_1895</td>\n",
       "      <td>-0.015735</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Fontane_Theodor_Die-Poppenpuhls_1896</td>\n",
       "      <td>-0.003428</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Moerike_Eduard_Maler-Bolten_1836</td>\n",
       "      <td>0.016478</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Siegfried_Walter_Tino-Moralt_1890</td>\n",
       "      <td>0.018287</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             book_name         y  c\n",
       "0           Alexis_Willibald_Der-falsche-Woldemar_1842  0.025530  3\n",
       "1    Alexis_Willibald_Ruhe-ist-die-erste-Buergerpfl...  0.023794  3\n",
       "2                        Alexis_Willibald_Cabanis_1830  0.023481  3\n",
       "3                 Alexis_Willibald_Schloss-Avalon_1826  0.027773  3\n",
       "4                      Alexis_Willibald_Walladmor_1824  0.046649  3\n",
       "..                                                 ...       ... ..\n",
       "133     Eichendorff_Joseph_Viel-Laermen-um-Nichts_1832  0.022402  2\n",
       "142                   Fontane_Theodor_Effi-Briest_1895 -0.015735  2\n",
       "145               Fontane_Theodor_Die-Poppenpuhls_1896 -0.003428  2\n",
       "178                   Moerike_Eduard_Maler-Bolten_1836  0.016478  2\n",
       "198                  Siegfried_Walter_Tino-Moralt_1890  0.018287  2\n",
       "\n",
       "[173 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a2338fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels[\"c\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "837d64ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 in labels[\"c\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f48ece3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Labels statistics\n",
    "print(len(pd.unique(labels[\"book_name\"]))) #197\n",
    "# 254 labels, 197 different book_names -> 57 second/third... reviews\n",
    "# 36 book_names with more than 1 label, these 36 book_names have 93 labels\n",
    "# 93 = 36 first reviews + 57 second/third... reviews\n",
    "# 6 texts have opposing reviews (13 reviews are opposing)\n",
    "# 191 texts after aggregating (without opposing reviews)\n",
    "\n",
    "labels[\"y\"].plot.hist(grid=True, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e960d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression(object):\n",
    "    def __init__(self, language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose):\n",
    "        assert isinstance(drop_columns_including, list)\n",
    "        for i in drop_columns_including:\n",
    "            assert isinstance(i, str)\n",
    "        assert (dimensionality_reduction in [\"k_best_f_reg_0_10\", \"k_best_mutual_info_0_10\", \"ss_pca_0_95\"]) or (dimensionality_reduction is None)\n",
    "        self._check_class_specific_assertions()\n",
    "        \n",
    "        self.language = language\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.labels = self._prepare_labels()\n",
    "        self.drop_columns_including = drop_columns_including\n",
    "        self.dimensionality_reduction = dimensionality_reduction\n",
    "        self.model_param = model_param\n",
    "        self.model = model\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if self.features == \"book\":\n",
    "            self.df = deepcopy(book_df)\n",
    "        elif self.features == \"chunk\":\n",
    "            self.df = deepcopy(chunk_df)\n",
    "        elif self.features == \"chunk_and_copied_book\":\n",
    "            self.df = deepcopy(chunk_and_copied_book_df)\n",
    "        elif self.features == \"book_and_averaged_chunk\":\n",
    "            self.df = deepcopy(book_and_averaged_chunk_df)\n",
    "\n",
    "        columns_before_drop = set(self.df.columns)\n",
    "        if self.drop_columns_including:\n",
    "            self.df = self.df[[column for column in self.df.columns if not self._drop_column(column)]].reset_index(drop=True)\n",
    "        columns_after_drop = set(self.df.columns)\n",
    "        if self.verbose:\n",
    "            print(f\"Dropped {len(columns_before_drop - columns_after_drop)} columns.\")\n",
    "            \n",
    "    def _check_class_specific_assertions(self):\n",
    "        assert model in [\"xgboost\", \"svr\", \"lasso\"]\n",
    "        assert features in [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "    \n",
    "    def _prepare_labels(self):\n",
    "        return self.labels.drop(columns=\"c\")\n",
    "\n",
    "    def _drop_column(self, column):\n",
    "        for string in self.drop_columns_including:\n",
    "            if string in column:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def _custom_pca(self, train_X):\n",
    "        for i in range(5, train_X.shape[1], int((train_X.shape[1] - 5) / 10)):\n",
    "            pca = PCA(n_components=i)\n",
    "            new_train_X = pca.fit_transform(train_X)\n",
    "            if pca.explained_variance_ratio_.sum() >= 0.95:\n",
    "                break\n",
    "        return new_train_X, pca\n",
    "\n",
    "    def _select_features(self, train_X, train_y, validation_X):\n",
    "        if self.dimensionality_reduction == \"ss_pca_0_95\":\n",
    "            ss = StandardScaler()\n",
    "            train_X = ss.fit_transform(train_X)\n",
    "            validation_X = ss.transform(validation_X)\n",
    "            train_X, pca = self._custom_pca(train_X)\n",
    "            validation_X = pca.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_f_reg_0_10\":\n",
    "            k_best = SelectKBest(f_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction == \"k_best_mutual_info_0_10\":\n",
    "            k_best = SelectKBest(mutual_info_regression, k=np.minimum(int(0.10 * train_X.shape[0]), train_X.shape[1]))\n",
    "            train_X = k_best.fit_transform(train_X, train_y)\n",
    "            validation_X = k_best.transform(validation_X)\n",
    "        elif self.dimensionality_reduction is None:\n",
    "            pass\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _impute(self, train_X, validation_X):\n",
    "        imputer = KNNImputer()\n",
    "        train_X = imputer.fit_transform(train_X)\n",
    "        validation_X = imputer.transform(validation_X)\n",
    "        return train_X, validation_X\n",
    "    \n",
    "    def _get_model(self, model_param, train_X=None, train_y=None, train_book_names=None, task_type=None):\n",
    "        if self.model == \"xgboost\":\n",
    "            if task_type == \"binary_classification\":\n",
    "                is_classification = True\n",
    "                class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1], y=train_y.astype(int).tolist())))\n",
    "            elif task_type == \"multiclass_classification\":\n",
    "                is_classification = True\n",
    "                class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1, 2, 3], y=train_y.astype(int).tolist())))\n",
    "            elif task_type == \"regression\":\n",
    "                is_classification = False\n",
    "            else:\n",
    "                raise Exception(\"Not a valid task_type\")\n",
    "            \n",
    "            def feval(preds, train_data):\n",
    "                labels = train_data.get_label()\n",
    "                if is_classification:\n",
    "                    labels = labels.astype(int)\n",
    "                    preds = preds.argmax(axis=1).astype(int)\n",
    "                    if task_type == \"binary_classification\":\n",
    "                        return 'acc', accuracy_score(labels, preds)\n",
    "                    elif task_type == \"multiclass_classification\":\n",
    "                        return 'f1', f1_score(labels, preds, average='macro')\n",
    "                else:\n",
    "                    return 'rmse', np.sqrt(mean_squared_error(labels, preds))\n",
    "            \n",
    "            if is_classification:\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y.astype(int), weight=[class_weights[int(i)] for i in train_y])\n",
    "            else:\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "            results = []\n",
    "            df = np.hstack((train_book_names, train_X))\n",
    "            df = pd.DataFrame(df, columns=[\"book_name\"] + [f\"col_{i}\" for i in range(train_X.shape[1])])\n",
    "            for max_depth in [2, 4, 6, 8]:\n",
    "                for learning_rate in [None, 0.01, 0.033, 0.1]:\n",
    "                    for colsample_bytree in [0.33, 0.60, 0.75]:\n",
    "                        if task_type == \"multiclass_classification\":\n",
    "                            params = {\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"n_jobs\": -1, \"objective\": \"multi:softmax\", \"num_class\": 4, \"eval_metric\": \"mlogloss\"}\n",
    "                        elif task_type == \"binary_classification\":\n",
    "                            params = {\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"n_jobs\": -1, \"objective\": \"multi:softmax\", \"num_class\": 2, \"eval_metric\": \"mlogloss\"}\n",
    "                        elif task_type == \"regression\":\n",
    "                            params = {\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"n_jobs\": -1}\n",
    "                        else:\n",
    "                            raise Exception(\"Not a valid task_type\")\n",
    "                        cv_results = xgboost.cv(\n",
    "                                        params,\n",
    "                                        dtrain,\n",
    "                                        num_boost_round=99999,\n",
    "                                        seed=42,\n",
    "                                        nfold=5,\n",
    "                                        folds=self._split_booknames(df, 5, return_indices=True),\n",
    "                                        feval=feval,\n",
    "                                        maximize=is_classification, # if classification, maximize f1/acc score.\n",
    "                                        early_stopping_rounds=10,\n",
    "                                        verbose_eval=False)\n",
    "\n",
    "                        if task_type == \"binary_classification\":\n",
    "                            nested_cv_score = cv_results.iloc[len(cv_results)-1][\"test-acc-mean\"]\n",
    "                        elif task_type == \"multiclass_classification\":\n",
    "                            nested_cv_score = cv_results.iloc[len(cv_results)-1][\"test-f1-mean\"]\n",
    "                        elif task_type == \"regression\":\n",
    "                            nested_cv_score = cv_results.iloc[len(cv_results)-1][\"test-rmse-mean\"]\n",
    "                        else:\n",
    "                            raise Exception(\"Not a valid task_type\")\n",
    "                        num_boost_round = len(cv_results)\n",
    "                        if task_type == \"multiclass_classification\":\n",
    "                            results.append({\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"num_boost_round\": num_boost_round, \"nested_cv_score\": nested_cv_score, \"objective\": \"multi:softmax\", \"num_class\": 4, \"eval_metric\": \"mlogloss\"})\n",
    "                        elif task_type == \"binary_classification\":\n",
    "                            results.append({\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"num_boost_round\": num_boost_round, \"nested_cv_score\": nested_cv_score, \"objective\": \"multi:softmax\", \"num_class\": 2, \"eval_metric\": \"mlogloss\"})\n",
    "                        elif task_type == \"regression\":\n",
    "                            results.append({\"max_depth\": max_depth, \"learning_rate\": learning_rate, \"colsample_bytree\": colsample_bytree, \"num_boost_round\": num_boost_round, \"nested_cv_score\": nested_cv_score})\n",
    "                        else:\n",
    "                            raise Exception(\"Not a valid task_type\")\n",
    "            best_parameters = sorted(results, key=lambda x: x[\"nested_cv_score\"], reverse=is_classification)[0]\n",
    "            return best_parameters\n",
    "        elif self.model == \"svr\":\n",
    "            return SVR(C=model_param)\n",
    "        elif self.model == \"lasso\":\n",
    "            return Lasso(alpha=model_param)\n",
    "        elif self.model == \"svc\":\n",
    "            print(\"SVC with balanced class weight\")\n",
    "            return SVC(C=model_param, class_weight=\"balanced\")\n",
    "        \n",
    "    def _split_booknames(self, df, nr_splits, return_indices=False):\n",
    "        \"\"\"\n",
    "        Distribute book names over splits.\n",
    "        All works of an author are in the same split.\n",
    "        \"\"\"\n",
    "        book_names = df[\"book_name\"].unique()\n",
    "        authors = []\n",
    "        booknames_authors_mapping = {}\n",
    "\n",
    "        #Get authors\n",
    "        for book_name in book_names:\n",
    "            author = \"_\".join(book_name.split(\"_\")[:2])\n",
    "            authors.append(author)\n",
    "            if author in booknames_authors_mapping:\n",
    "                booknames_authors_mapping[author].append(book_name)\n",
    "            else:\n",
    "                booknames_authors_mapping[author] = []\n",
    "                booknames_authors_mapping[author].append(book_name)\n",
    "        #Distribute authors over splits so that each split has approximately the same number of books\n",
    "        works_per_author = Counter(authors)\n",
    "        goal_sum = round(len(book_names)/nr_splits)\n",
    "        tolerance = 0.03\n",
    "        lower_threshold = goal_sum - round(tolerance*goal_sum)\n",
    "        upper_threshold = goal_sum + round(tolerance*goal_sum)\n",
    "        author_splits = []\n",
    "        popped_dict = {}\n",
    "\n",
    "        for i in range (0, nr_splits-1):\n",
    "            works_in_split = 0\n",
    "            split = []\n",
    "            curr_author_workcount = 0\n",
    "\n",
    "            # take values from popped dict first\n",
    "            if bool(popped_dict):  \n",
    "                popped = []\n",
    "                for curr_author, curr_author_workcount in popped_dict.items():\n",
    "                    # leave item in popped dict if value is too big\n",
    "                    if works_in_split + curr_author_workcount > upper_threshold:\n",
    "                        continue\n",
    "                    else:\n",
    "                        popped.append(curr_author)\n",
    "                        split.append(curr_author)\n",
    "                        works_in_split += curr_author_workcount\n",
    "                        if works_in_split >= lower_threshold:\n",
    "                            break\n",
    "                for current_author in popped:\n",
    "                    del popped_dict[current_author]\n",
    "            while works_in_split < upper_threshold:\n",
    "                if bool(works_per_author):\n",
    "                    curr_author = random.choice(list(works_per_author.keys()))\n",
    "                    curr_author_workcount = works_per_author.pop(curr_author)\n",
    "                    # Put values into separate dict if too big\n",
    "                    if works_in_split + curr_author_workcount > upper_threshold:\n",
    "                        popped_dict[curr_author] = curr_author_workcount\n",
    "                    else:\n",
    "                        split.append(curr_author)\n",
    "                        works_in_split += curr_author_workcount\n",
    "                        if works_in_split >= lower_threshold:\n",
    "                            break\n",
    "                else:\n",
    "                    #ignore upper threshold\n",
    "                    popped = []\n",
    "                    for curr_author, curr_author_workcount in popped_dict.items():\n",
    "                        popped.append(curr_author)\n",
    "                        split.append(curr_author)\n",
    "                        works_in_split += curr_author_workcount\n",
    "                        if works_in_split >= lower_threshold:\n",
    "                            break\n",
    "                    for current_author in popped:\n",
    "                        del popped_dict[current_author]\n",
    "\n",
    "            author_splits.append(split)\n",
    "        #Create last split directly from remaining dict\n",
    "        works_in_last_split = sum(works_per_author.values()) + sum(popped_dict.values())\n",
    "        split = list(works_per_author.keys()) + list(popped_dict.keys())\n",
    "        author_splits.append(split)\n",
    "\n",
    "        if not return_indices:\n",
    "            #Map author splits to book names\n",
    "            book_splits = []\n",
    "            for author_split in author_splits:\n",
    "                book_split = []\n",
    "                for author in author_split:\n",
    "                    book_split.extend(booknames_authors_mapping[author])\n",
    "                book_splits.append(book_split)\n",
    "        else:\n",
    "            book_name_idx_mapping = dict((book_name, index) for index, book_name in enumerate(book_names))\n",
    "            book_splits = []\n",
    "            for author_split in author_splits:\n",
    "                test_split = []\n",
    "                for author in author_split:\n",
    "                    test_split.extend([book_name_idx_mapping[book_name] for book_name in booknames_authors_mapping[author]])\n",
    "                train_split = list(set(book_name_idx_mapping.values()) - set(test_split))\n",
    "                book_splits.append((train_split, test_split))\n",
    "        return book_splits\n",
    "    \n",
    "    def _get_pvalue(self, validation_corr_pvalues):\n",
    "        # Harmonic mean p-value\n",
    "        denominator = sum([1/x for x in validation_corr_pvalues])\n",
    "        mean_p_value = len(validation_corr_pvalues)/denominator\n",
    "        return mean_p_value\n",
    "    \n",
    "    def _combine_df_labels(self, df):\n",
    "        #Average of sentiscores per book\n",
    "        df = df.merge(right=self.labels, on=\"book_name\", how=\"inner\", validate=\"many_to_one\")\n",
    "        return df\n",
    "    \n",
    "    def run(self):\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        train_mses = []\n",
    "        train_maes = []\n",
    "        train_r2s = []\n",
    "        train_corrs = []\n",
    "        \n",
    "        validation_mses = []\n",
    "        validation_maes = []\n",
    "        validation_r2s = []\n",
    "        validation_corrs = []\n",
    "        validation_corr_pvalues = []\n",
    "\n",
    "        df = self.df\n",
    "        df = self._combine_df_labels(df)\n",
    "        book_names_split = self._split_booknames(df, 5)\n",
    "        all_validation_books = []\n",
    "\n",
    "        for index, split in enumerate(book_names_split):\n",
    "            train_df = df[~df[\"book_name\"].isin(split)]\n",
    "            validation_df = df[df[\"book_name\"].isin(split)]\n",
    "            \n",
    "            train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            train_y = train_df[\"y\"].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            validation_y = validation_df[\"y\"].values.ravel()\n",
    "            train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            if self.model == \"xgboost\":\n",
    "                train_book_names = train_df[\"book_name\"].values.reshape(-1, 1)\n",
    "                best_parameters = self._get_model(self.model_param, train_X, train_y, train_book_names, task_type=\"regression\")\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "                num_boost_round = best_parameters[\"num_boost_round\"]\n",
    "                best_parameters.pop(\"nested_cv_score\")\n",
    "                best_parameters.pop(\"num_boost_round\")\n",
    "                model = xgboost.train(best_parameters,\n",
    "                                      dtrain,\n",
    "                                      num_boost_round=num_boost_round,\n",
    "                                      verbose_eval=False)\n",
    "            else:\n",
    "                model = self._get_model(self.model_param)\n",
    "                model.fit(train_X, train_y)\n",
    "            \n",
    "            train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "            validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            \n",
    "            if self.model == \"xgboost\":\n",
    "                train_books[\"yhat\"] = model.predict(xgboost.DMatrix(train_X))\n",
    "                validation_books[\"yhat\"] = model.predict(xgboost.DMatrix(validation_X))\n",
    "                \n",
    "                print(\"train preds:\", model.predict(xgboost.DMatrix(train_X)))\n",
    "                print(\"validation preds:\", model.predict(xgboost.DMatrix(validation_X)))\n",
    "            else:\n",
    "                train_books[\"yhat\"] = model.predict(train_X)\n",
    "                validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "            \n",
    "            train_books = train_books.groupby(\"book_name\").mean()\n",
    "            validation_books = validation_books.groupby(\"book_name\").mean()\n",
    "            all_validation_books.append(validation_books.reset_index())\n",
    "            \n",
    "            train_y = train_books[\"y\"].tolist()\n",
    "            train_yhat = train_books[\"yhat\"].tolist()\n",
    "            validation_y = validation_books[\"y\"].tolist()\n",
    "            validation_yhat = validation_books[\"yhat\"].tolist()\n",
    "            \n",
    "            all_labels.extend(validation_y)\n",
    "            all_predictions.extend(validation_yhat)\n",
    "            \n",
    "            train_mse = mean_squared_error(train_y, train_yhat)\n",
    "            train_mae = mean_absolute_error(train_y, train_yhat)\n",
    "            train_r2 = r2_score(train_y, train_yhat)\n",
    "            train_corr = pearsonr(train_y, train_yhat)[0]\n",
    "            \n",
    "            validation_mse = mean_squared_error(validation_y, validation_yhat)\n",
    "            validation_mae = mean_absolute_error(validation_y, validation_yhat)\n",
    "            validation_r2 = r2_score(validation_y, validation_yhat)\n",
    "            validation_corr = pearsonr(validation_y, validation_yhat)[0]\n",
    "            p_value = pearsonr(validation_y, validation_yhat)[1]\n",
    "            \n",
    "            train_mses.append(train_mse)\n",
    "            train_maes.append(train_mae)\n",
    "            train_r2s.append(train_r2)\n",
    "            train_corrs.append(train_corr)\n",
    "            \n",
    "            validation_mses.append(validation_mse)\n",
    "            validation_maes.append(validation_mae)\n",
    "            validation_r2s.append(validation_r2)\n",
    "            validation_corrs.append(validation_corr)\n",
    "            validation_corr_pvalues.append(p_value)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Fold: {index+1}, TrainMSE: {np.round(train_mse, 3)}, TrainMAE: {np.round(train_mae, 3)}, ValMSE: {np.round(validation_mse, 3)}, ValMAE: {np.round(validation_mae, 3)}, ValR2: {np.round(validation_r2, 3)}, ValCorr: {np.round(validation_corr, 3)}\")\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        \n",
    "        # Save y and y_pred for examples\n",
    "        #pd.concat(all_validation_books).to_csv(results_dir + \"/y-yhat-\" + param_dir + '_' + self.language + \"-\" + \".csv\", index=False)\n",
    "        \n",
    "        mean_train_mse = np.mean(train_mses)\n",
    "        mean_train_rmse = np.mean([sqrt(x) for x in train_mses])\n",
    "        mean_train_mae = np.mean(train_maes)\n",
    "        mean_train_r2 = np.mean(train_r2s)\n",
    "        mean_train_corr = np.mean(train_corrs)\n",
    "        \n",
    "        mean_validation_mse = np.mean(validation_mses)\n",
    "        mean_validation_rmse = np.mean([sqrt(x) for x in validation_mses])\n",
    "        mean_validation_mae = np.mean(validation_maes)\n",
    "        mean_validation_r2 = np.mean(validation_r2s)\n",
    "        mean_validation_corr = np.mean(validation_corrs)\n",
    "        mean_p_value = self._get_pvalue(validation_corr_pvalues)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\"\"TrainMSE: {np.round(mean_train_mse, 3)}, TrainRMSE: {np.round(mean_train_rmse, 3)}, TrainMAE: {np.round(mean_train_mae, 3)}, TrainR2: {np.round(mean_train_r2, 3)}, TrainCorr: {np.round(mean_train_corr, 3)}, ValMSE: {np.round(mean_validation_mse, 3)}, ValRMSE: {np.round(mean_validation_rmse, 3)}, ValMAE: {np.round(mean_validation_mae, 3)}, ValR2: {np.round(mean_validation_r2, 3)}, ValCorr: {np.round(mean_validation_corr, 3)}, ValCorrPValue: {np.round(mean_p_value, 3)}\"\"\")\n",
    "            print(\"\\n---------------------------------------------------\\n\")\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.xticks(fontsize=15)\n",
    "            plt.yticks(fontsize=15)\n",
    "            plt.xlim([0,1])\n",
    "            plt.ylim([0,1])\n",
    "\n",
    "            plt.scatter(all_labels, all_predictions, s=6)\n",
    "            plt.xlabel(\"Canonization Scores\", fontsize=20)\n",
    "            plt.ylabel(\"Predicted Scores\", fontsize=20)\n",
    "            plt.savefig(results_dir + lang + \"-\" + self.model + \"-\" + str(self.dimensionality_reduction) \n",
    "            + \"-\" + self.features + \"-\" + \"-\" + \"param\" + str(self.model_param) + \"-\" + \".png\", \n",
    "            dpi=400, bbox_inches=\"tight\")\n",
    "    \n",
    "            plt.show();\n",
    "        return mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09330efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Classification into reviewed/not reviewed\n",
    "'''\n",
    "\n",
    "class TwoclassClassification(Regression):\n",
    "    def __init__(self, language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose):\n",
    "        super().__init__(language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose)\n",
    "\n",
    "    def _check_class_specific_assertions(self):\n",
    "        assert model in [\"svc\", \"xgboost\"]\n",
    "        assert features in [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "        \n",
    "    def _prepare_labels(self):\n",
    "        labels = self.labels.drop(columns=\"y\").rename(columns={\"c\":\"y\"})\n",
    "        return labels\n",
    "        \n",
    "    def _combine_df_labels(self, df):\n",
    "        #Reviews zum englischen Korpus beginnnen mit 1759 und decken alles bis 1914 ab\n",
    "        agg_labels = self.labels[[\"book_name\"]].drop_duplicates()\n",
    "        agg_labels[\"y\"] = 1\n",
    "        df = df.merge(right=agg_labels, on=\"book_name\", how=\"left\", validate=\"many_to_one\")\n",
    "        df[\"y\"] = df[\"y\"].fillna(value=0)\n",
    "        #Select books written after year of first review)\n",
    "        year = df[\"book_name\"].str.replace('-', '_').str.split('_').str[-1].astype('int64')\n",
    "        df = df.loc[year>=min(year)]\n",
    "        return df\n",
    "    \n",
    "    def _get_sample_weights(self, df):\n",
    "        # Weights for calculating accuracy \n",
    "        chunks_per_book = df[\"book_name\"].value_counts(sort=False).rename('chunks_per_book')\n",
    "        chunks_per_book = chunks_per_book.reset_index().rename(columns={\"index\":'book_name'})\n",
    "        chunks_per_book[\"chunks_per_book\"] = 1/chunks_per_book[\"chunks_per_book\"]\n",
    "        df = df.merge(right=chunks_per_book, how=\"left\", on=\"book_name\")\n",
    "        sample_weights = df[\"chunks_per_book\"].tolist()\n",
    "        return sample_weights\n",
    "    \n",
    "    def _aggregate_chunk_predictions(self, df):\n",
    "        g = df.groupby(\"book_name\")\n",
    "        \n",
    "        # Majority vote\n",
    "        # If one value is more common, assign it to every chunk\n",
    "        # Therefore, accuracy is either 0 or 1\n",
    "        # If both values are equally likely, leave them unchanged, and accuracy is 0.5\n",
    "        def _get_mode_accuracy(group):\n",
    "            counts = group[\"yhat\"].value_counts()\n",
    "            if len(counts) == 1:\n",
    "                mode_acc = counts.index[0]\n",
    "            else:\n",
    "                mode_acc = 0.5\n",
    "            return mode_acc\n",
    "        mode_accs = g.apply(_get_mode_accuracy).rename(\"mode_acc\").reset_index()\n",
    "        mode_acc = mode_accs[\"mode_acc\"].mean()\n",
    "        \n",
    "        # Average accuracy within book\n",
    "        book_acc = g.apply(lambda group: accuracy_score(group[\"y\"], group[\"yhat\"])).mean()\n",
    "        #Accuracy when each chunk is treated as single document\n",
    "        chunk_acc = accuracy_score(df[\"y\"], df[\"yhat\"])#, sample_weight = self._get_sample_weights(df))\n",
    "        return {\"mode_acc\": mode_acc, \"book_acc\": book_acc, \"chunk_acc\": chunk_acc}\n",
    "    \n",
    "    def _split_booknames_stratified(self, df, nr_splits, return_indices=False):\n",
    "        label_splits = []\n",
    "        combined_splits = []\n",
    "        # Split df into folds for each label individualls\n",
    "        df_by_labels = df.groupby(\"y\")\n",
    "        for name, group in df_by_labels:\n",
    "            split = self._split_booknames(group, 5)\n",
    "            label_splits.append(split)\n",
    "        # Combine splits so that one splits combines splits for all labels\n",
    "        for fold in range(0, nr_splits):\n",
    "            combined_split = []\n",
    "            for label in range(0, len(pd.unique(df[\"y\"]))):\n",
    "                label_split = label_splits[label]\n",
    "                fold_split = label_split[fold]\n",
    "                combined_split.extend(fold_split)\n",
    "            combined_splits.append(combined_split)\n",
    "        return combined_splits                            \n",
    "                             \n",
    "    def run(self):\n",
    "        train_accs = []\n",
    "        validation_accs = []\n",
    "        df = self.df\n",
    "        df = self._combine_df_labels(df)\n",
    "        book_names_split_stratified = self._split_booknames_stratified(df, nr_splits=5, return_indices=False)\n",
    "        all_validation_books = []\n",
    "\n",
    "        for index, split in enumerate(book_names_split_stratified):\n",
    "            train_df = df[~df[\"book_name\"].isin(split)]\n",
    "            validation_df = df[df[\"book_name\"].isin(split)]\n",
    "            train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            train_y = train_df[\"y\"].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            validation_y = validation_df[\"y\"].values.ravel()\n",
    "            train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            if self.model == \"xgboost\":\n",
    "                train_book_names = train_df[\"book_name\"].values.reshape(-1, 1)\n",
    "                best_parameters = self._get_model(self.model_param, train_X, train_y, train_book_names, task_type=\"binary_classification\")\n",
    "                class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1], y=train_y.astype(int).tolist())))\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y.astype(int), weight=[class_weights[int(i)] for i in train_y])\n",
    "                num_boost_round = best_parameters[\"num_boost_round\"]\n",
    "                best_parameters.pop(\"nested_cv_score\")\n",
    "                best_parameters.pop(\"num_boost_round\")\n",
    "                model = xgboost.train(best_parameters,\n",
    "                                      dtrain,\n",
    "                                      num_boost_round=num_boost_round,\n",
    "                                      verbose_eval=False)\n",
    "            else:\n",
    "                model = self._get_model(self.model_param)\n",
    "                model.fit(train_X, train_y)\n",
    "            \n",
    "            train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "            validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            \n",
    "            if self.model == \"xgboost\":\n",
    "                train_books[\"yhat\"] = model.predict(xgboost.DMatrix(train_X))\n",
    "                validation_books[\"yhat\"] = model.predict(xgboost.DMatrix(validation_X))\n",
    "            else:\n",
    "                train_books[\"yhat\"] = model.predict(train_X)\n",
    "                validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "\n",
    "            train_acc = self._aggregate_chunk_predictions(train_books)\n",
    "            validation_acc = self._aggregate_chunk_predictions(validation_books)\n",
    "            \n",
    "            all_validation_books.append(validation_books)\n",
    "            \n",
    "            train_accs.append(train_acc)\n",
    "            validation_accs.append(validation_acc)\n",
    "        \n",
    "        # Save y and y_pred for examples\n",
    "        all_validation_books = pd.concat(all_validation_books)\n",
    "        #all_validation_books.to_csv(results_dir + \"valiationbooks-class-\" + self.language + \"-\" + \".csv\", index=False)\n",
    "        \n",
    "        print(confusion_matrix(all_validation_books[\"y\"], all_validation_books[\"yhat\"]))\n",
    "        print(pd.crosstab(all_validation_books[\"y\"], all_validation_books[\"yhat\"], rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "        train_accs = pd.DataFrame(train_accs)\n",
    "        validation_accs = pd.DataFrame(validation_accs)\n",
    "\n",
    "        mean_train_mode_acc = train_accs[\"mode_acc\"].mean()\n",
    "        mean_train_book_acc = train_accs[\"book_acc\"].mean()\n",
    "        mean_train_chunk_acc = train_accs[\"chunk_acc\"].mean()\n",
    "        mean_validation_mode_acc = validation_accs[\"mode_acc\"].mean()\n",
    "        mean_validation_book_acc = validation_accs[\"book_acc\"].mean()\n",
    "        mean_validation_chunk_acc = validation_accs[\"chunk_acc\"].mean()\n",
    "        print('validation mode, book, and chunk acc', mean_validation_mode_acc, mean_validation_book_acc, mean_validation_chunk_acc)\n",
    "\n",
    "        return mean_train_book_acc, mean_validation_book_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f655dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Classification into in library/not in library\n",
    "'''\n",
    "\n",
    "class LibraryClassification(TwoclassClassification):\n",
    "    def _combine_df_labels(self, df):\n",
    "        df = df.merge(right=self.labels, on=\"book_name\", how=\"left\", validate=\"one_to_one\")\n",
    "        #Select books written after year first one appeared in a library catalogues\n",
    "        df[\"year\"] = df[\"book_name\"].str.replace('-', '_').str.split('_').str[-1].astype('int64')\n",
    "        helper_df = df.loc[df[\"y\"]!=0]\n",
    "        first_library_year = min(helper_df[\"y\"])\n",
    "        df = df.loc[df[\"year\"]>=first_library_year]\n",
    "        df = df.drop(columns=\"year\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a62768d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Classification into not reviewed/negative/not classified/positive\n",
    "'''\n",
    "\n",
    "class MulticlassClassification(TwoclassClassification):\n",
    "    def __init__(self, language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose):\n",
    "        super().__init__(language, features, drop_columns_including, dimensionality_reduction, model_param, model, verbose)\n",
    "\n",
    "    def _check_class_specific_assertions(self):\n",
    "        assert model in [\"svc\", \"xgboost\"]\n",
    "        assert features in [\"book\", \"book_and_averaged_chunk\"]#, \"chunk_and_copied_book\", \"chunk\"]\n",
    "                \n",
    "    def _combine_df_labels(self, df):\n",
    "        #Reviews zum englischen Korpus beginnnen mit 1759 und decken alles bis 1914 ab\n",
    "        df = df.merge(right=self.labels, on=\"book_name\", how=\"left\", validate=\"many_to_one\")\n",
    "        df[\"y\"] = df[\"y\"].fillna(value=0)\n",
    "        #Select books written after year of first review\n",
    "        year = df[\"book_name\"].str.replace('-', '_').str.split('_').str[-1].astype('int64')\n",
    "        df = df.loc[year>=min(year)]\n",
    "        return df\n",
    "    \n",
    "    def _evaluate_predictions(self, df):\n",
    "        score = f1_score(df[\"y\"], df[\"yhat\"], average='macro')\n",
    "        return score\n",
    "            \n",
    "        \n",
    "    def run(self):\n",
    "        train_f1s = []\n",
    "        validation_f1s = []\n",
    "\n",
    "        df = self.df\n",
    "        df = self._combine_df_labels(df)\n",
    "        book_names_split_stratified = self._split_booknames_stratified(df, nr_splits=5, return_indices=False)\n",
    "        all_validation_books = []\n",
    "\n",
    "        for index, split in enumerate(book_names_split_stratified):\n",
    "            train_df = df[~df[\"book_name\"].isin(split)]\n",
    "            validation_df = df[df[\"book_name\"].isin(split)]\n",
    "            print(\"class distribution over dfs\")\n",
    "            print(train_df[\"y\"].value_counts())\n",
    "            print(validation_df[\"y\"].value_counts())\n",
    "            #print(train_df.loc[train_df[\"y\"]==1])\n",
    "            print(validation_df.loc[validation_df[\"y\"]==1])\n",
    "            \n",
    "            train_X = train_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            train_y = train_df[\"y\"].values.ravel()\n",
    "            validation_X = validation_df.drop(columns=[\"y\", \"book_name\"]).values\n",
    "            validation_y = validation_df[\"y\"].values.ravel()\n",
    "            train_X, validation_X = self._impute(train_X, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape before {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape before {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            train_X, validation_X = self._select_features(train_X, train_y, validation_X)\n",
    "            #if self.verbose:\n",
    "            #    print(f\"train_X.shape after {self.dimensionality_reduction}: {train_X.shape}, validation_X.shape after {self.dimensionality_reduction}: {validation_X.shape}\")\n",
    "            if self.model == \"xgboost\":\n",
    "                train_book_names = train_df[\"book_name\"].values.reshape(-1, 1)\n",
    "                best_parameters = self._get_model(self.model_param, train_X, train_y, train_book_names, task_type=\"multiclass_classification\")\n",
    "                class_weights = dict(enumerate(compute_class_weight(\"balanced\", classes=[0, 1, 2, 3], y=train_y.astype(int).tolist())))\n",
    "                dtrain = xgboost.DMatrix(train_X, label=train_y.astype(int), weight=[class_weights[int(i)] for i in train_y])\n",
    "                num_boost_round = best_parameters[\"num_boost_round\"]\n",
    "                best_parameters.pop(\"nested_cv_score\")\n",
    "                best_parameters.pop(\"num_boost_round\")\n",
    "                model = xgboost.train(best_parameters,\n",
    "                                      dtrain,\n",
    "                                      num_boost_round=num_boost_round,\n",
    "                                      verbose_eval=False)\n",
    "            else:\n",
    "                model = self._get_model(self.model_param)\n",
    "                model.fit(train_X, train_y)\n",
    "            \n",
    "            train_books = deepcopy(train_df[[\"book_name\", \"y\"]])\n",
    "            validation_books = deepcopy(validation_df[[\"book_name\", \"y\"]])\n",
    "            \n",
    "            if self.model == \"xgboost\":\n",
    "                train_books[\"yhat\"] = model.predict(xgboost.DMatrix(train_X))\n",
    "                validation_books[\"yhat\"] = model.predict(xgboost.DMatrix(validation_X))\n",
    "            else:\n",
    "                train_books[\"yhat\"] = model.predict(train_X)\n",
    "                validation_books[\"yhat\"] = model.predict(validation_X)\n",
    "            \n",
    "            train_f1 = self._evaluate_predictions(train_books)\n",
    "            validation_f1 = self._evaluate_predictions(validation_books)\n",
    "            all_validation_books.append(validation_books)\n",
    "            \n",
    "            train_f1s.append(train_f1)\n",
    "            validation_f1s.append(validation_f1)\n",
    "            if self.verbose:\n",
    "                print(f\"Fold: {index+1}, TrainF1: {np.round(train_f1, 3)}, ValF1: {np.round(validation_f1, 3)}\")\n",
    "        \n",
    "        # Save y and y_pred for examples\n",
    "        all_validation_books = pd.concat(all_validation_books)\n",
    "        #all_validation_books.to_csv(results_dir + \"valiationbooks-class-\" + self.language + \"-\" + \".csv\", index=False)\n",
    "        \n",
    "        print(confusion_matrix(all_validation_books[\"y\"], all_validation_books[\"yhat\"]))\n",
    "        print(pd.crosstab(all_validation_books[\"y\"], all_validation_books[\"yhat\"], rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "        mean_train_f1 = statistics.mean(train_f1s)\n",
    "        mean_validation_f1 = statistics.mean(validation_f1s)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\"\"TrainF1: {np.round(mean_train_f1, 3)}, ValidationF1: {np.round(mean_validation_f1, 3)}\"\"\")\n",
    "            print(\"\\n---------------------------------------------------\\n\")\n",
    "        return mean_train_f1, mean_validation_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f06b21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cross-validation\n",
    "\n",
    "# Feature split\n",
    "complexity_features = []\n",
    "\n",
    "# All parameters\n",
    "models = [\"svr\", \"lasso\", \"xgboost\", \"svc\"]\n",
    "model_params = {\"svr\": [1], \"lasso\": [1, 4], \"xgboost\": [None], \"svc\": [0.1, 1, 10, 100, 1000, 10000]} #\n",
    "dimensionality_reduction = [\"ss_pca_0_95\", 'k_best_f_reg_0_10', 'k_best_mutual_info_0_10', [None]]\n",
    "features = [\"book\", \"chunk\", \"book_and_averaged_chunk\", \"chunk_and_copied_book\"]\n",
    "\n",
    "# Which parameters to use\n",
    "regression_params = {\"model\": [\"xgboost\"], \"dimensionality_reduction\": dimensionality_reduction[-1], \"features\": features}\n",
    "testing_params = {\"model\": models[3], \"dimensionality_reduction\": dimensionality_reduction[-1], \n",
    "                  \"features\": [\"book\"]}\n",
    "twoclass_params = {\"model\": [\"svc\", \"xgboost\"], \"dimensionality_reduction\": dimensionality_reduction[-1], \n",
    "                  \"features\": [\"book\", \"book_and_averaged_chunk\"]}\n",
    "multiclass_params = {\"model\": [\"svc\", \"xgboost\"], \"dimensionality_reduction\": dimensionality_reduction[-1],\n",
    "                     \"features\": [\"book\", \"book_and_averaged_chunk\"]}\n",
    "library_params = {\"model\": [\"svc\", \"xgboost\"], \"dimensionality_reduction\": dimensionality_reduction[-1], \n",
    "                  \"features\": [\"book\", \"book_and_averaged_chunk\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd3d2f",
   "metadata": {},
   "source": [
    "book_df = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\")\n",
    "book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "book_df = drop_default_columns(book_df, drop_default_columns_including)\n",
    "book_and_averaged_chunk_df = drop_default_columns(book_and_averaged_chunk_df, drop_default_columns_including)\n",
    "chunk_df = drop_default_columns(chunk_df, drop_default_columns_including)\n",
    "chunk_and_copied_book_df = drop_default_columns(chunk_and_copied_book_df, drop_default_columns_including)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330757f4",
   "metadata": {},
   "source": [
    "print(len(book_df.columns), len(book_and_averaged_chunk_df.columns),len(chunk_df.columns),len(chunk_and_copied_book_df.columns),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7435d1",
   "metadata": {},
   "source": [
    "len(list(book_and_averaged_chunk_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e238a",
   "metadata": {},
   "source": [
    "for i in list(book_and_averaged_chunk_df.columns):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0889634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc\n",
      "Dropped 300 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 71 170]\n",
      " [ 46 260]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           71  170  241\n",
      "1           46  260  306\n",
      "All        117  430  547\n",
      "validation mode, book, and chunk acc 0.7857407115205279 0.6052445857033013 0.6052445857033013\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 0.1 0.6010118901374024 0.6052445857033013\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 74 167]\n",
      " [ 52 254]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           74  167  241\n",
      "1           52  254  306\n",
      "All        126  421  547\n",
      "validation mode, book, and chunk acc 0.7688345471150054 0.5995410782027747 0.5995410782027747\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 0.1 0.6115084106559288 0.5995410782027747\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 377 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 82 159]\n",
      " [ 55 251]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           82  159  241\n",
      "1           55  251  306\n",
      "All        137  410  547\n",
      "validation mode, book, and chunk acc 0.7497358385443668 0.6090827215672808 0.6090827215672808\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 0.1 0.6110965129618038 0.6090827215672808\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 377 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 68 173]\n",
      " [ 50 256]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           68  173  241\n",
      "1           50  256  306\n",
      "All        118  429  547\n",
      "validation mode, book, and chunk acc 0.7839824645490663 0.5932305599274825 0.5932305599274825\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 0.1 0.609781010325346 0.5932305599274825\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 26 215]\n",
      " [ 45 261]]\n",
      "Predicted   0    1  All\n",
      "True                   \n",
      "0          26  215  241\n",
      "1          45  261  306\n",
      "All        71  476  547\n",
      "validation mode, book, and chunk acc 0.8672897196261683 0.523563569367526 0.523563569367526\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 0.1 0.5539378192404552 0.523563569367526\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[157  84]\n",
      " [192 114]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          157   84  241\n",
      "1          192  114  306\n",
      "All        349  198  547\n",
      "validation mode, book, and chunk acc 0.36722970076309697 0.49581713340480765 0.49581713340480765\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 0.1 0.4806385157416706 0.49581713340480765\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1245 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 63 178]\n",
      " [ 79 227]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           63  178  241\n",
      "1           79  227  306\n",
      "All        142  405  547\n",
      "validation mode, book, and chunk acc 0.7349948078920041 0.5294525654338738 0.5294525654338738\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 0.1 0.5339430861028573 0.5294525654338738\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1245 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 58 183]\n",
      " [ 70 236]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           58  183  241\n",
      "1           70  236  306\n",
      "All        128  419  547\n",
      "validation mode, book, and chunk acc 0.7853211009174312 0.5397267901448272 0.5397267901448272\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 0.1 0.5368960159855544 0.5397267901448272\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[127 114]\n",
      " [126 180]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          127  114  241\n",
      "1          126  180  306\n",
      "All        253  294  547\n",
      "validation mode, book, and chunk acc 0.5376063334934542 0.561703549993861 0.561703549993861\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 1 0.6288697577734814 0.561703549993861\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[126 115]\n",
      " [114 192]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          126  115  241\n",
      "1          114  192  306\n",
      "All        240  307  547\n",
      "validation mode, book, and chunk acc 0.5625375960272804 0.5831944747585729 0.5831944747585729\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 1 0.6180613617283528 0.5831944747585729\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 377 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[142  99]\n",
      " [125 181]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          142   99  241\n",
      "1          125  181  306\n",
      "All        267  280  547\n",
      "validation mode, book, and chunk acc 0.5101470427566026 0.5919361100069371 0.5919361100069371\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 1 0.6229852584050436 0.5919361100069371\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 377 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[139 102]\n",
      " [117 189]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          139  102  241\n",
      "1          117  189  306\n",
      "All        256  291  547\n",
      "validation mode, book, and chunk acc 0.5304929563667 0.6004536073721403 0.6004536073721403\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 1 0.6201512163175887 0.6004536073721403\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 96 145]\n",
      " [133 173]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           96  145  241\n",
      "1          133  173  306\n",
      "All        229  318  547\n",
      "validation mode, book, and chunk acc 0.5744100394914132 0.48981866615648395 0.48981866615648395\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 1 0.5280197593346332 0.48981866615648395\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "SVC with balanced class weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 86 155]\n",
      " [129 177]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           86  155  241\n",
      "1          129  177  306\n",
      "All        215  332  547\n",
      "validation mode, book, and chunk acc 0.5991781403895968 0.4788214462785355 0.4788214462785355\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 1 0.5271318140224747 0.4788214462785355\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1245 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 95 146]\n",
      " [150 156]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           95  146  241\n",
      "1          150  156  306\n",
      "All        245  302  547\n",
      "validation mode, book, and chunk acc 0.5480103776171489 0.45913770070003534 0.45913770070003534\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 1 0.5308390376729857 0.45913770070003534\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1245 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 59 182]\n",
      " [122 184]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           59  182  241\n",
      "1          122  184  306\n",
      "All        181  366  547\n",
      "validation mode, book, and chunk acc 0.6724918817968675 0.4456170559630313 0.4456170559630313\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 1 0.5515585425663703 0.4456170559630313\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[145  96]\n",
      " [131 175]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          145   96  241\n",
      "1          131  175  306\n",
      "All        276  271  547\n",
      "validation mode, book, and chunk acc 0.495270827386021 0.5843202415198443 0.5843202415198443\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 10 0.63709464283349 0.5843202415198443\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[147  94]\n",
      " [129 177]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          147   94  241\n",
      "1          129  177  306\n",
      "All        276  271  547\n",
      "validation mode, book, and chunk acc 0.49543615656823203 0.5926200299785206 0.5926200299785206\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 10 0.6343749162447784 0.5926200299785206\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 377 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[146  95]\n",
      " [139 167]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          146   95  241\n",
      "1          139  167  306\n",
      "All        285  262  547\n",
      "validation mode, book, and chunk acc 0.4791098425694461 0.5721941144254876 0.5721941144254876\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 10 0.6335738084128497 0.5721941144254876\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 377 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[141 100]\n",
      " [137 169]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          141  100  241\n",
      "1          137  169  306\n",
      "All        278  269  547\n",
      "validation mode, book, and chunk acc 0.4926258979553209 0.5679488778212665 0.5679488778212665\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 10 0.6449576971329503 0.5679488778212665\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 44 197]\n",
      " [ 70 236]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           44  197  241\n",
      "1           70  236  306\n",
      "All        114  433  547\n",
      "validation mode, book, and chunk acc 0.7905877456161073 0.5110646648089444 0.5110646648089444\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 10 0.5598751393497814 0.5110646648089444\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 58 183]\n",
      " [125 181]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           58  183  241\n",
      "1          125  181  306\n",
      "All        183  364  547\n",
      "validation mode, book, and chunk acc 0.6664783464947533 0.43701350392080496 0.43701350392080496\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 10 0.5520728650237265 0.43701350392080496\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1245 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 35 206]\n",
      " [ 48 258]]\n",
      "Predicted   0    1  All\n",
      "True                   \n",
      "0          35  206  241\n",
      "1          48  258  306\n",
      "All        83  464  547\n",
      "validation mode, book, and chunk acc 0.8459267224282213 0.5354885918172816 0.5354885918172816\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 10 0.5654271828727684 0.5354885918172816\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1245 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 55 186]\n",
      " [112 194]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           55  186  241\n",
      "1          112  194  306\n",
      "All        167  380  547\n",
      "validation mode, book, and chunk acc 0.6989890194747497 0.45849156228487287 0.45849156228487287\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 10 0.5663924322775609 0.45849156228487287\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[149  92]\n",
      " [148 158]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          149   92  241\n",
      "1          148  158  306\n",
      "All        297  250  547\n",
      "validation mode, book, and chunk acc 0.4577271473913324 0.563614741048249 0.563614741048249\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 100 0.6462202778961647 0.563614741048249\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[150  91]\n",
      " [135 171]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          150   91  241\n",
      "1          135  171  306\n",
      "All        285  262  547\n",
      "validation mode, book, and chunk acc 0.4755407048315662 0.5839264712051924 0.5839264712051924\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 100 0.6492780466812358 0.5839264712051924\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 377 columns.\n",
      "SVC with balanced class weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[154  87]\n",
      " [132 174]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          154   87  241\n",
      "1          132  174  306\n",
      "All        286  261  547\n",
      "validation mode, book, and chunk acc 0.4792283781046384 0.5983313683799409 0.5983313683799409\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 100 0.6457700169269586 0.5983313683799409\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 377 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[146  95]\n",
      " [135 171]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          146   95  241\n",
      "1          135  171  306\n",
      "All        281  266  547\n",
      "validation mode, book, and chunk acc 0.48721849402873374 0.5811189916228517 0.5811189916228517\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 100 0.6463613078953748 0.5811189916228517\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 82 159]\n",
      " [159 147]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           82  159  241\n",
      "1          159  147  306\n",
      "All        241  306  547\n",
      "validation mode, book, and chunk acc 0.5624715363890889 0.4205299359534854 0.4205299359534854\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 100 0.5623342781526967 0.4205299359534854\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 52 189]\n",
      " [ 78 228]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           52  189  241\n",
      "1           78  228  306\n",
      "All        130  417  547\n",
      "validation mode, book, and chunk acc 0.7596789239356688 0.5105703033581716 0.5105703033581716\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 100 0.558091756489757 0.5105703033581716\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1245 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 68 173]\n",
      " [161 145]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           68  173  241\n",
      "1          161  145  306\n",
      "All        229  318  547\n",
      "validation mode, book, and chunk acc 0.5837409843632797 0.3895404189025446 0.3895404189025446\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 100 0.5496611370051426 0.3895404189025446\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1245 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 66 175]\n",
      " [121 185]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           66  175  241\n",
      "1          121  185  306\n",
      "All        187  360  547\n",
      "validation mode, book, and chunk acc 0.6611618588333303 0.4587101278373257 0.4587101278373257\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 100 0.5520485475041284 0.4587101278373257\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[130 111]\n",
      " [142 164]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          130  111  241\n",
      "1          142  164  306\n",
      "All        272  275  547\n",
      "validation mode, book, and chunk acc 0.5057913283284817 0.5385117263269266 0.5385117263269266\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 1000 0.6763888949519201 0.5385117263269266\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[146  95]\n",
      " [144 162]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          146   95  241\n",
      "1          144  162  306\n",
      "All        290  257  547\n",
      "validation mode, book, and chunk acc 0.46861787792175863 0.5639323512365616 0.5639323512365616\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 1000 0.676861766196404 0.5639323512365616\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 377 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[130 111]\n",
      " [149 157]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          130  111  241\n",
      "1          149  157  306\n",
      "All        279  268  547\n",
      "validation mode, book, and chunk acc 0.48791855247947635 0.5237547757597131 0.5237547757597131\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 1000 0.6864977323146839 0.5237547757597131\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 377 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[136 105]\n",
      " [133 173]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          136  105  241\n",
      "1          133  173  306\n",
      "All        269  278  547\n",
      "validation mode, book, and chunk acc 0.5088726541322186 0.5651821839226832 0.5651821839226832\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 1000 0.6732229646225606 0.5651821839226832\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 80 161]\n",
      " [125 181]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           80  161  241\n",
      "1          125  181  306\n",
      "All        205  342  547\n",
      "validation mode, book, and chunk acc 0.6273983668323291 0.4777596966276212 0.4777596966276212\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 1000 0.5506441821032956 0.4777596966276212\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[101 140]\n",
      " [179 127]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          101  140  241\n",
      "1          179  127  306\n",
      "All        280  267  547\n",
      "validation mode, book, and chunk acc 0.4874500589612264 0.4186112137799133 0.4186112137799133\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 1000 0.5548954765811258 0.4186112137799133\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1245 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[100 141]\n",
      " [184 122]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          100  141  241\n",
      "1          184  122  306\n",
      "All        284  263  547\n",
      "validation mode, book, and chunk acc 0.4813221859037606 0.40686746532438195 0.40686746532438195\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 1000 0.5479925895767765 0.40686746532438195\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1245 columns.\n",
      "SVC with balanced class weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 49 192]\n",
      " [ 75 231]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           49  192  241\n",
      "1           75  231  306\n",
      "All        124  423  547\n",
      "validation mode, book, and chunk acc 0.7700220396403618 0.5105878086714921 0.5105878086714921\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 1000 0.5616974704992496 0.5105878086714921\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[120 121]\n",
      " [152 154]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          120  121  241\n",
      "1          152  154  306\n",
      "All        272  275  547\n",
      "validation mode, book, and chunk acc 0.5021243933429659 0.5007239492447157 0.5007239492447157\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 10000 0.7660591351378553 0.5007239492447157\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 95 146]\n",
      " [158 148]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           95  146  241\n",
      "1          158  148  306\n",
      "All        253  294  547\n",
      "validation mode, book, and chunk acc 0.5366521232887859 0.4438135902910007 0.4438135902910007\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 10000 0.7673174653741713 0.4438135902910007\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 377 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[134 107]\n",
      " [130 176]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          134  107  241\n",
      "1          130  176  306\n",
      "All        264  283  547\n",
      "validation mode, book, and chunk acc 0.5160332294911735 0.5661572196424955 0.5661572196424955\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 10000 0.7202571153400962 0.5661572196424955\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 377 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[113 128]\n",
      " [132 174]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0          113  128  241\n",
      "1          132  174  306\n",
      "All        245  302  547\n",
      "validation mode, book, and chunk acc 0.5541691154316795 0.5263180069210751 0.5263180069210751\n",
      "ger svc book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 10000 0.7245105550327655 0.5263180069210751\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 85 156]\n",
      " [159 147]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           85  156  241\n",
      "1          159  147  306\n",
      "All        244  303  547\n",
      "validation mode, book, and chunk acc 0.5562888287893857 0.4242399811613911 0.4242399811613911\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= 10000 0.5752965355078723 0.4242399811613911\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 83 158]\n",
      " [154 152]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           83  158  241\n",
      "1          154  152  306\n",
      "All        237  310  547\n",
      "validation mode, book, and chunk acc 0.568522309585145 0.43097752574989895 0.43097752574989895\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= 10000 0.5835883787289724 0.43097752574989895\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1245 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 66 175]\n",
      " [135 171]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           66  175  241\n",
      "1          135  171  306\n",
      "All        201  346  547\n",
      "validation mode, book, and chunk acc 0.6321404989626299 0.4346847481094273 0.4346847481094273\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= 10000 0.6010026696375541 0.4346847481094273\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1245 columns.\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "SVC with balanced class weight\n",
      "[[ 83 158]\n",
      " [110 196]]\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           83  158  241\n",
      "1          110  196  306\n",
      "All        193  354  547\n",
      "validation mode, book, and chunk acc 0.6460315087649545 0.5096217597591354 0.5096217597591354\n",
      "ger svc book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= 10000 0.591426352407351 0.5096217597591354\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "xgboost\n",
      "Dropped 300 columns.\n",
      "[[117 124]\n",
      " [137 169]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0          117  124  241\n",
      "1          137  169  306\n",
      "All        254  293  547\n",
      "validation mode, book, and chunk acc 0.5367938966566753 0.522263515659742 0.522263515659742\n",
      "ger xgboost book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= None 0.9004612190625766 0.522263515659742\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 300 columns.\n",
      "[[107 134]\n",
      " [148 158]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0          107  134  241\n",
      "1          148  158  306\n",
      "All        255  292  547\n",
      "validation mode, book, and chunk acc 0.5335309385808831 0.48351684489309354 0.48351684489309354\n",
      "ger xgboost book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= None 0.8805361771728469 0.48351684489309354\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 377 columns.\n",
      "[[121 120]\n",
      " [139 167]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0          121  120  241\n",
      "1          139  167  306\n",
      "All        260  287  547\n",
      "validation mode, book, and chunk acc 0.5260162976859478 0.5276976524044978 0.5276976524044978\n",
      "ger xgboost book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= None 0.9298351545628281 0.5276976524044978\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 377 columns.\n",
      "[[118 123]\n",
      " [139 167]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0          118  123  241\n",
      "1          139  167  306\n",
      "All        257  290  547\n",
      "validation mode, book, and chunk acc 0.5296599819957085 0.5210419187110207 0.5210419187110207\n",
      "ger xgboost book ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= None 0.9658413466504534 0.5210419187110207\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n",
      "[[123 118]\n",
      " [142 164]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0          123  118  241\n",
      "1          142  164  306\n",
      "All        265  282  547\n",
      "validation mode, book, and chunk acc 0.5154558737749133 0.5230992024302488 0.5230992024302488\n",
      "ger xgboost book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding'] None param= None 0.8299517464915318 0.5230992024302488\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1168 columns.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[111 130]\n",
      " [153 153]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0          111  130  241\n",
      "1          153  153  306\n",
      "All        264  283  547\n",
      "validation mode, book, and chunk acc 0.5174169910544073 0.4835761698345438 0.4835761698345438\n",
      "ger xgboost book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->'] None param= None 0.9562844036697248 0.4835761698345438\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1245 columns.\n",
      "[[113 128]\n",
      " [157 149]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0          113  128  241\n",
      "1          157  149  306\n",
      "All        270  277  547\n",
      "validation mode, book, and chunk acc 0.5035381028721762 0.47849500543962564 0.47849500543962564\n",
      "ger xgboost book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', 'pos'] None param= None 0.727906001567421 0.47849500543962564\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Dropped 1245 columns.\n",
      "[[125 116]\n",
      " [168 138]]\n",
      "Predicted  0.0  1.0  All\n",
      "True                    \n",
      "0          125  116  241\n",
      "1          168  138  306\n",
      "All        293  254  547\n",
      "validation mode, book, and chunk acc 0.46443857030391367 0.4799389928857047 0.4799389928857047\n",
      "ger xgboost book_and_averaged_chunk ['average_sentence_embedding', '100_most_common_', 'doc2vec_chunk_embedding', '->', 'pos'] None param= None 0.8059253425329022 0.4799389928857047\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/results/classification/classification_library/ger_library.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a5ea9ef7b921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m results_df = pd.DataFrame(results, columns=[\"lang\", \"model\", \"features\", \"drop_columns_including\", \n\u001b[1;32m     60\u001b[0m \"dimensionality_reduction\", \"model_param\", \"mean_train_book_acc\", \"mean_validation_book_acc\"])\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparam_dict\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3385\u001b[0m         )\n\u001b[1;32m   3386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3387\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         )\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/results/classification/classification_library/ger_library.csv'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run two-class classification\n",
    "'''\n",
    "results = []\n",
    "param_dict = \"library\" #\"twoclass\"\n",
    "for lang in [lang]:\n",
    "    if param_dict == \"testing\":\n",
    "        param_dir = testing_params\n",
    "    elif param_dict == \"twoclass\":\n",
    "        param_dir = twoclass_params\n",
    "    elif param_dict == \"multiclass\":\n",
    "        param_dir = multiclass_params\n",
    "    elif param_dict == \"full_cv\":\n",
    "        param_dir = full_cv_params\n",
    "    elif param_dict == \"library\":\n",
    "        param_dir = library_params\n",
    "    #Eng: 606 books, 14146 chunks, 13170 chunks of books published after 1759\n",
    "    book_df = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\")\n",
    "    book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "    chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "    chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "\n",
    "    for model in [] + param_dir['model']:\n",
    "        print(model)\n",
    "        model_param = model_params[model]\n",
    "        for model_param in model_param:\n",
    "            for dimensionality_reduction in param_dir[\"dimensionality_reduction\"]:\n",
    "                for features in param_dir[\"features\"]:\n",
    "                    for drop_columns_including in [[\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\"],\n",
    "                                                   [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\"],\n",
    "                                                   [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"pos\"],\n",
    "                                                   [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\", \"pos\"]]:\n",
    "                        if param_dict ==\"library\":\n",
    "                            labels = library_scores\n",
    "                            experiment = LibraryClassification(\n",
    "                                language=lang,\n",
    "                                features=features,\n",
    "                                drop_columns_including=drop_columns_including,\n",
    "                                dimensionality_reduction=dimensionality_reduction,\n",
    "                                model_param=model_param,\n",
    "                                model=model,\n",
    "                                verbose=True\n",
    "                            )\n",
    "                        else:\n",
    "                            experiment = TwoclassClassification(\n",
    "                                language=lang,\n",
    "                                features=features,\n",
    "                                drop_columns_including=drop_columns_including,\n",
    "                                dimensionality_reduction=dimensionality_reduction,\n",
    "                                model_param=model_param,\n",
    "                                model=model,\n",
    "                                verbose=True\n",
    "                            )\n",
    "                        mean_train_book_acc, mean_validation_book_acc = experiment.run()\n",
    "                        print(lang, model, features, drop_columns_including, dimensionality_reduction, 'param=', model_param, mean_train_book_acc, mean_validation_book_acc)\n",
    "                        print('\\n-----------------------------------------------------------\\n')\n",
    "                        results.append((lang, model, features, drop_columns_including, dimensionality_reduction, model_param, mean_train_book_acc, mean_validation_book_acc))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"lang\", \"model\", \"features\", \"drop_columns_including\", \n",
    "\"dimensionality_reduction\", \"model_param\", \"mean_train_book_acc\", \"mean_validation_book_acc\"])\n",
    "results_df.to_csv(results_dir + lang + '_' + param_dict + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2777a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(lang + '_' + param_dict + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86da650",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# Run Multiclass Classification\n",
    "# '''\n",
    "# results = []\n",
    "# param_dict = \"multiclass\" \n",
    "# for lang in [lang]:    \n",
    "#     if param_dict == \"testing\":\n",
    "#         param_dir = testing_params\n",
    "#         model_params = {\"svr\": [1], \"lasso\": [1], \"xgboost\": [None], \"svc\": [1]} \n",
    "#     elif param_dict == \"multiclass\":\n",
    "#         param_dir = multiclass_params\n",
    "#     elif param_dict == \"full_cv\":\n",
    "#         param_dir = full_cv_params\n",
    "#     elif param_dict == \"language_specific\":\n",
    "#         if lang == \"eng\":\n",
    "#             param_dir = eng_params\n",
    "#         else: \n",
    "#             param_dir = ger_params\n",
    "    \n",
    "#     #Eng: 606 books, 14146 chunks, 13170 chunks of books published after 1759\n",
    "#     book_df = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\")\n",
    "#     book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "#     #chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "#     #chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "    \n",
    "#     for model in [] + param_dir['model']:\n",
    "#         model_param = model_params[model]\n",
    "#         for model_param in model_param:\n",
    "#             for dimensionality_reduction in param_dir[\"dimensionality_reduction\"]:\n",
    "#                 for features in param_dir[\"features\"]:\n",
    "#                     for drop_columns_including in [[\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"pos\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\", \"pos\"]]:\n",
    "#                                                 #try:\n",
    "#                         print(param_dict, lang, model, features, drop_columns_including, dimensionality_reduction, 'param=', model_param)\n",
    "#                         experiment = MulticlassClassification(\n",
    "#                             language=lang,\n",
    "#                             features=features,\n",
    "#                             drop_columns_including=drop_columns_including,\n",
    "#                             dimensionality_reduction=dimensionality_reduction,\n",
    "#                             model_param=model_param,\n",
    "#                             model=model,\n",
    "#                             verbose=True\n",
    "#                         )\n",
    "#                         mean_train_f1, mean_validation_f1 = experiment.run()\n",
    "#                         results.append((lang, model, features, drop_columns_including, dimensionality_reduction, model_param, mean_train_f1, mean_validation_f1))\n",
    "#                         print(param_dict, lang, model, features, drop_columns_including, dimensionality_reduction, 'param=', model_param, mean_train_f1, mean_validation_f1)\n",
    "# results_df = pd.DataFrame(results, columns=[\"lang\", \"model\", \"features\", \"drop_columns_including\", \n",
    "# \"dimensionality_reduction\", \"model_param\", \"mean_train_f1\", \"mean_validation_f1\"])\n",
    "# results_df.to_csv(results_dir + lang + '_' + param_dict + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03eca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 378 texts without reviews, 197 different books with reviews, 6 of which have opposing reviews and are left out.\n",
    "# 378 + 197 - 6 = 569 texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Run Regression\n",
    "# '''\n",
    "# results = []\n",
    "# param_dict = \"regression\" \n",
    "# for lang in [lang]:\n",
    "#     if param_dict == \"regression\":\n",
    "#         param_dir = regression_params\n",
    "#     if param_dict == \"testing\":\n",
    "#         param_dir = testing_params\n",
    "#     elif param_dict == \"multiclass\":\n",
    "#         param_dir = multiclass_params\n",
    "#     elif param_dict == \"full_cv\":\n",
    "#         param_dir = full_cv_params\n",
    "#     elif param_dict == \"language_specific\":\n",
    "#         if lang == \"eng\":\n",
    "#             param_dir = eng_params\n",
    "#         else: \n",
    "#             param_dir = ger_params\n",
    "    \n",
    "#     #Eng: 606 books, 14146 chunks, 13170 chunks of books published after 1759\n",
    "#     book_df = pd.read_csv(f\"{extracted_features_dir}{lang}/book_df.csv\")\n",
    "#     book_and_averaged_chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/book_and_averaged_chunk_df.csv\")\n",
    "#     chunk_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_df.csv\")\n",
    "#     chunk_and_copied_book_df = pd.read_csv(f\"{extracted_features_dir}/{lang}/chunk_and_copied_book_df.csv\")\n",
    "\n",
    "#     for model in param_dir['model']:\n",
    "#         model_param = model_params[model]\n",
    "#         for model_param in model_param:\n",
    "#             for dimensionality_reduction in param_dir[\"dimensionality_reduction\"]:\n",
    "#                 for features in param_dir[\"features\"]:\n",
    "#                     for drop_columns_including in [[\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"pos\"],\n",
    "#                                                    [\"average_sentence_embedding\", \"100_most_common_\", \"doc2vec_chunk_embedding\", \"->\", \"pos\"]]:\n",
    "#                         print(param_dict, lang, model, features, drop_columns_including, dimensionality_reduction, 'param=', model_param)\n",
    "#                         experiment = Regression(\n",
    "#                             language=lang,\n",
    "#                             features=features,\n",
    "#                             drop_columns_including=drop_columns_including,\n",
    "#                             dimensionality_reduction=dimensionality_reduction,\n",
    "#                             model_param=model_param,\n",
    "#                             model=model,\n",
    "#                             verbose=True\n",
    "#                         )\n",
    "#                         mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value = experiment.run()\n",
    "#                         results.append((lang, model, features, drop_columns_including, dimensionality_reduction, model_param, mean_train_mse, mean_train_rmse, mean_train_mae, mean_train_r2, mean_train_corr, mean_validation_mse, mean_validation_rmse, mean_validation_mae, mean_validation_r2, mean_validation_corr, mean_p_value))\n",
    "#                         #except Exception as e:\n",
    "# #                             print(f\"Error in {lang}, {model}, {features}, {drop_columns_including}, {dimensionality_reduction}\")\n",
    "# #                             print(e)\n",
    "# results_df = pd.DataFrame(results, columns=[\"lang\", \"model\", \"features\", \"drop_columns_including\", \n",
    "# \"dimensionality_reduction\", \"model_param\", \"mean_train_mse\", \"mean_train_rmse\", \n",
    "# \"mean_train_mae\", \"mean_train_r2\", \"mean_train_corr\", \"mean_validation_mse\", \"mean_validation_rmse\",\n",
    "# \"mean_validation_mae\", \"mean_validation_r2\", \"mean_validation_corr\", \"mean_p_value\"])\n",
    "# results_df.to_csv(results_dir + lang + '_' + 'regression_' + param_dict + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ef4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
